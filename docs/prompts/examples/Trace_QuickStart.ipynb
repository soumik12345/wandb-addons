{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/parambharat/wandb-addons/blob/prompts/trace-api/docs/prompts/examples/Trace_QuickStart.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A quick start example that demonstrates how to use the `Trace` class, a high-level API to log LLM calls with the wandb prompts feature."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install -qqq -U openai langchain wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import wandb\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms.fake import FakeListLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from wandb_addons.prompts import Trace"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "PROJECT = \"high_level_trace\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# trace langchain chains\n",
    "run = wandb.init(project=PROJECT)\n",
    "llm = FakeListLLM(responses=[f\"Fake response: {i}\" for i in range(100)])\n",
    "prompt_template = \"What is a good name for a company that makes {product}?\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "for i in range(2):\n",
    "    product = f\"q: {i} - {datetime.datetime.now().timestamp()}\"\n",
    "    start_time_ms = datetime.datetime.now().timestamp() * 1000\n",
    "    response = chain(product)\n",
    "    end_time_ms = datetime.datetime.now().timestamp() * 1000\n",
    "    trace = Trace(\n",
    "        name=f\"fake_chain_{i}\",\n",
    "        kind=\"chain\",\n",
    "        status_code=\"success\",\n",
    "        metadata=None,\n",
    "        start_time_ms=start_time_ms,\n",
    "        end_time_ms=end_time_ms,\n",
    "        inputs={\"prompt\": prompt_template.format(product=product)},\n",
    "        outputs={\"response\": response[\"text\"]},\n",
    "    )\n",
    "    trace.log(name=f\"trace_{i}\")\n",
    "run.finish()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# trace openai api calls\n",
    "from getpass import getpass\n",
    "import openai\n",
    "\n",
    "openai.api_key = getpass(\"Please enter your openai api key\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run = wandb.init(project=PROJECT)\n",
    "request_kwargs = dict(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "start_time_ms = datetime.datetime.now().timestamp() * 1000\n",
    "response = openai.ChatCompletion.create(**request_kwargs)\n",
    "end_time_ms = datetime.datetime.now().timestamp() * 1000\n",
    "\n",
    "trace = Trace(\n",
    "    name=f\"openai_chat_completion\",\n",
    "    kind=\"llm\",\n",
    "    status_code=\"success\",\n",
    "    metadata={\"model\": \"gpt-3.5-turbo\"},\n",
    "    start_time_ms=start_time_ms,\n",
    "    end_time_ms=end_time_ms,\n",
    "    inputs={\"messages\": request_kwargs[\"messages\"]},\n",
    "    outputs={\"response\": response.choices[0][\"message\"][\"content\"]},\n",
    ")\n",
    "\n",
    "trace.log(name=f\"openai_trace\")\n",
    "run.finish()\n",
    "display(run)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# use with promprtify\n",
    "!pip install -qqq -U promptify"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from promptify import OpenAI\n",
    "from promptify import Prompter\n",
    "\n",
    "run = wandb.init(project=PROJECT)\n",
    "\n",
    "# NER example\n",
    "sentence = \"The patient is a 93-year-old female with a medical history of chronic right hip pain, osteoporosis, hypertension, depression, and chronic atrial fibrillation admitted for evaluation and management of severe nausea and vomiting and urinary tract infection\"\n",
    "\n",
    "model = OpenAI(openai.api_key)  # or `HubModel()` for Huggingface-based inference\n",
    "nlp_prompter = Prompter(model)\n",
    "\n",
    "start_time_ms = datetime.datetime.now().timestamp() * 1000\n",
    "result = nlp_prompter.fit(\n",
    "    \"ner.jinja\", domain=\"medical\", text_input=sentence, labels=None\n",
    ")\n",
    "end_time_ms = datetime.datetime.now().timestamp() * 1000\n",
    "\n",
    "\n",
    "trace = Trace(\n",
    "    name=f\"openai_chat_completion\",\n",
    "    kind=\"llm\",\n",
    "    status_code=\"success\",\n",
    "    metadata={k: v for k, v in result.items() if k != \"text\"},\n",
    "    start_time_ms=start_time_ms,\n",
    "    end_time_ms=end_time_ms,\n",
    "    inputs={\"sentence\": sentence},\n",
    "    outputs={\"entities\": result[\"text\"]},\n",
    ")\n",
    "trace.log(name=\"promptify_ner\")\n",
    "run.finish()\n",
    "display(run)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -qqq -U guidance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import guidance\n",
    "\n",
    "run = wandb.init(project=PROJECT)\n",
    "# define the model we will use\n",
    "guidance.llm = guidance.llms.OpenAI(\"text-davinci-003\", api_key=openai.api_key)\n",
    "\n",
    "# define the few shot examples\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"I wrote about shakespeare\",\n",
    "        \"entities\": [\n",
    "            {\"entity\": \"I\", \"time\": \"present\"},\n",
    "            {\"entity\": \"Shakespeare\", \"time\": \"16th century\"},\n",
    "        ],\n",
    "        \"reasoning\": \"I can write about Shakespeare because he lived in the past with respect to me.\",\n",
    "        \"answer\": \"No\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Shakespeare wrote about me\",\n",
    "        \"entities\": [\n",
    "            {\"entity\": \"Shakespeare\", \"time\": \"16th century\"},\n",
    "            {\"entity\": \"I\", \"time\": \"present\"},\n",
    "        ],\n",
    "        \"reasoning\": \"Shakespeare cannot have written about me, because he died before I was born\",\n",
    "        \"answer\": \"Yes\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# define the guidance program\n",
    "structure_prompt = guidance(\n",
    "    \"\"\"Given a sentence tell me whether it contains an anachronism (i.e. whether it could have happened or not based on the time periods associated with the entities).\n",
    "----\n",
    "\n",
    "{{~! display the few-shot examples ~}}\n",
    "{{~#each examples}}\n",
    "Sentence: {{this.input}}\n",
    "Entities and dates:{{#each this.entities}}\n",
    "{{this.entity}}: {{this.time}}{{/each}}\n",
    "Reasoning: {{this.reasoning}}\n",
    "Anachronism: {{this.answer}}\n",
    "---\n",
    "{{~/each}}\n",
    "\n",
    "{{~! place the real question at the end }}\n",
    "Sentence: {{input}}\n",
    "Entities and dates:\n",
    "{{gen \"entities\"}}\n",
    "Reasoning:{{gen \"Reasoning\"}}\n",
    "Anachronism:{{#select \"answer\"}} Yes{{or}} No{{/select}}\"\"\"\n",
    ")\n",
    "\n",
    "start_time_ms = datetime.datetime.now().timestamp() * 1000\n",
    "# execute the program\n",
    "result = structure_prompt(examples=examples, input=\"The T-rex bit my dog\")\n",
    "end_time_ms = datetime.datetime.now().timestamp() * 1000\n",
    "# trace guidance\n",
    "\n",
    "trace = Trace(\n",
    "    name=f\"guidance_anachronism\",\n",
    "    kind=\"llm\",\n",
    "    status_code=\"success\",\n",
    "    metadata=None,\n",
    "    start_time_ms=start_time_ms,\n",
    "    end_time_ms=end_time_ms,\n",
    "    inputs={\"sentence\": result.variables()[\"input\"]},\n",
    "    outputs={\n",
    "        \"entities\": result.variables()[\"entities\"],\n",
    "        \"answer\": result.variables()[\"answer\"],\n",
    "    },\n",
    ")\n",
    "trace.log(name=\"guidance_anachronism\")\n",
    "run.finish()\n",
    "display(run)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# example hierarchies usage in the Trace class\n",
    "import time\n",
    "\n",
    "root_trace = Trace(\n",
    "    name=\"Parent Model\",\n",
    "    kind=\"LLM\",\n",
    "    status_code=\"SUCCESS\",\n",
    "    metadata={\n",
    "        \"attr_1\": 1,\n",
    "        \"attr_2\": 2,\n",
    "    },\n",
    "    start_time_ms=int(round(time.time() * 1000)),\n",
    "    end_time_ms=int(round(time.time() * 1000)) + 1000,\n",
    "    inputs={\"user\": \"How old is google?\"},\n",
    "    outputs={\"assistant\": \"25 years old\"},\n",
    "    model_dict={\"_kind\": \"openai\", \"api_type\": \"azure\"},\n",
    ")\n",
    "\n",
    "first_child = Trace(\n",
    "    name=\"Child 1 Model\",\n",
    "    kind=\"LLM\",\n",
    "    status_code=\"ERROR\",\n",
    "    metadata={\n",
    "        \"child1_attr_1\": 1,\n",
    "        \"child1_attr_2\": 2,\n",
    "    },\n",
    "    start_time_ms=int(round(time.time() * 1000)) + 2000,\n",
    "    end_time_ms=int(round(time.time() * 1000)) + 3000,\n",
    "    inputs={\"user\": \"How old is google?\"},\n",
    "    outputs={\"assistant\": \"25 years old\"},\n",
    "    model_dict={\"_kind\": \"openai\", \"api_type\": \"child1_azure\"},\n",
    ")\n",
    "\n",
    "second_child = Trace(\n",
    "    name=\"Child 2 Model\",\n",
    "    kind=\"LLM\",\n",
    "    status_code=\"SUCCESS\",\n",
    "    metadata={\n",
    "        \"child2_attr_1\": 1,\n",
    "        \"child2_attr_2\": 2,\n",
    "    },\n",
    "    start_time_ms=int(round(time.time() * 1000)) + 4000,\n",
    "    end_time_ms=int(round(time.time() * 1000)) + 5000,\n",
    "    inputs={\"user\": \"How old is google?\"},\n",
    "    outputs={\"assistant\": \"25 years old\"},\n",
    "    model_dict={\"_kind\": \"openai\", \"api_type\": \"child2_azure\"},\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# simple heirarchy\n",
    "run = wandb.init(project=PROJECT, job_type=\"simple_heirarchy\")\n",
    "\n",
    "root_trace.add_child(first_child)\n",
    "first_child.add_child(second_child)\n",
    "\n",
    "root_trace.log(\"root_trace\")\n",
    "\n",
    "wandb.finish()\n",
    "display(run)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# nested heirarchy\n",
    "run = wandb.init(project=PROJECT, job_type=\"nested_heirarchy\")\n",
    "\n",
    "root_trace.add_child(first_child)\n",
    "first_child.add_child(second_child)\n",
    "root_trace.add_child(second_child)\n",
    "\n",
    "root_trace.log(\"root_trace\")\n",
    "\n",
    "wandb.finish()\n",
    "display(run)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# all traces\n",
    "run = wandb.init(project=PROJECT, job_type=\"all_traces\")\n",
    "\n",
    "root_trace.add_child(first_child)\n",
    "first_child.add_child(second_child)\n",
    "\n",
    "second_child.log(\"second_child\")\n",
    "first_child.log(\"first_child\")\n",
    "root_trace.log(\"root_trace\")\n",
    "\n",
    "wandb.finish()\n",
    "display(run)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
