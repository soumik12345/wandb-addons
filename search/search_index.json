{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udc1d Weights &amp; Biases Addons","text":"<p>Weights &amp; Biases Addons is a repository that provides of integrations and utilities that will supercharge your Weights &amp; Biases workflows. Its a repositpry built and maintained by <code>wandb</code> users for <code>wandb</code> users.</p>"},{"location":"#integrations","title":"Integrations","text":""},{"location":"#tensorflow-datasets","title":"TensorFlow Datasets","text":"<p>A set of utilities for easily accessing datasets for various machine learning tasks using Weights &amp; Biases artifacts built on top of TensorFlow Datasets.</p> <p>In order to install <code>wandb-addons</code> along with the dependencies for the dataset utilities, you can run:</p> <pre><code>git clone https://github.com/soumik12345/wandb-addons\npip install ./wandb-addons[dataset]\n</code></pre> <ul> <li> <p><code>WandbDatasetBuilder</code>: An abstract class for Dataset builder that enables building a dataset and upload it as a Weights &amp; Biases Artifact.</p> </li> <li> <p><code>upload_dataset</code>: Upload and register a dataset with a TFDS module or a TFDS builder script as a Weights &amp; Biases artifact. This function would verify if a TFDS build/registration is possible with the current specified dataset path and upload it as a Weights &amp; Biases artifact.</p> </li> <li> <p><code>load_dataset</code>: Load a dataset from a wandb artifact. Using this function you can load a dataset hosted as a wandb artifact in a single line of code, and use our powerful data processing methods to quickly get your dataset ready for training in a deep learning model.</p> </li> </ul>"},{"location":"#keras","title":"\ud83e\udd84 Keras","text":"<p>Backend-agnostic callbacks integrating Weights &amp; Biases with Keras-Core.</p> <p>In order to install <code>wandb-addons</code> along with the dependencies for the ciclo callbacks, you can run:</p> <pre><code>git clone https://github.com/soumik12345/wandb-addons\npip install ./wandb-addons[keras]\n</code></pre> <p>Once you've installed <code>wandb-addons</code>, you can import it using:</p> <pre><code>from wandb_addons.keras import WandbMetricsLogger, WandbModelCheckpoint\n\ncallbacks = [\n    WandbMetricsLogger(),           # Logs metrics to W&amp;B\n    WandbModelCheckpoint(filepath)  # Logs and versions model checkpoints to W&amp;B\n]\n\nmodel.fit(..., callbacks=callbacks)\n</code></pre> <p>For more information, check out - Backend-agnostic callbacks for KerasCore. - Object-detection integration for KerasCV.</p>"},{"location":"#monai","title":"MonAI","text":"<p>Event handlers for experiment tracking on Weights &amp; Biases with MonAI Engine for deep learning in healthcare imaging.</p> <p>In order to install <code>wandb-addons</code> along with the dependencies for the ciclo callbacks, you can run:</p> <pre><code>git clone https://github.com/soumik12345/wandb-addons\npip install ./wandb-addons[monai]\n</code></pre> <p>Once you've installed <code>wandb-addons</code>, you can import it using:</p> <pre><code>from wandb_addons.monai import WandbStatsHandler, WandbModelCheckpointHandler\n</code></pre> <p>For more information, check out more at the docs.</p>"},{"location":"#diffusers","title":"\ud83e\udde8 Diffusers","text":"<p>Callbacks for logging experiment details, configs and generated images for multi-modal diffusion pipelines from Diffusers \ud83e\udde8 to your Weights &amp; Biases workspace or Weave Dashboard.</p> <p>In order to install <code>wandb-addons</code> along with the dependencies for the ciclo callbacks, you can run:</p> <pre><code>git clone https://github.com/soumik12345/wandb-addons\npip install ./wandb-addons[huggingface]\n</code></pre> <p>Once you've installed <code>wandb-addons</code>, you can use the callbacks in the following manner:</p> <pre><code>import torch\nfrom diffusers import StableDiffusionPipeline\n\nfrom wandb_addons.diffusers import StableDiffusionCallback\n\n\npipeline = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16\n)\npipeline = pipeline.to(\"cuda\")\n\nprompt = [\n    \"a photograph of an astronaut riding a horse\",\n    \"a photograph of a dragon\"\n]\nnegative_prompt = [\"ugly, deformed\", \"ugly, deformed\"]\nnum_images_per_prompt = 2\nconfigs = {\n    \"eta\": 0.0,\n    \"guidance_rescale\": 0.0,\n}\n\n# Create the WandB callback for StableDiffusionPipeline\ncallback = StableDiffusionCallback(\n    pipe,\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    wandb_project=\"diffusers\",\n    num_images_per_prompt=num_images_per_prompt,\n    configs=configs,\n)\n\n# Add the callback to the pipeline\nimage = pipe(\n    prompt,\n    negative_prompt=negative_prompt,\n    callback=callback,\n    num_images_per_prompt=num_images_per_prompt,\n    **configs,\n)\n</code></pre> <p>For more information, check out more at the docs.</p>"},{"location":"#converting-ipython-notebooks-to-reports","title":"Converting IPython Notebooks to Reports","text":"<p>A set of utilities to convert an IPython notebook to a Weights &amp; Biases report.</p> <p>Simply install <code>wandb-addons</code> using</p> <pre><code>git clone https://github.com/soumik12345/wandb-addons\npip install ./wandb-addons\n</code></pre> <p>You can convert your notebook to a report using either the Python function or the CLI:</p> CLIPython API <pre><code>nb2report \\\\\n    --filepath Use_WandbMetricLogger_in_your_Keras_workflow.ipynb \\\\\n    --wandb_project report-to-notebook \\\\\n    --wandb_entity geekyrakshit \\\\\n    --report_title \"Use WandbMetricLogger in your Keras Workflow\" \\\\\n    --description \"A guide to using the WandbMetricLogger callback in your Keras and TensorFlow training worflow\" \\\\\n    --width \"readable\"\n</code></pre> <pre><code>from wandb_addons.report import convert_to_wandb_report\n\nconvert_to_wandb_report(\n    filepath=\"Use_WandbMetricLogger_in_your_Keras_workflow.ipynb\",\n    wandb_project=\"report-to-notebook\",\n    wandb_entity=\"geekyrakshit\",\n    report_title=\"Use WandbMetricLogger in your Keras Workflow\",\n    description=\"A guide to using the WandbMetricLogger callback in your Keras and TensorFlow training worflow\",\n    width=\"readable\"\n)\n</code></pre> <p>For more information, check out more at the docs.</p>"},{"location":"#status","title":"Status","text":"<p><code>wandb-addons</code> is still in early development, the API for integrations and utilities is subject to change, expect things to break. If you are interested in contributing, please feel free to open an issue and/or raise a pull request.</p>"},{"location":"datatypes/","title":"Experimental Datatypes","text":"<p>A list of experimental multi-modal datatypes that can be passed into a WandB loggable dictionary.</p>"},{"location":"datatypes/#wandb_addons.datatype.video.InteractiveVideo","title":"<code>InteractiveVideo</code>","text":"<p>             Bases: <code>Html</code></p> <p>Format a video such that it is logged in an interactive format with controls to contrast the default uncontrollable gif offered by <code>wandb.Video</code>.</p> <p>Example WandB Run</p> <p>https://wandb.ai/geekyrakshit/test/runs/vi00rpc5</p> Logging a video fileLogging a list of numpy arrays corresponding to frames <pre><code>import wandb\nfrom wandb_addons.datatype import loggable_video\n\nwith wandb.init(project=\"test\", entity=\"geekyrakshit\"):\n    wandb.log({\"Test-Video\": InteractiveVideo(\"video.mp4\")})\n</code></pre> <pre><code>import numpy as np\n\nimport wandb\nfrom wandb_addons.datatype import loggable_video\n\nwith wandb.init(project=\"test\", entity=\"geekyrakshit\"):\n    frames = [np.ones((256, 256, 3)) * 255] * 10 + [np.zeros((256, 256, 3))] * 10\n    wandb.log({\"Test-Video\": InteractiveVideo(frames)})\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>video</code> <code>Union[str, List[array]]</code> <p>The path to a video file or a list of numpy arrays of shape <code>(H, W, C)</code> corresponding to the frames of the video.</p> required <code>video_format</code> <code>str</code> <p>Format of the video.</p> <code>'mp4'</code> <code>fps</code> <code>int</code> <p>Frame-rate of the video, applicable only when logging list of numpy arrays.</p> <code>30</code> <p>Returns:</p> Type Description <code>Union[Html, None]</code> <p>A <code>wandb.Html</code> object that can be passed to a WandB loggable dictionary.</p> Source code in <code>wandb_addons/datatype/video.py</code> <pre><code>class InteractiveVideo(wandb.Html):\n    \"\"\"Format a video such that it is logged in an interactive format with controls\n    to contrast the default uncontrollable gif offered by\n    [`wandb.Video`](https://docs.wandb.ai/ref/python/data-types/video).\n\n    !!! example \"Example WandB Run\"\n        [https://wandb.ai/geekyrakshit/test/runs/vi00rpc5](https://wandb.ai/geekyrakshit/test/runs/vi00rpc5)\n\n    === \"Logging a video file\"\n\n        ```python\n        import wandb\n        from wandb_addons.datatype import loggable_video\n\n        with wandb.init(project=\"test\", entity=\"geekyrakshit\"):\n            wandb.log({\"Test-Video\": InteractiveVideo(\"video.mp4\")})\n        ```\n\n    === \"Logging a list of numpy arrays corresponding to frames\"\n\n        ```python\n        import numpy as np\n\n        import wandb\n        from wandb_addons.datatype import loggable_video\n\n        with wandb.init(project=\"test\", entity=\"geekyrakshit\"):\n            frames = [np.ones((256, 256, 3)) * 255] * 10 + [np.zeros((256, 256, 3))] * 10\n            wandb.log({\"Test-Video\": InteractiveVideo(frames)})\n        ```\n\n    Arguments:\n        video (Union[str, List[np.array]]): The path to a video file or a list of\n            numpy arrays of shape `(H, W, C)` corresponding to the frames of the video.\n        video_format (str): Format of the video.\n        fps (int): Frame-rate of the video, applicable only when logging list of\n            numpy arrays.\n\n    Returns:\n        (Union[wandb.Html, None]): A `wandb.Html` object that can be passed to a WandB\n            loggable dictionary.\n    \"\"\"\n\n    def __init__(\n        self,\n        video: Union[str, List[np.array]],\n        video_format: str = \"mp4\",\n        fps: int = 30,\n    ) -&gt; None:\n        if isinstance(video, str):\n            if os.path.isfile(video):\n                with open(video, \"rb\") as video_file:\n                    encoded_string = base64.b64encode(video_file.read()).decode(\"utf-8\")\n        elif isinstance(video, list):\n            encoded_string = create_video_from_np_arrays(video, fps)\n        else:\n            wandb.termwarn(\"Unable to log video\", repeat=False)\n            return\n        html_string = HTML_VIDEO_FORMAT.format(\n            encoded_string=encoded_string, format=video_format\n        )\n        super().__init__(html_string, inject=True)\n</code></pre>"},{"location":"datatypes/#wandb_addons.datatype.rgbd.RGBDPointCloud","title":"<code>RGBDPointCloud</code>","text":"<p>             Bases: <code>Object3D</code></p> <p>Format an RGB image and a depthmap such that it is logged as an interactive 3d point cloud.</p> <p>Example WandB Run</p> <p>https://wandb.ai/geekyrakshit/test/runs/8ftwuuwf</p> <p>Logging an RGB Image and a Depthmap as a Point Cloud</p> <pre><code>from PIL import Image\n\nimport wandb\nfrom wandb_addons.datatype import RGBDPointCloud\n\nwith wandb.init(project=\"test\"):\n    rgb_image = Image.open(\"./docs/assets/sample_image.jpg\")\n    depth_image = Image.open(\"./docs/assets/sample_depth.png\")\n    wandb.log({\"Test-RGBD\": RGBDPointCloud(rgb_image, depth_image)})\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>rgb_image</code> <code>Union[str, Image, array]</code> <p>The RGB image. Either a path to an image file, or a PIL Image, or a numpy array can be passed.</p> required <code>depth_image</code> <code>Union[str, Image, array]</code> <p>The Depthmap. Either a path to an image file, or a PIL Image, or a numpy array can be passed.</p> required <code>camera_intrinsic_parameters</code> <code>Dict[str, float]</code> <p>The camera intrinsic parameters as a dictionary. Defaults to <code>o3d.camera.PinholeCameraIntrinsicParameters.PrimeSenseDefault</code> if not specified.</p> <code>None</code> Source code in <code>wandb_addons/datatype/rgbd.py</code> <pre><code>class RGBDPointCloud(wandb.Object3D):\n    \"\"\"Format an RGB image and a depthmap such that it is logged as an interactive 3d point cloud.\n\n    !!! example \"Example WandB Run\"\n        [https://wandb.ai/geekyrakshit/test/runs/8ftwuuwf](https://wandb.ai/geekyrakshit/test/runs/8ftwuuwf)\n\n    !!! example \"Logging an RGB Image and a Depthmap as a Point Cloud\"\n        ```python\n        from PIL import Image\n\n        import wandb\n        from wandb_addons.datatype import RGBDPointCloud\n\n        with wandb.init(project=\"test\"):\n            rgb_image = Image.open(\"./docs/assets/sample_image.jpg\")\n            depth_image = Image.open(\"./docs/assets/sample_depth.png\")\n            wandb.log({\"Test-RGBD\": RGBDPointCloud(rgb_image, depth_image)})\n        ```\n\n    Arguments:\n        rgb_image (Union[str, Image.Image, np.array]): The RGB image. Either a path to\n            an image file, or a PIL Image, or a numpy array can be passed.\n        depth_image (Union[str, Image.Image, np.array]): The Depthmap. Either a path to\n            an image file, or a PIL Image, or a numpy array can be passed.\n        camera_intrinsic_parameters (Dict[str, float]): The camera intrinsic parameters\n            as a dictionary. Defaults to\n            `o3d.camera.PinholeCameraIntrinsicParameters.PrimeSenseDefault` if not\n            specified.\n    \"\"\"\n\n    def __init__(\n        self,\n        rgb_image: Union[str, Image.Image, np.array],\n        depth_image: Union[str, Image.Image, np.array],\n        camera_intrinsic_parameters: Dict[str, float] = None,\n        **kwargs,\n    ) -&gt; None:\n        self.camera_intrinsic_parameters = (\n            o3d.camera.PinholeCameraIntrinsicParameters.PrimeSenseDefault\n            if camera_intrinsic_parameters is None\n            else camera_intrinsic_parameters\n        )\n        rgb_image_numpy, point_cloud = self.create_point_cloud(rgb_image, depth_image)\n        normalized_point_cloud = self.normalize_point_cloud(point_cloud)\n        colored_point_cloud = self.get_colored_point_cloud(\n            rgb_image_numpy, normalized_point_cloud\n        )\n        super().__init__(colored_point_cloud, **kwargs)\n\n    def _get_images_as_numpy_arrays(\n        self,\n        rgb_image: Union[str, Image.Image, np.array],\n        depth_image: Union[str, Image.Image, np.array],\n    ):\n        if isinstance(rgb_image, str) and os.path.isfile(rgb_image):\n            rgb_image = Image.open(rgb_image)\n        if isinstance(depth_image, str) and os.path.isfile(depth_image):\n            depth_image = Image.open(depth_image)\n        if isinstance(rgb_image, Image.Image):\n            rgb_image = np.array(rgb_image)\n        if isinstance(depth_image, Image.Image):\n            depth_image = np.array(depth_image)\n        assert (\n            len(rgb_image.shape) == 3\n        ), \"Batched pair of RGB images and Depthmaps are not yet supported\"\n        assert rgb_image.shape[-1] == 3, \"RGB image must have 3 channels\"\n        return rgb_image, depth_image\n\n    def create_point_cloud(\n        self,\n        rgb_image: Union[str, Image.Image, np.array],\n        depth_image: Union[str, Image.Image, np.array],\n    ):\n        rgb_image, depth_image = self._get_images_as_numpy_arrays(\n            rgb_image, depth_image\n        )\n        rgb_image_numpy = rgb_image\n        rgb_image = o3d.geometry.Image(rgb_image)\n        depth_image = o3d.geometry.Image(depth_image)\n        rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(\n            rgb_image, depth_image, convert_rgb_to_intensity=False\n        )\n        camera_intrinsic = o3d.camera.PinholeCameraIntrinsic(\n            self.camera_intrinsic_parameters\n        )\n        point_cloud = o3d.geometry.PointCloud.create_from_rgbd_image(\n            rgbd_image, camera_intrinsic\n        )\n        point_cloud.transform(\n            [[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]]\n        )\n        return rgb_image_numpy, np.asarray(point_cloud.points)\n\n    def normalize_point_cloud(self, point_cloud):\n        min_values = np.min(point_cloud, axis=0)\n        max_values = np.max(point_cloud, axis=0)\n        range_values = max_values - min_values\n        normalized_point_cloud = (point_cloud - min_values) / range_values\n        return normalized_point_cloud\n\n    def get_colored_point_cloud(self, rgb_image_numpy, normalized_point_cloud):\n        rgb_image_numpy = rgb_image_numpy.reshape(-1, 3)\n        colored_point_cloud = np.concatenate(\n            (normalized_point_cloud, rgb_image_numpy), axis=-1\n        )\n        return colored_point_cloud\n</code></pre>"},{"location":"datatypes/#wandb_addons.datatype.voxelized_pcd.VoxelizedPointCloud","title":"<code>VoxelizedPointCloud</code>","text":"<p>             Bases: <code>Object3D</code></p> <p>Voxelizes a high-resolution point-cloud and format as a wandb-loggable 3D mesh.</p> <p>Example WandB Run</p> <p>Logging the voxelized mesh of this point cloud takes up a memort of ~29 MB of space on the wandb run, whereas logging the raw point cloud takes up ~700 MB of space.</p> <p>https://wandb.ai/geekyrakshit/test/runs/w2nrw85q</p> From Laspy FileFrom Numpy Array <pre><code>import wandb\nfrom wandb_addons.datatype import VoxelizedPointCloud\n\nwith wandb.init(project=\"test\"):\n    wandb.log(\n        {\"Test-Point-Cloud\": VoxelizedPointCloud(\"2021_heerlen_table.las\")}\n    )\n</code></pre> <pre><code>import laspy as lp\nimport numpy as np\n\nimport wandb\nfrom wandb_addons.datatype import VoxelizedPointCloud\n\npoint_cloud = lp.read(\"2021_heerlen_table.las\")\nnumpy_point_cloud = np.vstack((point_cloud.x, point_cloud.y, point_cloud.z)).transpose()\nnumpy_color_cloud = (\n    np.vstack((point_cloud.red, point_cloud.green, point_cloud.blue)).transpose()\n    / 65535\n)\n\nwith wandb.init(project=\"test\"):\n    wandb.log(\n        {\"Test-Point-Cloud\": VoxelizedPointCloud(numpy_point_cloud, numpy_color_cloud)}\n    )\n</code></pre> <p>Reference</p> <p>How to Automate Voxel Modelling of 3D Point Cloud with Python</p> <p>Parameters:</p> Name Type Description Default <code>point_cloud</code> <code> Optional[Union[str, np.array]]</code> <p>The point cloud. Either a path to a laspy file, or a numpy array of shape <code>(N, 3)</code> where <code>N</code> is the number of points.</p> <code>None</code> <code>colors</code> <code>Optional[array]</code> <p>The colors of the point cloud. A numpy array of shape <code>(N, 3)</code> consisting of color values corresponsing to the point cloud in the range <code>[0, 1]</code>. This is only necessary to be specified for point clouds as numpy arrays.</p> <code>None</code> <code>voxel_size_percentage</code> <code>Optional[float]</code> <p>The size of each voxel as a percentage of the maximum edge of the point cloud.</p> <code>0.5</code> <code>voxel_precision</code> <code>Optional[int]</code> <p>The precision of the voxel size.</p> <code>4</code> Source code in <code>wandb_addons/datatype/voxelized_pcd.py</code> <pre><code>class VoxelizedPointCloud(wandb.Object3D):\n    \"\"\"Voxelizes a high-resolution point-cloud and format as a wandb-loggable 3D mesh.\n\n    !!! example \"Example WandB Run\"\n        Logging the voxelized mesh of\n        [this point cloud](https://drive.google.com/file/d/1Zr1y8BSYRHBKxvs_nUXo2LSQr2i5ulgj/view?usp=sharing)\n        takes up a memort of ~29 MB of space on the wandb run, whereas logging the raw\n        point cloud takes up ~700 MB of space.\n\n        [https://wandb.ai/geekyrakshit/test/runs/w2nrw85q](https://wandb.ai/geekyrakshit/test/runs/w2nrw85q)\n\n    === \"From Laspy File\"\n\n        ```python\n        import wandb\n        from wandb_addons.datatype import VoxelizedPointCloud\n\n        with wandb.init(project=\"test\"):\n            wandb.log(\n                {\"Test-Point-Cloud\": VoxelizedPointCloud(\"2021_heerlen_table.las\")}\n            )\n        ```\n\n    === \"From Numpy Array\"\n\n        ```python\n        import laspy as lp\n        import numpy as np\n\n        import wandb\n        from wandb_addons.datatype import VoxelizedPointCloud\n\n        point_cloud = lp.read(\"2021_heerlen_table.las\")\n        numpy_point_cloud = np.vstack((point_cloud.x, point_cloud.y, point_cloud.z)).transpose()\n        numpy_color_cloud = (\n            np.vstack((point_cloud.red, point_cloud.green, point_cloud.blue)).transpose()\n            / 65535\n        )\n\n        with wandb.init(project=\"test\"):\n            wandb.log(\n                {\"Test-Point-Cloud\": VoxelizedPointCloud(numpy_point_cloud, numpy_color_cloud)}\n            )\n\n        ```\n\n    !!! Info \"Reference\"\n        [How to Automate Voxel Modelling of 3D Point Cloud with Python](https://towardsdatascience.com/how-to-automate-voxel-modelling-of-3d-point-cloud-with-python-459f4d43a227)\n\n    Arguments:\n        point_cloud ( Optional[Union[str, np.array]]): The point cloud. Either a path to\n            a laspy file, or a numpy array of shape `(N, 3)` where `N` is the number of\n            points.\n        colors (Optional[np.array]): The colors of the point cloud. A numpy array of\n            shape `(N, 3)` consisting of color values corresponsing to the point cloud\n            in the range `[0, 1]`. This is only necessary to be specified for point\n            clouds as numpy arrays.\n        voxel_size_percentage (Optional[float]): The size of each voxel as a percentage\n            of the maximum edge of the point cloud.\n        voxel_precision (Optional[int]): The precision of the voxel size.\n    \"\"\"\n\n    def __init__(\n        self,\n        point_cloud: Optional[Union[str, np.array]] = None,\n        colors: Optional[np.array] = None,\n        voxel_size_percentage: Optional[float] = 0.5,\n        voxel_precision: Optional[int] = 4,\n        **kwargs\n    ) -&gt; None:\n        o3d_point_cloud = self.build_open3d_point_cloud(point_cloud, colors)\n        voxel_mesh = self.voxelize_point_cloud(\n            o3d_point_cloud, voxel_size_percentage, voxel_precision\n        )\n        voxel_file = os.path.join(wandb.run.dir, \"voxelized_point_cloud.glb\")\n        o3d.io.write_triangle_mesh(voxel_file, voxel_mesh)\n        super().__init__(open(voxel_file), **kwargs)\n\n    def build_open3d_point_cloud(self, point_cloud, colors):\n        if isinstance(point_cloud, str):\n            point_cloud = lp.read(point_cloud)\n            numpy_point_cloud = np.vstack(\n                (point_cloud.x, point_cloud.y, point_cloud.z)\n            ).transpose()\n            numpy_color_cloud = (\n                np.vstack(\n                    (point_cloud.red, point_cloud.green, point_cloud.blue)\n                ).transpose()\n                / 65535\n            )\n            o3d_point_cloud = o3d.geometry.PointCloud()\n            o3d_point_cloud.points = o3d.utility.Vector3dVector(numpy_point_cloud)\n            o3d_point_cloud.colors = o3d.utility.Vector3dVector(numpy_color_cloud)\n            return o3d_point_cloud\n        elif isinstance(point_cloud, np.ndarray):\n            o3d_point_cloud = o3d.geometry.PointCloud()\n            o3d_point_cloud.points = o3d.utility.Vector3dVector(point_cloud)\n            if colors is not None:\n                o3d_point_cloud.colors = o3d.utility.Vector3dVector(colors)\n            return o3d_point_cloud\n\n    def voxelize_point_cloud(\n        self, o3d_point_cloud, voxel_size_percentage, voxel_precision\n    ):\n        voxel_size = max(\n            o3d_point_cloud.get_max_bound() - o3d_point_cloud.get_min_bound()\n        ) * (voxel_size_percentage / 100.0)\n        voxel_size = round(voxel_size, voxel_precision)\n        voxel_grid = o3d.geometry.VoxelGrid.create_from_point_cloud(\n            o3d_point_cloud, voxel_size=voxel_size\n        )\n        voxels = voxel_grid.get_voxels()\n        voxel_mesh = o3d.geometry.TriangleMesh()\n        for v in voxels:\n            cube = o3d.geometry.TriangleMesh.create_box(width=1, height=1, depth=1)\n            cube.paint_uniform_color(v.color)\n            cube.translate(v.grid_index, relative=False)\n            voxel_mesh += cube\n        voxel_mesh.translate([0.5, 0.5, 0.5], relative=True)\n        voxel_mesh.scale(voxel_size, [0, 0, 0])\n        voxel_mesh.merge_close_vertices(1e-7)\n        voxel_mesh.translate(voxel_grid.origin, relative=True)\n        return voxel_mesh\n</code></pre>"},{"location":"report/","title":"Notebook to Report Conversion","text":""},{"location":"report/#wandb_addons.report.notebook_convert.convert_to_wandb_report","title":"<code>convert_to_wandb_report(filepath, wandb_project, wandb_entity, report_title='Untitled Report', description='', width='readable')</code>","text":"<p>Convert an IPython Notebook to a Weights &amp; Biases Report.</p> <p>Usage:</p> CLIPython API <pre><code>nb2report \\\n    --filepath Use_WandbMetricLogger_in_your_Keras_workflow.ipynb \\\n    --wandb_project report-to-notebook \\\n    --wandb_entity geekyrakshit \\\n    --report_title \"Use WandbMetricLogger in your Keras Workflow\" \\\n    --description \"A guide to using the WandbMetricLogger callback in your Keras and TensorFlow training worflow\" \\\n    --width \"readable\"\n</code></pre> <pre><code>from wandb_addons.report import convert_to_wandb_report\n\nconvert_to_wandb_report(\n    filepath=\"Use_WandbMetricLogger_in_your_Keras_workflow.ipynb\",\n    wandb_project=\"report-to-notebook\",\n    wandb_entity=\"geekyrakshit\",\n    report_title=\"Use WandbMetricLogger in your Keras Workflow\",\n    description=\"A guide to using the WandbMetricLogger callback in your Keras and TensorFlow training worflow\",\n    width=\"readable\"\n)\n</code></pre> <p>Note</p> <p>In order to include panel grids with runsets and line plots in your report, you need to include YAML metadata regarding the runsets and line plots you want to include in a panel grid in a markdown cell of your notebook in the following format:</p> <pre><code>---\npanelgrid:\nrunsets:\n- project: report-to-notebook\n    entity: geekyrakshit\n    name: Training-Logs\nlineplots:\n- x: batch/batch_step\n    y: batch/accuracy\n- x: batch/batch_step\n    y: batch/learning_rate\n- x: batch/batch_step\n    y: batch/loss\n- x: batch/batch_step\n    y: batch/top@5_accuracy\n- x: epoch/epoch\n    y: epoch/accuracy\n- x: epoch/epoch\n    y: epoch/learning_rate\n- x: epoch/epoch\n    y: epoch/loss\n- x: epoch/epoch\n    y: epoch/top@5_accuracy\n- x: epoch/epoch\n    y: epoch/val_accuracy\n- x: epoch/epoch\n    y: epoch/val_loss\n- x: epoch/epoch\n    y: epoch/val_top@5_accuracy\n---\n</code></pre> <p>Currently only line plots are supported inside panel grids.</p> <p>Converting using CLI</p> <p>Convert an IPython notebook to a Weights &amp; Biases report using the <code>nb2report</code> CLI:</p> <pre><code>Usage: nb2report [OPTIONS]\n\nOptions:\n--filepath TEXT       Path to an IPython notebook\n--wandb_project TEXT  The name of the Weights &amp; Biases project where you're\n                        creating the project\n--wandb_entity TEXT   The name of the Weights &amp; Biases entity (username or\n                        team name)\n--report_title TEXT   The title of the report\n--description TEXT    The description of the report\n--width TEXT          Width of the report, one of `'readable'`, `'fixed'`,\n                        or `'fluid'`\n--help                Show this message and exit.\n</code></pre> <p>Example</p> <p>Here is a report was generated for this notebook.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to an IPython notebook.</p> required <code>wandb_project</code> <code>str</code> <p>The name of the Weights &amp; Biases project where you're creating the project.</p> required <code>wandb_entity</code> <code>str</code> <p>The name of the Weights &amp; Biases entity (username or team name).</p> required <code>report_title</code> <code>Optional[str]</code> <p>The title of the report.</p> <code>'Untitled Report'</code> <code>description</code> <code>Optional[str]</code> <p>The description of the report.</p> <code>''</code> <code>width</code> <code>Optional[str]</code> <p>Width of the report, one of <code>\"readable\"</code>, <code>\"fixed\"</code>, or <code>\"fluid\"</code>.</p> <code>'readable'</code> Source code in <code>wandb_addons/report/notebook_convert.py</code> <pre><code>def convert_to_wandb_report(\n    filepath: str,\n    wandb_project: str,\n    wandb_entity: str,\n    report_title: Optional[str] = \"Untitled Report\",\n    description: Optional[str] = \"\",\n    width: Optional[str] = \"readable\",\n):\n    \"\"\"Convert an IPython Notebook to a [Weights &amp; Biases Report](https://docs.wandb.ai/guides/reports).\n\n    **Usage:**\n\n    === \"CLI\"\n        ```shell\n        nb2report \\\\\n            --filepath Use_WandbMetricLogger_in_your_Keras_workflow.ipynb \\\\\n            --wandb_project report-to-notebook \\\\\n            --wandb_entity geekyrakshit \\\\\n            --report_title \"Use WandbMetricLogger in your Keras Workflow\" \\\\\n            --description \"A guide to using the WandbMetricLogger callback in your Keras and TensorFlow training worflow\" \\\\\n            --width \"readable\"\n        ```\n\n    === \"Python API\"\n        ```python\n        from wandb_addons.report import convert_to_wandb_report\n\n        convert_to_wandb_report(\n            filepath=\"Use_WandbMetricLogger_in_your_Keras_workflow.ipynb\",\n            wandb_project=\"report-to-notebook\",\n            wandb_entity=\"geekyrakshit\",\n            report_title=\"Use WandbMetricLogger in your Keras Workflow\",\n            description=\"A guide to using the WandbMetricLogger callback in your Keras and TensorFlow training worflow\",\n            width=\"readable\"\n        )\n        ```\n\n\n    !!! note\n        In order to include panel grids with runsets and line plots in your report, you need to include\n        YAML metadata regarding the runsets and line plots you want to include in a panel grid in a\n        markdown cell of your notebook in the following format:\n\n        ```yaml\n        ---\n        panelgrid:\n        runsets:\n        - project: report-to-notebook\n            entity: geekyrakshit\n            name: Training-Logs\n        lineplots:\n        - x: batch/batch_step\n            y: batch/accuracy\n        - x: batch/batch_step\n            y: batch/learning_rate\n        - x: batch/batch_step\n            y: batch/loss\n        - x: batch/batch_step\n            y: batch/top@5_accuracy\n        - x: epoch/epoch\n            y: epoch/accuracy\n        - x: epoch/epoch\n            y: epoch/learning_rate\n        - x: epoch/epoch\n            y: epoch/loss\n        - x: epoch/epoch\n            y: epoch/top@5_accuracy\n        - x: epoch/epoch\n            y: epoch/val_accuracy\n        - x: epoch/epoch\n            y: epoch/val_loss\n        - x: epoch/epoch\n            y: epoch/val_top@5_accuracy\n        ---\n        ```\n\n        Currently only line plots are supported inside panel grids.\n\n    !!! note \"Converting using CLI\"\n        Convert an IPython notebook to a Weights &amp; Biases report using the `nb2report` CLI:\n\n        ```shell\n        Usage: nb2report [OPTIONS]\n\n        Options:\n        --filepath TEXT       Path to an IPython notebook\n        --wandb_project TEXT  The name of the Weights &amp; Biases project where you're\n                                creating the project\n        --wandb_entity TEXT   The name of the Weights &amp; Biases entity (username or\n                                team name)\n        --report_title TEXT   The title of the report\n        --description TEXT    The description of the report\n        --width TEXT          Width of the report, one of `'readable'`, `'fixed'`,\n                                or `'fluid'`\n        --help                Show this message and exit.\n        ```\n\n    !!! example \"Example\"\n        [Here](https://wandb.ai/geekyrakshit/report-to-notebook/reports/Use-WandbMetricLogger-in-your-Keras-Workflow--Vmlldzo0Mjg4NTM2) is a report was generated for [this](https://github.com/wandb/examples/blob/master/colabs/keras/Use_WandbMetricLogger_in_your_Keras_workflow.ipynb) notebook.\n\n    Args:\n        filepath (str): Path to an IPython notebook.\n        wandb_project (str): The name of the Weights &amp; Biases project where you're creating the project.\n        wandb_entity (str): The name of the Weights &amp; Biases entity (username or team name).\n        report_title (Optional[str]): The title of the report.\n        description (Optional[str]): The description of the report.\n        width (Optional[str]): Width of the report, one of `\"readable\"`, `\"fixed\"`, or `\"fluid\"`.\n    \"\"\"\n    notebook_cells = _parse_notebook_cells(filepath)\n\n    report = wr.Report(\n        project=wandb_project,\n        title=report_title,\n        entity=wandb_entity,\n        description=description,\n        width=width,\n    )\n\n    blocks = []\n    for cell in tqdm(notebook_cells, desc=\"Converting notebook cells to report cells\"):\n        if cell[\"type\"] == \"markdown\":\n            blocks.append(wr.MarkdownBlock(text=cell[\"source\"]))\n        elif cell[\"type\"] == \"code\":\n            blocks.append(wr.MarkdownBlock(text=f\"```python\\n{cell['source']}\\n```\"))\n        elif cell[\"type\"] == \"panel_metadata\":\n            blocks.append(_convert_metadata_to_panelgrid(metadata=cell[\"source\"]))\n\n    report.blocks = blocks\n    report.save()\n    wandb.termlog(\n        \"Report {report_title} created successfully. \"\n        + f\"Check list of reports at {report.url}.\"\n    )\n</code></pre>"},{"location":"utils/","title":"Utilities","text":""},{"location":"utils/#wandb_addons.utils.autogenerate_seed","title":"<code>autogenerate_seed()</code>","text":"<p>Automatically generate a random seed for machine-learning experiments.</p> Source code in <code>wandb_addons/utils.py</code> <pre><code>def autogenerate_seed():\n    \"\"\"Automatically generate a random seed for machine-learning experiments.\"\"\"\n    max_seed = int(1024 * 1024 * 1024)\n    seed = random.randint(1, max_seed)\n    seed = -seed if seed &lt; 0 else seed\n    seed = seed % max_seed\n    return seed\n</code></pre>"},{"location":"utils/#wandb_addons.utils.fetch_wandb_artifact","title":"<code>fetch_wandb_artifact(artifact_address, artifact_type)</code>","text":"<p>Utility function for fetching a Weights &amp; Biases artifact irrespective of whether a run has been initialized or not.</p> <p>Parameters:</p> Name Type Description Default <code>artifact_address</code> <code>str</code> <p>A human-readable name for the artifact, which is how you can identify the artifact in the UI or reference it in <code>use_artifact</code> calls. Names can contain letters, numbers, underscores, hyphens, and dots. The name must be unique across a project.</p> required <code>artifact_type</code> <code>str</code> <p>The type of the artifact, which is used to organize and differentiate artifacts. Common typesCinclude dataset or model, but you can use any string containing letters, numbers, underscores, hyphens, and dots.</p> required <p>Returns:</p> Type Description <code>FilePathStr</code> <p>The path to the downloaded contents.</p> Source code in <code>wandb_addons/utils.py</code> <pre><code>def fetch_wandb_artifact(artifact_address: str, artifact_type: str) -&gt; FilePathStr:\n    \"\"\"Utility function for fetching a\n    [Weights &amp; Biases artifact](https://docs.wandb.ai/guides/artifacts)\n    irrespective of whether a [run](https://docs.wandb.ai/guides/runs) has been initialized or not.\n\n    Args:\n        artifact_address (str): A human-readable name for the artifact, which is how you can\n            identify the artifact in the UI or reference it in\n            [`use_artifact`](https://docs.wandb.ai/ref/python/run#use_artifact) calls. Names can\n            contain letters, numbers, underscores, hyphens, and dots. The name must be unique across\n            a project.\n        artifact_type (str): The type of the artifact, which is used to organize and differentiate\n            artifacts. Common typesCinclude dataset or model, but you can use any string containing\n            letters, numbers, underscores, hyphens, and dots.\n\n    Returns:\n        (wandb.util.FilePathStr): The path to the downloaded contents.\n    \"\"\"\n    return (\n        wandb.Api().artifact(artifact_address, type=artifact_type).download()\n        if wandb.run is None\n        else wandb.use_artifact(artifact_address, type=artifact_type).download()\n    )\n</code></pre>"},{"location":"utils/#wandb_addons.utils.flatten_nested_dictionaries","title":"<code>flatten_nested_dictionaries(d, parent_key='', sep='/')</code>","text":"<p>A recursive function for flattening nested dictionaries.</p>"},{"location":"utils/#wandb_addons.utils.flatten_nested_dictionaries--reference","title":"Reference:","text":"<pre><code>Answer to\n[**Flatten nested dictionaries, compressing keys**](https://stackoverflow.com/q/6027558)\non StackOverflow: [stackoverflow.com/a/6027615](https://stackoverflow.com/a/6027615)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>Dict</code> <p>The input nested dictionary.</p> required <code>parent_key</code> <code>str</code> <p>The parent key.</p> <code>''</code> <code>sep</code> <code>str</code> <p>The separator to use for the flattened keys.</p> <code>'/'</code> <p>Returns:</p> Type Description <code>Dict</code> <p>The flattened dictionary.</p> Source code in <code>wandb_addons/utils.py</code> <pre><code>def flatten_nested_dictionaries(d: Dict, parent_key: str = \"\", sep: str = \"/\") -&gt; Dict:\n    \"\"\"A recursive function for flattening nested dictionaries.\n\n    # Reference:\n        Answer to\n        [**Flatten nested dictionaries, compressing keys**](https://stackoverflow.com/q/6027558)\n        on StackOverflow: [stackoverflow.com/a/6027615](https://stackoverflow.com/a/6027615)\n\n    Args:\n        d (Dict): The input nested dictionary.\n        parent_key (str): The parent key.\n        sep (str): The separator to use for the flattened keys.\n\n    Returns:\n        (Dict): The flattened dictionary.\n    \"\"\"\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, MutableMapping):\n            items.extend(flatten_nested_dictionaries(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n</code></pre>"},{"location":"ciclo/ciclo/","title":"Ciclo Callbacks","text":"<p>Functional callbacks for experiment tracking on Weights &amp; Biases with Ciclo.</p>"},{"location":"ciclo/ciclo/#wandb_addons.ciclo.wandb_log.WandbLogger","title":"<code>WandbLogger</code>","text":"<p>             Bases: <code>LoopCallbackBase[S]</code></p> <p>A ciclo callback for logging to Weights &amp; Biases.</p> Example notebooks: <ul> <li>MNIST classification using Ciclo.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>additional_logging</code> <code>Optional[Callable]</code> <p>A function to be called after each logging step and can be used to log additional values or media to Weights &amp; Biases.</p> <code>None</code> Source code in <code>wandb_addons/ciclo/wandb_log.py</code> <pre><code>class WandbLogger(LoopCallbackBase[S]):\n    \"\"\"A [ciclo](https://github.com/cgarciae/ciclo) callback for logging to Weights &amp; Biases.\n\n    ??? example \"Example notebooks:\"\n        - [MNIST classification using Ciclo](../examples/ciclo_MNIST).\n\n    Args:\n        additional_logging (Optional[Callable]): A function to be called after each logging step\n            and can be used to log additional values or media to Weights &amp; Biases.\n    \"\"\"\n\n    def __init__(self, additional_logging: Optional[Callable] = None):\n        self.additional_logging = additional_logging\n\n    def __call__(self, elapsed: Elapsed, state: S, logs: Optional[Logs] = None):\n        wandb.log(flatten_nested_dictionaries(logs, sep=\"/\"))\n        if self.additional_logging is not None:\n            self.additional_logging()\n\n    def __loop_callback__(self, loop_state: LoopState[S]) -&gt; CallbackOutput[S]:\n        self(loop_state.elapsed, loop_state.state, loop_state.accumulated_logs)\n        return Logs(), loop_state.state\n\n    def on_epoch_end(\n        self, state, batch, elapsed, loop_state: LoopState[S]\n    ) -&gt; CallbackOutput[S]:\n        return self.__loop_callback__(loop_state)\n</code></pre>"},{"location":"ciclo/examples/ciclo_MNIST/","title":"ciclo MNIST","text":"In\u00a0[\u00a0]: Copied! <pre>!git clone https://github.com/soumik12345/wandb-addons\n!pip install .[jax]\n</pre> !git clone https://github.com/soumik12345/wandb-addons !pip install .[jax] In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\nfrom time import time\nfrom typing import Optional, Callable\nfrom collections.abc import MutableMapping\n\nimport flax.linen as nn\nimport jax.numpy as jnp\nimport jax_metrics as jm\nimport matplotlib.pyplot as plt\nimport optax\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\nimport ciclo\nfrom ciclo.logging import Logs\nfrom ciclo.types import Batch, S\nfrom ciclo.timetracking import Elapsed\nfrom ciclo.loops.loop import LoopCallbackBase\nfrom ciclo.callbacks import LoopState, CallbackOutput\n\nimport wandb\nfrom wandb_addons.ciclo import WandbLogger\n</pre> from pathlib import Path from time import time from typing import Optional, Callable from collections.abc import MutableMapping  import flax.linen as nn import jax.numpy as jnp import jax_metrics as jm import matplotlib.pyplot as plt import optax import tensorflow as tf import tensorflow_datasets as tfds  import ciclo from ciclo.logging import Logs from ciclo.types import Batch, S from ciclo.timetracking import Elapsed from ciclo.loops.loop import LoopCallbackBase from ciclo.callbacks import LoopState, CallbackOutput  import wandb from wandb_addons.ciclo import WandbLogger In\u00a0[\u00a0]: Copied! <pre>wandb.init(project=\"ciclo-integration\", entity=\"geekyrakshit\", job_type=\"test\")\n</pre> wandb.init(project=\"ciclo-integration\", entity=\"geekyrakshit\", job_type=\"test\") In\u00a0[\u00a0]: Copied! <pre>batch_size = 32\ntotal_samples = 32 * 100\ntotal_steps = total_samples // batch_size\nsteps_per_epoch = total_steps // 10\ntest_steps = 10\n</pre> batch_size = 32 total_samples = 32 * 100 total_steps = total_samples // batch_size steps_per_epoch = total_steps // 10 test_steps = 10 In\u00a0[\u00a0]: Copied! <pre># load the MNIST dataset\nds_train: tf.data.Dataset = tfds.load(\"mnist\", split=\"train\", shuffle_files=True)\nds_train = ds_train.map(lambda x: (x[\"image\"], x[\"label\"]))\nds_train = ds_train.repeat().shuffle(1024).batch(batch_size).prefetch(1)\nds_test: tf.data.Dataset = tfds.load(\"mnist\", split=\"test\")\nds_test = ds_test.map(lambda x: (x[\"image\"], x[\"label\"]))  # .take(10)\nds_test = ds_test.batch(32, drop_remainder=True).prefetch(1)\n</pre> # load the MNIST dataset ds_train: tf.data.Dataset = tfds.load(\"mnist\", split=\"train\", shuffle_files=True) ds_train = ds_train.map(lambda x: (x[\"image\"], x[\"label\"])) ds_train = ds_train.repeat().shuffle(1024).batch(batch_size).prefetch(1) ds_test: tf.data.Dataset = tfds.load(\"mnist\", split=\"test\") ds_test = ds_test.map(lambda x: (x[\"image\"], x[\"label\"]))  # .take(10) ds_test = ds_test.batch(32, drop_remainder=True).prefetch(1) In\u00a0[\u00a0]: Copied! <pre># Define model\nclass Linear(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        x = x / 255.0\n        x = x.reshape((x.shape[0], -1))  # flatten\n        x = nn.Dense(features=10)(x)\n        return x\n</pre> # Define model class Linear(nn.Module):     @nn.compact     def __call__(self, x):         x = x / 255.0         x = x.reshape((x.shape[0], -1))  # flatten         x = nn.Dense(features=10)(x)         return x In\u00a0[\u00a0]: Copied! <pre># Initialize state\nmodel = Linear()\nstate = ciclo.create_flax_state(\n    model,\n    inputs=jnp.empty((1, 28, 28, 1)),\n    tx=optax.adamw(1e-3),\n    losses={\"loss\": jm.losses.Crossentropy()},\n    metrics={\"accuracy\": jm.metrics.Accuracy()},\n    strategy=\"jit\",\n)\n</pre> # Initialize state model = Linear() state = ciclo.create_flax_state(     model,     inputs=jnp.empty((1, 28, 28, 1)),     tx=optax.adamw(1e-3),     losses={\"loss\": jm.losses.Crossentropy()},     metrics={\"accuracy\": jm.metrics.Accuracy()},     strategy=\"jit\", ) In\u00a0[\u00a0]: Copied! <pre>state, history, _ = ciclo.train_loop(\n    state,\n    ds_train.as_numpy_iterator(),\n    callbacks=[\n        ciclo.keras_bar(total=total_steps),\n        ciclo.checkpoint(\n            f\"logdir/checkpoint/{int(time())}\",\n            monitor=\"accuracy_test\",\n            mode=\"max\",\n        ),\n        WandbLogger(),\n    ],\n    test_dataset=lambda: ds_test.as_numpy_iterator(),\n    epoch_duration=steps_per_epoch,\n    test_duration=test_steps,\n    stop=total_steps,\n)\n</pre> state, history, _ = ciclo.train_loop(     state,     ds_train.as_numpy_iterator(),     callbacks=[         ciclo.keras_bar(total=total_steps),         ciclo.checkpoint(             f\"logdir/checkpoint/{int(time())}\",             monitor=\"accuracy_test\",             mode=\"max\",         ),         WandbLogger(),     ],     test_dataset=lambda: ds_test.as_numpy_iterator(),     epoch_duration=steps_per_epoch,     test_duration=test_steps,     stop=total_steps, ) In\u00a0[\u00a0]: Copied! <pre>wandb.finish()\n</pre> wandb.finish()"},{"location":"dataset/dataset_loading/","title":"Weights &amp; Biases Dataloader","text":"<p>A set of utilities for easily accessing datasets for various machine learning tasks using Weights &amp; Biases artifacts.</p>"},{"location":"dataset/dataset_loading/#wandb_addons.dataset.dataset_builder.WandbDatasetBuilder","title":"<code>WandbDatasetBuilder</code>","text":"<p>             Bases: <code>GeneratorBasedBuilder</code></p> <p>An abstract class for Dataset builder that enables building a dataset and upload it as a Weights &amp; Biases Artifact. It expects subclasses to override the following functions:</p> <ul> <li> <p><code>_split_generators</code> to return a dict of splits, generators.</p> </li> <li> <p><code>_generate_examples</code> to return a generator or an iterator corresponding to the split.</p> </li> </ul> <p>Note</p> <p>Note that this process is alternative to the dataset preparation process using tfds module described here. The dataset registered and uploaded using both approaches is easily consumable using the fuction <code>load_dataset</code>.</p> <p>Example Artifacts</p> <ul> <li>\ud83d\udc12 Monkey Dataset.</li> </ul> <p>Usage:</p> <pre><code>import os\nfrom glob import glob\nfrom typing import Any, Mapping, Optional, Union\n\nfrom etils import epath\nimport tensorflow_datasets as tfds\n\nimport wandb\nfrom wandb_addons.dataset import WandbDatasetBuilder\n\n\nclass MonkeyDatasetBuilder(WandbDatasetBuilder):\n    def __init__(\n        self,\n        *,\n        name: str,\n        dataset_path: str,\n        features: tfds.features.FeatureConnector,\n        upload_raw_dataset: bool = True,\n        config: Union[None, str, tfds.core.BuilderConfig] = None,\n        data_dir: Optional[epath.PathLike] = None,\n        description: Optional[str] = None,\n        release_notes: Optional[Mapping[str, str]] = None,\n        homepage: Optional[str] = None,\n        file_format: Optional[Union[str, tfds.core.FileFormat]] = None,\n        disable_shuffling: Optional[bool] = False,\n        **kwargs: Any,\n    ):\n        super().__init__(\n            name=name,\n            dataset_path=dataset_path,\n            features=features,\n            upload_raw_dataset=upload_raw_dataset,\n            config=config,\n            description=description,\n            data_dir=data_dir,\n            release_notes=release_notes,\n            homepage=homepage,\n            file_format=file_format,\n            disable_shuffling=disable_shuffling,\n        )\n\n    def _split_generators(self, dl_manager: tfds.download.DownloadManager):\n        return {\n            \"train\": self._generate_examples(\n                os.path.join(self.dataset_path, \"training\", \"training\")\n            ),\n            \"val\": self._generate_examples(\n                os.path.join(self.dataset_path, \"validation\", \"validation\")\n            ),\n        }\n\n    def _generate_examples(self, path):\n        image_paths = glob(os.path.join(path, \"*\", \"*.jpg\"))\n        for image_path in image_paths:\n            label = _CLASS_LABELS[int(image_path.split(\"/\")[-2][-1])]\n            yield image_path, {\n                \"image\": image_path,\n                \"label\": label,\n            }\n\n\nif __name__ == \"__main__\":\n    wandb.init(project=\"artifact-accessor\", entity=\"geekyrakshit\")\n\n    builder = MonkeyDatasetBuilder(\n        name=\"monkey_dataset\",\n        dataset_path=\"path/to/my/datase\",\n        features=tfds.features.FeaturesDict(\n            {\n                \"image\": tfds.features.Image(shape=(None, None, 3)),\n                \"label\": tfds.features.ClassLabel(names=_CLASS_LABELS),\n            }\n        ),\n        data_dir=\"build_dir/\",\n        description=_DESCRIPTION,\n    )\n\n    builder.build_and_upload(create_visualizations=True)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>A human-readable name for this artifact, which is how you can identify this artifact in the UI or reference it in <code>use_artifact</code> calls. Names can contain letters, numbers, underscores, hyphens, and dots. The name must be unique across a project.</p> required <code>dataset_path</code> <code>str</code> <p>Path to the dataset.</p> required <code>features</code> <code>FeatureConnector</code> <p>The dataset feature types. Refer to the <code>tfds.features</code> module for more information.</p> required <code>upload_raw_dataset</code> <code>Optional[bool]</code> <p>Whether to upload the raw dataset to Weights &amp; Biases artifacts as well or not. If set to <code>True</code>, the dataset builder would upload the raw dataset besides the built dataset, as different versions of the same artifact; with the raw dataset being the lower version.</p> <code>False</code> <code>config</code> <code>Union[None, str, BuilderConfig]</code> <p>Dataset configuration.</p> <code>None</code> <code>data_dir</code> <code>Optional[PathLike]</code> <p>The directory where the dataset will be built.</p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>Description of the dataset as a valid markdown string.</p> <code>None</code> <code>release_notes</code> <code>Optional[Mapping[str, str]]</code> <p>Release notes.</p> <code>None</code> <code>homepage</code> <code>Optional[str]</code> <p>Homepage of the dataset.</p> <code>None</code> <code>file_format</code> <code>Optional[Union[str, FileFormat]]</code> <p>EXPERIMENTAL, may change at any time; Format of the record files in which dataset will be read/written to. If <code>None</code>, defaults to <code>tfrecord</code>.</p> <code>None</code> <code>disable_shuffling</code> <code>Optional[bool]</code> <p>Disable shuffling of the dataset order.</p> <code>True</code> Source code in <code>wandb_addons/dataset/dataset_builder.py</code> <pre><code>class WandbDatasetBuilder(tfds.core.GeneratorBasedBuilder):\n    \"\"\"An abstract class for Dataset builder that enables building a dataset and upload it as a\n    [Weights &amp; Biases Artifact](https://docs.wandb.ai/guides/artifacts). It expects subclasses\n    to override the following functions:\n\n    - **`_split_generators`** to return a dict of splits, generators.\n\n    - **`_generate_examples`** to return a generator or an iterator corresponding to the split.\n\n    !!! note \"Note\"\n        Note that this process is alternative to the dataset preparation process using tfds module\n        described [here](../dataset_preparation). The dataset registered and uploaded using both\n        approaches is easily consumable using the fuction\n        [`load_dataset`](./#wandb_addons.dataset.dataset_loading.load_dataset).\n\n    !!! example \"Example Artifacts\"\n        - [\ud83d\udc12 Monkey Dataset](https://wandb.ai/geekyrakshit/artifact-accessor/artifacts/dataset/monkey_dataset).\n\n    **Usage:**\n\n    ```python\n    import os\n    from glob import glob\n    from typing import Any, Mapping, Optional, Union\n\n    from etils import epath\n    import tensorflow_datasets as tfds\n\n    import wandb\n    from wandb_addons.dataset import WandbDatasetBuilder\n\n\n    class MonkeyDatasetBuilder(WandbDatasetBuilder):\n        def __init__(\n            self,\n            *,\n            name: str,\n            dataset_path: str,\n            features: tfds.features.FeatureConnector,\n            upload_raw_dataset: bool = True,\n            config: Union[None, str, tfds.core.BuilderConfig] = None,\n            data_dir: Optional[epath.PathLike] = None,\n            description: Optional[str] = None,\n            release_notes: Optional[Mapping[str, str]] = None,\n            homepage: Optional[str] = None,\n            file_format: Optional[Union[str, tfds.core.FileFormat]] = None,\n            disable_shuffling: Optional[bool] = False,\n            **kwargs: Any,\n        ):\n            super().__init__(\n                name=name,\n                dataset_path=dataset_path,\n                features=features,\n                upload_raw_dataset=upload_raw_dataset,\n                config=config,\n                description=description,\n                data_dir=data_dir,\n                release_notes=release_notes,\n                homepage=homepage,\n                file_format=file_format,\n                disable_shuffling=disable_shuffling,\n            )\n\n        def _split_generators(self, dl_manager: tfds.download.DownloadManager):\n            return {\n                \"train\": self._generate_examples(\n                    os.path.join(self.dataset_path, \"training\", \"training\")\n                ),\n                \"val\": self._generate_examples(\n                    os.path.join(self.dataset_path, \"validation\", \"validation\")\n                ),\n            }\n\n        def _generate_examples(self, path):\n            image_paths = glob(os.path.join(path, \"*\", \"*.jpg\"))\n            for image_path in image_paths:\n                label = _CLASS_LABELS[int(image_path.split(\"/\")[-2][-1])]\n                yield image_path, {\n                    \"image\": image_path,\n                    \"label\": label,\n                }\n\n\n    if __name__ == \"__main__\":\n        wandb.init(project=\"artifact-accessor\", entity=\"geekyrakshit\")\n\n        builder = MonkeyDatasetBuilder(\n            name=\"monkey_dataset\",\n            dataset_path=\"path/to/my/datase\",\n            features=tfds.features.FeaturesDict(\n                {\n                    \"image\": tfds.features.Image(shape=(None, None, 3)),\n                    \"label\": tfds.features.ClassLabel(names=_CLASS_LABELS),\n                }\n            ),\n            data_dir=\"build_dir/\",\n            description=_DESCRIPTION,\n        )\n\n        builder.build_and_upload(create_visualizations=True)\n    ```\n\n    Args:\n        name (str): A human-readable name for this artifact, which is how you can identify this\n            artifact in the UI or reference it in\n            [`use_artifact`](https://docs.wandb.ai/ref/python/run#use_artifact) calls. Names can\n            contain letters, numbers, underscores, hyphens, and dots. The name must be unique\n            across a project.\n        dataset_path (str): Path to the dataset.\n        features (tfds.features.FeatureConnector): The dataset feature types. Refer to the\n            [`tfds.features`](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/)\n            module for more information.\n        upload_raw_dataset (Optional[bool]): Whether to upload the raw dataset to Weights &amp; Biases\n            artifacts as well or not. If set to `True`, the dataset builder would upload the raw\n            dataset besides the built dataset, as different versions of the same artifact; with the\n            raw dataset being the lower version.\n        config (Union[None, str, tfds.core.BuilderConfig]): Dataset configuration.\n        data_dir (Optional[epath.PathLike]): The directory where the dataset will be built.\n        description (Optional[str]): Description of the dataset as a valid markdown string.\n        release_notes (Optional[Mapping[str, str]]): Release notes.\n        homepage (Optional[str]): Homepage of the dataset.\n        file_format (Optional[Union[str, tfds.core.FileFormat]]): **EXPERIMENTAL**, may change at any\n            time; Format of the record files in which dataset will be read/written to. If `None`,\n            defaults to `tfrecord`.\n        disable_shuffling (Optional[bool]): Disable shuffling of the dataset order.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        name: str,\n        dataset_path: str,\n        features: tfds.features.FeatureConnector,\n        upload_raw_dataset: Optional[bool] = False,\n        config: Union[None, str, tfds.core.BuilderConfig] = None,\n        data_dir: Optional[epath.PathLike] = None,\n        description: Optional[str] = None,\n        release_notes: Optional[Mapping[str, str]] = None,\n        homepage: Optional[str] = None,\n        file_format: Optional[Union[str, tfds.core.FileFormat]] = None,\n        disable_shuffling: Optional[bool] = True,\n        **kwargs: Any,\n    ):\n        if wandb.run is None:\n            raise wandb.Error(\n                \"You must call `wandb.init()` before instantiating a subclass of `WandbDatasetBuilder`\"\n            )\n\n        self.name = name\n        self.dataset_path = dataset_path\n        self.upload_raw_dataset = upload_raw_dataset\n        self.VERSION = self._get_version()\n        self.RELEASE_NOTES = release_notes\n        if config:\n            if isinstance(config, str):\n                config = tfds.core.BuilderConfig(\n                    name=config, version=self.VERSION, release_notes=release_notes\n                )\n            self.BUILDER_CONFIGS = [config]\n        self._feature_spec = features\n        self._description = (\n            description or \"Dataset built without a DatasetBuilder class.\"\n        )\n        self._homepage = homepage\n        self._disable_shuffling = disable_shuffling\n\n        self._initialize_wandb_artifact()\n\n        super().__init__(\n            data_dir=data_dir,\n            config=config,\n            version=self.VERSION,\n            file_format=file_format,\n            **kwargs,\n        )\n\n    def _initialize_wandb_artifact(self):\n        metadata = {\n            \"description\": self._description,\n            \"release-notes\": self.RELEASE_NOTES,\n            \"homepage\": self._homepage,\n        }\n        if self.upload_raw_dataset:\n            self._wandb_raw_artifact = wandb.Artifact(\n                name=self.name,\n                type=\"dataset\",\n                description=self._description,\n                metadata=metadata,\n            )\n        self._wandb_build_artifact = wandb.Artifact(\n            name=self.name,\n            type=\"dataset\",\n            description=self._description,\n            metadata=metadata,\n        )\n\n    def _get_version(self) -&gt; tfds.core.utils.Version:\n        try:\n            api = wandb.Api()\n            versions = api.artifact_versions(\n                type_name=\"dataset\",\n                name=f\"{wandb.run.entity}/{wandb.run.project}/{self.name}\",\n            )\n            version = int(next(versions).source_version[1:])\n            version = version + 1 if self.upload_raw_dataset else version\n            return str(version) + \".0.0\"\n        except wandb.errors.CommError:\n            version = 1 if self.upload_raw_dataset else 0\n            return tfds.core.utils.Version(str(version) + \".0.0\")\n\n    def _info(self) -&gt; tfds.core.DatasetInfo:\n        return tfds.core.DatasetInfo(\n            builder=self,\n            description=self._description,\n            features=self._feature_spec,\n            homepage=self._homepage,\n            disable_shuffling=self._disable_shuffling,\n        )\n\n    def _create_report(self):\n        report = wr.Report(project=wandb.run.project)\n\n        dataset_splits = flatten_nested_dictionaries(self.info.splits).keys()\n        scalar_panels = [\n            wr.ScalarChart(\n                title=f\"{split}/num_examples\", metric=f\"{split}/num_examples\"\n            )\n            for split in dataset_splits\n        ]\n        scalar_panels += [\n            wr.ScalarChart(title=f\"{split}/num_shards\", metric=f\"{split}/num_shards\")\n            for split in dataset_splits\n        ]\n\n        report.title = f\"Dataset: {self.name}\"\n\n        report.blocks = [\n            wr.MarkdownBlock(\n                text=f\"**Disclaimer:** This report was generated automatically.\"\n            ),\n            wr.H1(\"Description\"),\n            wr.MarkdownBlock(text=self._description),\n            wr.MarkdownBlock(\"\\n\"),\n        ]\n\n        if self._homepage is not None:\n            report.blocks += [\n                wr.MarkdownBlock(text=f\"**Homepage:** {self._homepage}\"),\n                wr.MarkdownBlock(\"\\n\"),\n            ]\n\n        report.blocks += [\n            wr.MarkdownBlock(\n                f\"\"\"\n            ```python\n            import from wandb_addons.dataset import load_dataset\n\n            datasets, dataset_builder_info = load_dataset(\"{wandb.run.entity}/{wandb.run.project}/{self.name}:tfrecord\")\n            ```\n            \"\"\"\n            ),\n            wr.MarkdownBlock(\"\\n\"),\n        ]\n\n        report.blocks += [\n            wr.PanelGrid(\n                runsets=[wr.Runset(project=wandb.run.project, entity=wandb.run.entity)],\n                panels=scalar_panels\n                + [\n                    wr.WeavePanelSummaryTable(table_name=f\"{self.name}-Table\"),\n                    wr.WeavePanelArtifact(self.name),\n                ],\n            )\n        ]\n\n        report.save()\n\n    def build_and_upload(\n        self,\n        create_visualizations: bool = False,\n        max_visualizations_per_split: Optional[int] = None,\n    ):\n        \"\"\"Build and prepare the dataset for loading and uploads as a\n        [Weights &amp; Biases Artifact](https://docs.wandb.ai/guides/artifacts). This function also\n        creates a Weights &amp; Biases reports that contains the dataset description, visualizations\n        and all additional metadata logged to Weights &amp; Biases.\n\n        !!! example \"Sample Auto-generated Report\"\n            [\ud83d\udc12 Dataset: monkey-dataset](https://wandb.ai/geekyrakshit/artifact-accessor/reports/Dataset-monkey-dataset--Vmlldzo0MjgxNTAz)\n        Args:\n            create_visualizations (bool): Automatically parse the dataset and visualize using a\n                [Weights &amp; Biase Table](https://docs.wandb.ai/guides/data-vis).\n            max_visualizations_per_split (Optional[int]): Maximum number of visualizations per\n                split to be visualized in WandB Table. By default, the whole dataset is visualized.\n        \"\"\"\n        super().download_and_prepare()\n\n        if create_visualizations:\n            table_creator = TableCreator(\n                dataset_builder=self,\n                dataset_info=self.info,\n                max_visualizations_per_split=max_visualizations_per_split,\n            )\n            table_creator.populate_table()\n            table_creator.log(dataset_name=self.name)\n\n        if self.upload_raw_dataset:\n            self._wandb_raw_artifact.add_dir(self.dataset_path)\n            wandb.log_artifact(self._wandb_raw_artifact, aliases=[\"raw\"])\n        self._wandb_build_artifact.add_dir(self.data_dir)\n        wandb.log_artifact(self._wandb_build_artifact, aliases=[\"tfrecord\"])\n\n        self._create_report()\n</code></pre>"},{"location":"dataset/dataset_loading/#wandb_addons.dataset.dataset_builder.WandbDatasetBuilder.build_and_upload","title":"<code>build_and_upload(create_visualizations=False, max_visualizations_per_split=None)</code>","text":"<p>Build and prepare the dataset for loading and uploads as a Weights &amp; Biases Artifact. This function also creates a Weights &amp; Biases reports that contains the dataset description, visualizations and all additional metadata logged to Weights &amp; Biases.</p> <p>Sample Auto-generated Report</p> <p>\ud83d\udc12 Dataset: monkey-dataset</p> <p>Args:     create_visualizations (bool): Automatically parse the dataset and visualize using a         Weights &amp; Biase Table.     max_visualizations_per_split (Optional[int]): Maximum number of visualizations per         split to be visualized in WandB Table. By default, the whole dataset is visualized.</p> Source code in <code>wandb_addons/dataset/dataset_builder.py</code> <pre><code>def build_and_upload(\n    self,\n    create_visualizations: bool = False,\n    max_visualizations_per_split: Optional[int] = None,\n):\n    \"\"\"Build and prepare the dataset for loading and uploads as a\n    [Weights &amp; Biases Artifact](https://docs.wandb.ai/guides/artifacts). This function also\n    creates a Weights &amp; Biases reports that contains the dataset description, visualizations\n    and all additional metadata logged to Weights &amp; Biases.\n\n    !!! example \"Sample Auto-generated Report\"\n        [\ud83d\udc12 Dataset: monkey-dataset](https://wandb.ai/geekyrakshit/artifact-accessor/reports/Dataset-monkey-dataset--Vmlldzo0MjgxNTAz)\n    Args:\n        create_visualizations (bool): Automatically parse the dataset and visualize using a\n            [Weights &amp; Biase Table](https://docs.wandb.ai/guides/data-vis).\n        max_visualizations_per_split (Optional[int]): Maximum number of visualizations per\n            split to be visualized in WandB Table. By default, the whole dataset is visualized.\n    \"\"\"\n    super().download_and_prepare()\n\n    if create_visualizations:\n        table_creator = TableCreator(\n            dataset_builder=self,\n            dataset_info=self.info,\n            max_visualizations_per_split=max_visualizations_per_split,\n        )\n        table_creator.populate_table()\n        table_creator.log(dataset_name=self.name)\n\n    if self.upload_raw_dataset:\n        self._wandb_raw_artifact.add_dir(self.dataset_path)\n        wandb.log_artifact(self._wandb_raw_artifact, aliases=[\"raw\"])\n    self._wandb_build_artifact.add_dir(self.data_dir)\n    wandb.log_artifact(self._wandb_build_artifact, aliases=[\"tfrecord\"])\n\n    self._create_report()\n</code></pre>"},{"location":"dataset/dataset_loading/#wandb_addons.dataset.dataset_upload.upload_dataset","title":"<code>upload_dataset(dataset_name, dataset_path, aliases=None, upload_tfrecords=True, quiet=False)</code>","text":"<p>Upload and register a dataset with a TFDS module or a TFDS builder script as a Weights &amp; Biases artifact. This function would verify if a TFDS build/registration is possible with the current specified dataset path and upload it as a Weights &amp; Biases artifact.</p> <p>Check this guide for preparing a dataset for registering in on Weights &amp; Biases</p> <ul> <li>Preparing the Dataset with Builder Script or TFDS Module.</li> </ul> <p>Usage:</p> <pre><code>import wandb\nfrom wandb_addons.dataset import upload_dataset\n\n# Initialize a W&amp;B Run\nwandb.init(project=\"my-awesome-project\", job_type=\"upload_dataset\")\n\n# Note that we should set our dataset name as the name of the artifact\nupload_dataset(dataset_name=\"my_awesome_dataset\", dataset_path=\"./my/dataset/path\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>Name of the dataset. This name should follow the PEP8 package and module name convenmtions.</p> required <code>dataset_path</code> <code>str</code> <p>Path to the dataset.</p> required <code>aliases</code> <code>Optional[List[str]]</code> <p>Aliases to apply to this artifact. If the parameter <code>upload_tfrecords</code> is set to <code>True</code>, then the alias <code>\"tfrecord\"</code> is automatically appended to the provided list of aliases. Otherwise, the alias <code>\"tfds-module\"</code> is automatically appended to the provided list of aliases.</p> <code>None</code> <code>upload_tfrecords</code> <code>bool</code> <p>Upload dataset as TFRecords or not. If set to <code>False</code>, then the dataset is uploaded with a TFDS module, otherwise only the TFRrecords and the necessary metadata files would be uploaded.</p> <code>True</code> <code>quiet</code> <code>bool</code> <p>Whether to suppress the output of dataset build process or not.</p> <code>False</code> Source code in <code>wandb_addons/dataset/dataset_upload.py</code> <pre><code>def upload_dataset(\n    dataset_name: str,\n    dataset_path: str,\n    aliases: Optional[List[str]] = None,\n    upload_tfrecords: bool = True,\n    quiet: bool = False,\n):\n    \"\"\"Upload and register a dataset with a TFDS module or a TFDS builder script as a\n    Weights &amp; Biases artifact. This function would verify if a TFDS build/registration is possible\n    with the current specified dataset path and upload it as a Weights &amp; Biases artifact.\n\n    !!! example \"Check this guide for preparing a dataset for registering in on Weights &amp; Biases\"\n        - [Preparing the Dataset with Builder Script or TFDS Module](../dataset_preparation).\n\n    **Usage:**\n\n    ```python\n    import wandb\n    from wandb_addons.dataset import upload_dataset\n\n    # Initialize a W&amp;B Run\n    wandb.init(project=\"my-awesome-project\", job_type=\"upload_dataset\")\n\n    # Note that we should set our dataset name as the name of the artifact\n    upload_dataset(dataset_name=\"my_awesome_dataset\", dataset_path=\"./my/dataset/path\")\n    ```\n\n    Args:\n        dataset_name (str): Name of the dataset. This name should follow the\n            [PEP8 package and module name convenmtions](https://peps.python.org/pep-0008/#package-and-module-names).\n        dataset_path (str): Path to the dataset.\n        aliases (Optional[List[str]]): Aliases to apply to this artifact. If the parameter `upload_tfrecords` is set\n            to `True`, then the alias `\"tfrecord\"` is automatically appended to the provided list of aliases.\n            Otherwise, the alias `\"tfds-module\"` is automatically appended to the provided list of aliases.\n        upload_tfrecords (bool): Upload dataset as TFRecords or not. If set to `False`, then the dataset is uploaded\n            with a TFDS module, otherwise only the TFRrecords and the necessary metadata files would be uploaded.\n        quiet (bool): Whether to suppress the output of dataset build process or not.\n    \"\"\"\n    is_tfds_module_structure_valid = _verify_and_create_tfds_module_structure(\n        dataset_name, dataset_path\n    )\n    if not is_tfds_module_structure_valid:\n        wandb.termerror(\n            f\"Unable to generate or detect valid TFDS module at {dataset_path}\"\n        )\n    else:\n        wandb.termlog(f\"Verified TFDS module at {dataset_path}\")\n        _build_from_tfds_module(dataset_name, dataset_path, quiet=quiet)\n        if upload_tfrecords:\n            _upload_tfrecords(dataset_name, \"dataset\", aliases=aliases + [\"tfrecord\"])\n        else:\n            upload_wandb_artifact(\n                name=dataset_name,\n                artifact_type=\"dataset\",\n                path=dataset_path,\n                aliases=aliases + [\"tfds-module\"],\n            )\n</code></pre>"},{"location":"dataset/dataset_loading/#wandb_addons.dataset.dataset_loading.load_dataset","title":"<code>load_dataset(artifact_address, artifact_type='dataset', remove_redundant_data_files=True, quiet=False)</code>","text":"<p>Load a dataset from a wandb artifact.</p> <p>Using this function you can load a dataset hosted as a wandb artifact in a single line of code, and use our powerful data processing methods to quickly get your dataset ready for training in a deep learning model.</p> <p>Usage:</p> <pre><code>from wandb_addons.dataset import load_dataset\n\ndatasets, dataset_builder_info = load_dataset(\"geekyrakshit/artifact-accessor/monkey_species:v0\")\n</code></pre> <p>Example notebooks:</p> <ul> <li>\ud83d\udd25 Data Loading with WandB Artifacts \ud83e\ude84\ud83d\udc1d.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>artifact_address</code> <code>str</code> <p>A human-readable name for the artifact, which is how you can identify the artifact in the UI or reference it in <code>use_artifact</code> calls. Names can contain letters, numbers, underscores, hyphens, and dots. The name must be unique across a project.</p> required <code>artifact_type</code> <code>str</code> <p>The type of the artifact, which is used to organize and differentiate artifacts. Common typesCinclude dataset or model, but you can use any string containing letters, numbers, underscores, hyphens, and dots.</p> <code>'dataset'</code> <code>remove_redundant_data_files</code> <code>bool</code> <p>Whether to remove the redundant data files from the artifacts directory after building the tfrecord dataset.</p> <code>True</code> <code>quiet</code> <code>bool</code> <p>Whether to suppress the output of dataset build process or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[Dict[str, Dataset], DatasetInfo]</code> <p>A tuple of dictionary Dictionary mapping split aliases to the respective TensorFlow Prefetched dataset objects and the <code>tfds.core.DatasetInfo</code> that documents datasets, including its name, version, and features.</p> Source code in <code>wandb_addons/dataset/dataset_loading.py</code> <pre><code>def load_dataset(\n    artifact_address: str,\n    artifact_type: Optional[str] = \"dataset\",\n    remove_redundant_data_files: bool = True,\n    quiet: bool = False,\n) -&gt; Tuple[Dict[str, _DATASET_TYPE], DatasetInfo]:\n    \"\"\"Load a dataset from a [wandb artifact](https://docs.wandb.ai/guides/artifacts).\n\n    Using this function you can load a dataset hosted as a\n    [wandb artifact](https://docs.wandb.ai/guides/artifacts) in a single line of code,\n    and use our powerful data processing methods to quickly get your dataset ready for\n    training in a deep learning model.\n\n    **Usage:**\n\n    ```python\n    from wandb_addons.dataset import load_dataset\n\n    datasets, dataset_builder_info = load_dataset(\"geekyrakshit/artifact-accessor/monkey_species:v0\")\n    ```\n\n    !!! example \"Example notebooks:\"\n        - [\ud83d\udd25 Data Loading with WandB Artifacts \ud83e\ude84\ud83d\udc1d](../examples/load_dataset).\n\n    Args:\n        artifact_address (str): A human-readable name for the artifact, which is how you can\n            identify the artifact in the UI or reference it in\n            [`use_artifact`](https://docs.wandb.ai/ref/python/run#use_artifact) calls. Names can\n            contain letters, numbers, underscores, hyphens, and dots. The name must be unique across\n            a project.\n        artifact_type (str): The type of the artifact, which is used to organize and differentiate\n            artifacts. Common typesCinclude dataset or model, but you can use any string containing\n            letters, numbers, underscores, hyphens, and dots.\n        remove_redundant_data_files (bool): Whether to remove the redundant data files from the\n            artifacts directory after building the tfrecord dataset.\n        quiet (bool): Whether to suppress the output of dataset build process or not.\n\n    Returns:\n        (Tuple[Dict[str, tf.data.Dataset], DatasetInfo]): A tuple of dictionary Dictionary mapping\n            split aliases to the respective\n            [TensorFlow Prefetched dataset](https://www.tensorflow.org/guide/data_performance#prefetching)\n            objects and the\n            [`tfds.core.DatasetInfo`](https://www.tensorflow.org/datasets/api_docs/python/tfds/core/DatasetInfo)\n            that documents datasets, including its name, version, and features.\n    \"\"\"\n    artifact_dir = fetch_wandb_artifact(\n        artifact_address=artifact_address, artifact_type=artifact_type\n    )\n    artifact_dir = _change_artifact_dir_name(artifact_dir)\n    dataset_name = _get_dataset_name_from_artifact_address(artifact_address)\n\n    try:\n        dataset_builder = tfds.builder_from_directory(artifact_dir)\n        dataset_builder.download_and_prepare()\n        dataset_splits, dataset_builder_info = _build_datasets(dataset_builder)\n    except Exception as e:\n        wandb.termwarn(\n            \"Failed to detect TFRecords in the artifact. Attempting to build tfrecords\"\n        )\n        dataset_splits, dataset_builder_info = _load_dataset_from_tfds_module(\n            artifact_address,\n            artifact_dir,\n            dataset_name,\n            remove_redundant_data_files,\n            quiet,\n        )\n\n    return dataset_splits, dataset_builder_info\n</code></pre>"},{"location":"dataset/dataset_preparation/","title":"Preparing the Dataset with Builder Script or TFDS Module","text":"<ol> <li> <p>For this example, we will be using the 10 Monkey Species dataset.</p> <p>The directory structure of this dataset is as follows: <pre><code>monkey_species_dataset\n|-- __init__.py\n|-- monkey_labels.txt\n|-- training\n|   `-- training\n|       |-- n0\n|       |-- n1\n|       |-- n2\n|       |-- n3\n|       |-- n4\n|       |-- n5\n|       |-- n6\n|       |-- n7\n|       |-- n8\n|       `-- n9\n|           |-- n9151jpg\n|           `-- n9160.png\n`-- validation\n    `-- validation\n        |-- n0\n        |-- n1\n        |-- n2\n        |-- n3\n        |-- n4\n        |-- n5\n        |-- n6\n        |-- n7\n        |-- n8\n        `-- n9\n</code></pre></p> </li> <li> <p>Next let us setup <code>wandb-addons</code>. We can do this using the following command:     <pre><code>git clone https://github.com/soumik12345/wandb-addons\npip install -q wandb-addons[dataset]\n</code></pre></p> <p>This would install <code>wandb-addons</code> and also optional dependencies including <code>tensorflow</code> and <code>tfds-nightly</code>, the nightly release of <code>tensorflow-datasets</code>.</p> </li> <li> <p>Now, let us <code>cd</code> into the <code>monkey_species_dataset</code> directory and initialize the tensorflow datasets template files, which would be used for interpreting and registering features from our dataset.     <pre><code>cd monkey_species_dataset\n# Create `monkey_species_dataset/monkey_species` template files.\ntfds new monkey_species \n</code></pre></p> <p>This would create a directory with the following structure inside the directory <code>monkey_species_dataset</code>:</p> <pre><code>monkey_species\n   |-- CITATIONS.bib\n   |-- README.md\n   |-- TAGS.txt\n   |-- __init__.py\n   |-- checksums.tsv\n   |-- dummy_data\n   |   `-- TODO-add_fake_data_in_this_directory.txt\n   |-- monkey_species_dataset_builder.py\n   `-- monkey_species_dataset_builder_test.py\n</code></pre> <p>The complete directory structure of <code>monkey_species_dataset</code> at this point is going to something like:</p> <pre><code>monkey_species_dataset\n|-- __init__.py\n|-- monkey_labels.txt\n|-- monkey_species\n|   |-- CITATIONS.bib\n|   |-- README.md\n|   |-- TAGS.txt\n|   |-- __init__.py\n|   |-- checksums.tsv\n|   |-- dummy_data\n|   |   `-- TODO-add_fake_data_in_this_directory.txt\n|   |-- monkey_species_dataset_builder.py\n|   `-- monkey_species_dataset_builder_test.py\n|-- training\n|   `-- training\n|       |-- n0\n|       |-- n1\n|       |-- n2\n|       |-- n3\n|       |-- n4\n|       |-- n5\n|       |-- n6\n|       |-- n7\n|       |-- n8\n|       `-- n9\n|           |-- n9151jpg\n|           `-- n9160.png\n`-- validation\n    `-- validation\n        |-- n0\n        |-- n1\n        |-- n2\n        |-- n3\n        |-- n4\n        |-- n5\n        |-- n6\n        |-- n7\n        |-- n8\n        `-- n9\n</code></pre> <p>Note</p> <p>The name with which you initialize the <code>tfds new</code> command would be used as the <code>name</code> of your dataset.</p> </li> <li> <p>Now we will write our dataset builder in the file <code>monkey_species_dataset/monkey_species/monkey_species_dataset_builder.py</code>. This logic for writing a dataset builder is exactly similar to that of creating the same for HuggingFace Datasets or a vanilla TensorFlow dataset.</p> <p>Note</p> <p>Alternative to step 3, you could also simply inclide a builder script <code>&lt;dataset_name&gt;.py</code> into the <code>monkey_species_dataset</code> directory, instead of creating the TFDS module.</p> <p>You can refer to the following examples</p> <ul> <li>An example of a dataset with a custom builder script</li> <li>An example of a dataset with a TFDS module</li> </ul> <p>You can refer to the following guides for writing builder scripts</p> <ul> <li>Writing custom datasets using <code>tfds</code>.</li> <li>HuggingFace: Create a dataset.</li> <li>HuggingFace: Create an Image Dataset.</li> </ul> </li> <li> <p>Now that our dataset is ready with the specifications for loading the features, we can upload it to our Weights &amp; Biases project as an artifact using the <code>upload_dataset</code> function, which would verify if the dataset build is successful or not before uploading the dataset.</p> <pre><code>import wandb\nfrom wandb_addons.dataset import upload_dataset\n\n# Initialize a W&amp;B Run\nwandb.init(project=\"my-awesome-project\", job_type=\"upload_dataset\") \n\n# Note that we should set our dataset name as the name of the artifact\nupload_dataset(name=\"my_awesome_dataset\", path=\"./my/dataset/path\", type=\"dataset\")\n</code></pre> <p>In order to load this dataset in your ML workflow you can simply use the <code>load_dataset</code> function:</p> <pre><code>from wandb_addons.dataset import load_dataset\n\ndatasets, dataset_builder_info = load_dataset(\"entity-name/my-awesome-project/my_awesome_dataset:v0\")\n</code></pre> <p>Note</p> <ul> <li>In the <code>upload_dataset</code> function by default convert a registered dataset to TFRecords (like this artifact). You can alternatively upload the dataset in its original state along with the added TFDS module containing the builder script by simply setting <code>upload_tfrecords</code> parameter to <code>False</code>.</li> <li>Note that this won't affect loading the dataset using <code>load_dataset</code>, dataset loading from artifacts would work as long as the artifact contains either the TFRecords or the original dataset with the TFDS module.</li> <li>The TFRecord artifact has to follow the specification specified in this guide. However, if you're using the <code>upload_dataset</code> function, you don't need to worry about this.</li> </ul> <p>You can take a look at this artifact that demonstrates the aforementioned directory structure and builder logic.</p> </li> </ol>"},{"location":"dataset/examples/load_dataset/","title":"\ud83d\udd25 Data Loading with WandB Artifacts \ud83e\ude84\ud83d\udc1d","text":"In\u00a0[1]: Copied! <pre>import tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom wandb_addons.dataset import load_dataset\n</pre> import tensorflow as tf import matplotlib.pyplot as plt from wandb_addons.dataset import load_dataset <pre>2023-04-30 12:34:52.474435: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n</pre> <p>Just pass in the artifact address of the dataset artifact, and we are all set. For detailed documentation of all parameters and option, refer to this page.</p> <p>Note: For loading and ingesting the dataset from a wandb artifact, its not compulsory to initialize a run. However, loading inside the context of a run has added advantages of tracking lineage of artifacts and ease of versioning.</p> In\u00a0[2]: Copied! <pre>datasets, dataset_builder_info = load_dataset(\n    \"geekyrakshit/artifact-accessor/monkey_dataset:v1\", quiet=True\n)\n</pre> datasets, dataset_builder_info = load_dataset(     \"geekyrakshit/artifact-accessor/monkey_dataset:v1\", quiet=True ) <pre>wandb: Downloading large artifact monkey_dataset:v1, 553.56MB. 8 files... \nwandb:   8 of 8 files downloaded.  \nDone. 0:0:0.9\nwandb: Building dataset for split: train...\n2023-04-30 12:34:56.083939: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\nwandb: Built dataset for split: train, num_shards: 4, num_examples: 1096\nwandb: Building dataset for split: val...\nwandb: Built dataset for split: val, num_shards: 1, num_examples: 272\n</pre> <p>Now we that we have created the TensorFlow datasets corresponding to the splits along with the general info of the dataset, we can verify them.</p> In\u00a0[3]: Copied! <pre>class_names = dataset_builder_info.features[\"label\"].names\n</pre> class_names = dataset_builder_info.features[\"label\"].names In\u00a0[4]: Copied! <pre>sample = next(iter(datasets[\"train\"]))\nplt.imshow(sample[\"image\"].numpy())\nlabel_name = class_names[sample[\"label\"].numpy()]\nplt.title(f\"Label: {label_name}\")\nplt.show()\n</pre> sample = next(iter(datasets[\"train\"])) plt.imshow(sample[\"image\"].numpy()) label_name = class_names[sample[\"label\"].numpy()] plt.title(f\"Label: {label_name}\") plt.show() In\u00a0[5]: Copied! <pre>sample = next(iter(datasets[\"val\"]))\nplt.imshow(sample[\"image\"].numpy())\nlabel_name = class_names[sample[\"label\"].numpy()]\nplt.title(f\"Label: {label_name}\")\nplt.show()\n</pre> sample = next(iter(datasets[\"val\"])) plt.imshow(sample[\"image\"].numpy()) label_name = class_names[sample[\"label\"].numpy()] plt.title(f\"Label: {label_name}\") plt.show() In\u00a0[6]: Copied! <pre>print(\"Train Dataset Cardinality:\", tf.data.experimental.cardinality(datasets[\"train\"]).numpy())\nprint(\"Validation Dataset Cardinality:\", tf.data.experimental.cardinality(datasets[\"val\"]).numpy())\n</pre> print(\"Train Dataset Cardinality:\", tf.data.experimental.cardinality(datasets[\"train\"]).numpy()) print(\"Validation Dataset Cardinality:\", tf.data.experimental.cardinality(datasets[\"val\"]).numpy()) <pre>Train Dataset Cardinality: 1096\nValidation Dataset Cardinality: 272\n</pre> <p>Now that we have verified the dataset splits, we can use them to build high-performance input pipelines for our training workflows not onlt in TensorFlow, but also JAX and PyTorch. You can refer to the following docs regarding building input pipelines:</p> <ul> <li><code>tf.data</code>: Build TensorFlow input pipelines.</li> <li>Better performance with the <code>tf.data</code> API.</li> <li>TFDS for Jax and PyTorch</li> </ul>"},{"location":"dataset/examples/load_dataset/#data-loading-with-wandb-artifacts","title":"\ud83d\udd25 Data Loading with WandB Artifacts \ud83e\ude84\ud83d\udc1d\u00b6","text":"<p>This notebook demonstrates the usage of a simple and easy-to use data loading API built on top of Tensorflow Datasets and WandB Artifacts.</p>"},{"location":"dataset/examples/load_dataset/#loading-the-dataset","title":"Loading the Dataset\u00b6","text":"<p>Now that the dataset is uploaded as an artifact with the builder logic, loading and ingesting the dataset is incredibly easy. To do this we would simply use the <code>wandb_addons.dataset.load_dataset</code> function.</p>"},{"location":"diffusers/auto_integrate/","title":"Automatic Integration with \ud83e\udd17 Diffusers \ud83e\udde8","text":"<p>This function automatically integrates Weights &amp; Biases logging by inferring the necessary callback for the Diffusion Pipeline.</p>"},{"location":"diffusers/auto_integrate/#wandb_addons.diffusers.auto_callback.get_wandb_callback","title":"<code>get_wandb_callback(pipeline, prompt, wandb_project, num_inference_steps=None, num_images_per_prompt=None, wandb_entity=None, weave_mode=False, negative_prompt=None, configs={}, **kwargs)</code>","text":"<p>A function for automatically inferring the W&amp;B callback for the respective <code>DiffusionPipeline</code>.</p> <p>Warning</p> <p>While using this function to automatically infer the type of the <code>DiffusionPipeline</code>, we must ensure to explicitly set the parameters of the respective callback exclusive to that particular callback, for example, in order to use the <code>StableDiffusionCallback</code> we must explicitly pass the <code>guidance_scale</code>.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>DiffusionPipeline</code> <p>The <code>DiffusionPipeline</code> from <code>diffusers</code>.</p> required <code>prompt</code> <code>Union[str, List[str]]</code> <p>The prompt or prompts to guide the image generation.</p> required <code>wandb_project</code> <code>Optional[str]</code> <p>The name of the project where you're sending the new run. The project is not necessary to be specified unless the run has automatically been initiatlized before the callback is defined.</p> required <code>wandb_entity</code> <code>Optional[str]</code> <p>An entity is a username or team name where you're sending runs. This entity must exist before you can send runs there, so make sure to create your account or team in the UI before starting to log runs. If you don't specify an entity, the run will be sent to your default entity, which is usually your username. Change your default entity in your settings under \"default location to create new projects\".</p> <code>None</code> <code>weave_mode</code> <code>bool</code> <p>Whether to use log to a weave board instead of W&amp;B dashboard or not. The weave mode logs the configs, generated images and timestamp in a <code>StreamTable</code> instead of a <code>wandb.Table</code> and does not require a wandb run to be initialized in order to start logging. This makes it possible to log muliple generations without having to initialize or terminate runs. Note that the parameter <code>wandb_entity</code> must be explicitly specified in order to use weave mode.</p> <code>False</code> <code>num_inference_steps</code> <code>Optional[int]</code> <p>The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.</p> <code>None</code> <code>num_images_per_prompt</code> <code>Optional[int]</code> <p>The number of images to generate per prompt.</p> <code>None</code> <code>negative_prompt</code> <code>Optional[Union[str, List[str]]]</code> <p>The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored if <code>guidance_scale</code> is less than <code>1</code>).</p> <code>None</code> <code>configs</code> <code>Optional[Dict]</code> <p>Additional configs for the experiment you want to sync, for example, seed could be a good config to be passed here.</p> <code>{}</code> Source code in <code>wandb_addons/diffusers/auto_callback.py</code> <pre><code>def get_wandb_callback(\n    pipeline: DiffusionPipeline,\n    prompt: Union[str, List[str]],\n    wandb_project: str,\n    num_inference_steps: Optional[int] = None,\n    num_images_per_prompt: Optional[int] = None,\n    wandb_entity: Optional[str] = None,\n    weave_mode: bool = False,\n    negative_prompt: Optional[Union[str, List[str]]] = None,\n    configs: Dict = {},\n    **kwargs,\n):\n    \"\"\"A function for automatically inferring the W&amp;B callback for the respective\n    `DiffusionPipeline`.\n\n    !!! warning\n        While using this function to automatically infer the type of the\n        `DiffusionPipeline`, we must ensure to explicitly set the parameters of\n        the respective callback exclusive to that particular callback, for example,\n        in order to use the `StableDiffusionCallback` we must explicitly pass the\n        `guidance_scale`.\n\n    Arguments:\n        pipeline (diffusers.DiffusionPipeline): The `DiffusionPipeline` from\n            `diffusers`.\n        prompt (Union[str, List[str]]): The prompt or prompts to guide the image\n            generation.\n        wandb_project (Optional[str]): The name of the project where you're sending\n            the new run. The project is not necessary to be specified unless the run\n            has automatically been initiatlized before the callback is defined.\n        wandb_entity (Optional[str]): An entity is a username or team name where\n            you're sending runs. This entity must exist before you can send runs there,\n            so make sure to create your account or team in the UI before starting to\n            log runs. If you don't specify an entity, the run will be sent to your\n            default entity, which is usually your username. Change your default entity\n            in [your settings](https://wandb.ai/settings) under \"default location to\n            create new projects\".\n        weave_mode (bool): Whether to use log to a\n            [weave board](https://docs.wandb.ai/guides/weave) instead of W&amp;B dashboard or\n            not. The weave mode logs the configs, generated images and timestamp in a\n            [`StreamTable`](https://docs.wandb.ai/guides/weave/streamtable) instead of a\n            `wandb.Table` and does not require a wandb run to be initialized in order to\n            start logging. This makes it possible to log muliple generations without having\n            to initialize or terminate runs. Note that the parameter `wandb_entity` must be\n            explicitly specified in order to use weave mode.\n        num_inference_steps (Optional[int]): The number of denoising steps. More denoising steps\n            usually lead to a higher quality image at the expense of slower inference.\n        num_images_per_prompt (Optional[int]): The number of images to generate per\n            prompt.\n        negative_prompt (Optional[Union[str, List[str]]]): The prompt or prompts not\n            to guide the image generation. Ignored when not using guidance\n            (i.e., ignored if `guidance_scale` is less than `1`).\n        configs (Optional[Dict]): Additional configs for the experiment you want to\n            sync, for example, seed could be a good config to be passed here.\n    \"\"\"\n    pipeline_name = pipeline.__class__.__name__\n    kwargs = {\n        \"pipeline\": pipeline,\n        \"prompt\": prompt,\n        \"wandb_project\": wandb_project,\n        \"wandb_entity\": wandb_entity,\n        \"weave_mode\": weave_mode,\n        \"negative_prompt\": negative_prompt,\n        \"configs\": configs,\n        **kwargs,\n    }\n    if num_inference_steps is not None:\n        kwargs[\"num_inference_steps\"] = num_inference_steps\n    if num_images_per_prompt is not None:\n        kwargs[\"num_images_per_prompt\"] = num_images_per_prompt\n    if pipeline_name == \"StableDiffusionPipeline\":\n        return StableDiffusionCallback(**kwargs)\n    elif pipeline_name == \"StableDiffusionImg2ImgPipeline\":\n        return StableDiffusionImg2ImgCallback(**kwargs)\n    elif pipeline_name in [\"KandinskyCombinedPipeline\", \"KandinskyPipeline\"]:\n        return KandinskyCallback(**kwargs)\n    elif pipeline_name == \"IFPipeline\":\n        return IFCallback(**kwargs)\n    elif pipeline_name == \"StableDiffusionXLPipeline\":\n        return StableDiffusionXLCallback(**kwargs)\n    else:\n        wandb.Error(f\"{pipeline_name} is not supported currently.\")\n</code></pre>"},{"location":"diffusers/base/","title":"Base Classes for Diffusers Integration","text":"<p>The base callback classes for integration with Diffusers.</p>"},{"location":"diffusers/base/#wandb_addons.diffusers.callbacks.base.BaseDiffusersCallback","title":"<code>BaseDiffusersCallback</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Base callback for \ud83e\udde8 Diffusers logging the results of a <code>DiffusionPipeline</code> generation to Weights &amp; Biases.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>DiffusionPipeline</code> <p>The <code>DiffusionPipeline</code> from <code>diffusers</code>.</p> required <code>prompt</code> <code>Union[str, List[str]]</code> <p>The prompt or prompts to guide the image generation.</p> required <code>wandb_project</code> <code>Optional[str]</code> <p>The name of the project where you're sending the new run. The project is not necessary to be specified unless the run has automatically been initiatlized before the callback is defined.</p> required <code>wandb_entity</code> <code>Optional[str]</code> <p>An entity is a username or team name where you're sending runs. This entity must exist before you can send runs there, so make sure to create your account or team in the UI before starting to log runs. If you don't specify an entity, the run will be sent to your default entity, which is usually your username. Change your default entity in your settings under \"default location to create new projects\".</p> <code>None</code> <code>weave_mode</code> <code>bool</code> <p>Whether to use log to a weave board instead of W&amp;B dashboard or not. The weave mode logs the configs, generated images and timestamp in a <code>StreamTable</code> instead of a <code>wandb.Table</code> and does not require a wandb run to be initialized in order to start logging. This makes it possible to log muliple generations without having to initialize or terminate runs. Note that the parameter <code>wandb_entity</code> must be explicitly specified in order to use weave mode.</p> <code>False</code> <code>num_inference_steps</code> <code>int</code> <p>The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.</p> <code>50</code> <code>num_images_per_prompt</code> <code>Optional[int]</code> <p>The number of images to generate per prompt.</p> <code>1</code> <code>negative_prompt</code> <code>Optional[Union[str, List[str]]]</code> <p>The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored if <code>guidance_scale</code> is less than <code>1</code>).</p> <code>None</code> <code>configs</code> <code>Optional[Dict]</code> <p>Additional configs for the experiment you want to sync, for example, for example, <code>seed</code> could be a good config to be passed here.</p> <code>None</code> Source code in <code>wandb_addons/diffusers/callbacks/base/base_callback.py</code> <pre><code>class BaseDiffusersCallback(ABC):\n    \"\"\"Base callback for [\ud83e\udde8 Diffusers](https://huggingface.co/docs/diffusers/index)\n    logging the results of a\n    [`DiffusionPipeline`](https://github.com/huggingface/diffusers/blob/v0.21.0/src/diffusers/pipelines/pipeline_utils.py#L480)\n    generation to Weights &amp; Biases.\n\n    Arguments:\n        pipeline (diffusers.DiffusionPipeline): The `DiffusionPipeline` from\n            `diffusers`.\n        prompt (Union[str, List[str]]): The prompt or prompts to guide the image\n            generation.\n        wandb_project (Optional[str]): The name of the project where you're sending\n            the new run. The project is not necessary to be specified unless the run\n            has automatically been initiatlized before the callback is defined.\n        wandb_entity (Optional[str]): An entity is a username or team name where\n            you're sending runs. This entity must exist before you can send runs there,\n            so make sure to create your account or team in the UI before starting to\n            log runs. If you don't specify an entity, the run will be sent to your\n            default entity, which is usually your username. Change your default entity\n            in [your settings](https://wandb.ai/settings) under \"default location to\n            create new projects\".\n        weave_mode (bool): Whether to use log to a\n            [weave board](https://docs.wandb.ai/guides/weave) instead of W&amp;B dashboard\n            or not. The weave mode logs the configs, generated images and timestamp in a\n            [`StreamTable`](https://docs.wandb.ai/guides/weave/streamtable) instead of a\n            `wandb.Table` and does not require a wandb run to be initialized in order to\n            start logging. This makes it possible to log muliple generations without\n            having to initialize or terminate runs. Note that the parameter\n            `wandb_entity` must be explicitly specified in order to use weave mode.\n        num_inference_steps (int): The number of denoising steps. More denoising steps\n            usually lead to a higher quality image at the expense of slower inference.\n        num_images_per_prompt (Optional[int]): The number of images to generate per\n            prompt.\n        negative_prompt (Optional[Union[str, List[str]]]): The prompt or prompts not\n            to guide the image generation. Ignored when not using guidance\n            (i.e., ignored if `guidance_scale` is less than `1`).\n        configs (Optional[Dict]): Additional configs for the experiment you want to\n            sync, for example, for example, `seed` could be a good config to be passed\n            here.\n    \"\"\"\n\n    def __init__(\n        self,\n        pipeline: DiffusionPipeline,\n        prompt: Union[str, List[str]],\n        wandb_project: str,\n        wandb_entity: Optional[str] = None,\n        weave_mode: bool = False,\n        num_inference_steps: int = 50,\n        num_images_per_prompt: Optional[int] = 1,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        configs: Optional[Dict] = None,\n        **kwargs,\n    ) -&gt; None:\n        super().__init__()\n        self.pipeline = pipeline\n        self.prompt = prompt\n        self.wandb_project = wandb_project\n        self.wandb_entity = wandb_entity\n        self.weave_mode = weave_mode\n        self.num_inference_steps = num_inference_steps\n        self.num_images_per_prompt = num_images_per_prompt\n        self.negative_prompt = negative_prompt\n        self.configs = configs\n        self.wandb_table = None\n        self.table_row = []\n        self.starting_step = 1\n        self.log_step = num_inference_steps\n        self.job_type = \"text-to-image\"\n        self.table_name = \"Text-To-Image\"\n        self.initialize_wandb(wandb_project, wandb_entity)\n        self.build_wandb_table()\n\n    def update_configs(self) -&gt; None:\n        \"\"\"Update the configs as a state of the callback. This function is called inside\n        `initialize_wandb()`.\n        \"\"\"\n        pipeline_configs = dict(self.pipeline.config)\n        pipeline_configs[\"scheduler\"] = list(pipeline_configs[\"scheduler\"])\n        pipeline_configs[\"scheduler\"][1] = dict(self.pipeline.scheduler.config)\n        additional_configs = {\n            \"prompt\": self.prompt,\n            \"negative_prompt\": self.negative_prompt\n            if self.negative_prompt is not None\n            else \"\",\n            \"num_inference_steps\": self.num_inference_steps,\n            \"num_images_per_prompt\": self.num_images_per_prompt,\n            \"pipeline\": pipeline_configs,\n        }\n        self.configs = (\n            {**self.configs, **additional_configs}\n            if self.configs is not None\n            else additional_configs\n        )\n\n    def initialize_wandb(self, wandb_project: str, wandb_entity: str) -&gt; None:\n        \"\"\"Initializes the wandb run if not already initialized. If `weave_mode` is\n        `True`, then a [StreamTable](https://docs.wandb.ai/guides/weave/streamtable) is\n        initialized instead of a wandb run. This function is called automatically when\n        the callback is initialized.\n\n        Arguments:\n            wandb_project (str): The name of the W&amp;B project.\n            wandb_entity (str): The name of the W&amp;B entity.\n        \"\"\"\n        self.update_configs()\n        if self.weave_mode:\n            if self.wandb_entity is None:\n                wandb.termerror(\n                    \"The parameter wandb_entity must be provided when weave_mode is enabled.\"\n                )\n            else:\n                self.stream_table = StreamTable(\n                    f\"{self.wandb_entity}/{self.wandb_project}/{self.table_name}\"\n                )\n                self.table_row = []\n        else:\n            if wandb.run is None:\n                if wandb_project is not None:\n                    wandb.init(\n                        project=wandb_project,\n                        entity=wandb_entity,\n                        job_type=self.job_type,\n                        config=self.configs,\n                    )\n                else:\n                    wandb.termerror(\"The parameter wandb_project must be provided.\")\n\n    def build_wandb_table(self) -&gt; None:\n        \"\"\"Specifies the columns of the wandb table if not in weave mode. This function\n        is called automatically when the callback is initialized.\n        \"\"\"\n        self.table_columns = [\n            \"Prompt\",\n            \"Negative-Prompt\",\n            \"Generated-Image\",\n            \"Image-Size\",\n        ]\n\n    @abstractmethod\n    def generate(self, latents: torch.FloatTensor) -&gt; List:\n        \"\"\"Generate images from latents. This method must be implemented in the child\n        class.\"\"\"\n        pass\n\n    def populate_table_row(\n        self, prompt: str, negative_prompt: str, image: Image\n    ) -&gt; None:\n        \"\"\"Populates the table row with the prompt, negative prompt, the generated\n        image, and the configs.\n\n        Arguments:\n            prompt (str): The prompt to guide the image generation.\n            negative_prompt (str): The prompt not to guide the image generation.\n            image (Image): The generated image.\n        \"\"\"\n        width, height = image.size\n        if self.weave_mode:\n            self.table_row += [\n                {\n                    \"Generated-Image\": image,\n                    \"Image-Size\": {\"Width\": width, \"Height\": height},\n                    \"Configs\": self.configs,\n                }\n            ]\n        else:\n            self.table_row = [\n                prompt,\n                negative_prompt if negative_prompt is not None else \"\",\n                wandb.Image(image),\n                {\"Width\": width, \"Height\": height},\n            ]\n\n    def at_initial_step(self):\n        \"\"\"A function that will be called at the initial step of the denoising loop\n        during inference.\"\"\"\n        if not self.weave_mode:\n            self.wandb_table = wandb.Table(columns=self.table_columns)\n\n    def __call__(\n        self,\n        step: int,\n        timestep: int,\n        latents: torch.FloatTensor,\n        end_experiment: bool = True,\n    ):\n        \"\"\"A function that will be called every `callback_steps` steps during\n        inference with the `diffusers.DiffusionPipeline`.\n\n        Arguments:\n            step (int): The current step of the inference.\n            timestep (int): The current timestep of the inference.\n            latents (torch.FloatTensor): The latent tensor used to generate the image.\n            end_experiment (bool): Whether to end the experiment automatically or not\n                after the pipeline is executed.\n        \"\"\"\n        if step == self.starting_step:\n            self.at_initial_step()\n        if step == self.log_step:\n            images = self.generate(latents)\n            prompt_logging = (\n                self.prompt if isinstance(self.prompt, list) else [self.prompt]\n            )\n            negative_prompt_logging = (\n                self.negative_prompt\n                if isinstance(self.negative_prompt, list)\n                else [self.negative_prompt] * len(prompt_logging)\n            )\n            images = chunkify(images, len(prompt_logging))\n            for idx in range(len(prompt_logging)):\n                for image in images[idx]:\n                    self.populate_table_row(\n                        prompt_logging[idx], negative_prompt_logging[idx], image\n                    )\n                    if not self.weave_mode:\n                        self.wandb_table.add_data(*self.table_row)\n                        wandb.log({\"Generated-Images\": wandb.Image(image)})\n            if end_experiment:\n                self.end_experiment()\n\n    def end_experiment(self):\n        \"\"\"Ends the experiment. This function is called automatically at the end of\n        `__call__` the parameter `end_experiment` is set to `True`.\n        \"\"\"\n        if self.weave_mode:\n            self.stream_table.log(self.table_row)\n            self.stream_table.finish()\n        elif wandb.run is not None:\n            wandb.log({self.table_name: self.wandb_table})\n            wandb.finish()\n</code></pre>"},{"location":"diffusers/base/#wandb_addons.diffusers.callbacks.base.BaseDiffusersCallback.__call__","title":"<code>__call__(step, timestep, latents, end_experiment=True)</code>","text":"<p>A function that will be called every <code>callback_steps</code> steps during inference with the <code>diffusers.DiffusionPipeline</code>.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>The current step of the inference.</p> required <code>timestep</code> <code>int</code> <p>The current timestep of the inference.</p> required <code>latents</code> <code>FloatTensor</code> <p>The latent tensor used to generate the image.</p> required <code>end_experiment</code> <code>bool</code> <p>Whether to end the experiment automatically or not after the pipeline is executed.</p> <code>True</code> Source code in <code>wandb_addons/diffusers/callbacks/base/base_callback.py</code> <pre><code>def __call__(\n    self,\n    step: int,\n    timestep: int,\n    latents: torch.FloatTensor,\n    end_experiment: bool = True,\n):\n    \"\"\"A function that will be called every `callback_steps` steps during\n    inference with the `diffusers.DiffusionPipeline`.\n\n    Arguments:\n        step (int): The current step of the inference.\n        timestep (int): The current timestep of the inference.\n        latents (torch.FloatTensor): The latent tensor used to generate the image.\n        end_experiment (bool): Whether to end the experiment automatically or not\n            after the pipeline is executed.\n    \"\"\"\n    if step == self.starting_step:\n        self.at_initial_step()\n    if step == self.log_step:\n        images = self.generate(latents)\n        prompt_logging = (\n            self.prompt if isinstance(self.prompt, list) else [self.prompt]\n        )\n        negative_prompt_logging = (\n            self.negative_prompt\n            if isinstance(self.negative_prompt, list)\n            else [self.negative_prompt] * len(prompt_logging)\n        )\n        images = chunkify(images, len(prompt_logging))\n        for idx in range(len(prompt_logging)):\n            for image in images[idx]:\n                self.populate_table_row(\n                    prompt_logging[idx], negative_prompt_logging[idx], image\n                )\n                if not self.weave_mode:\n                    self.wandb_table.add_data(*self.table_row)\n                    wandb.log({\"Generated-Images\": wandb.Image(image)})\n        if end_experiment:\n            self.end_experiment()\n</code></pre>"},{"location":"diffusers/base/#wandb_addons.diffusers.callbacks.base.BaseDiffusersCallback.at_initial_step","title":"<code>at_initial_step()</code>","text":"<p>A function that will be called at the initial step of the denoising loop during inference.</p> Source code in <code>wandb_addons/diffusers/callbacks/base/base_callback.py</code> <pre><code>def at_initial_step(self):\n    \"\"\"A function that will be called at the initial step of the denoising loop\n    during inference.\"\"\"\n    if not self.weave_mode:\n        self.wandb_table = wandb.Table(columns=self.table_columns)\n</code></pre>"},{"location":"diffusers/base/#wandb_addons.diffusers.callbacks.base.BaseDiffusersCallback.build_wandb_table","title":"<code>build_wandb_table()</code>","text":"<p>Specifies the columns of the wandb table if not in weave mode. This function is called automatically when the callback is initialized.</p> Source code in <code>wandb_addons/diffusers/callbacks/base/base_callback.py</code> <pre><code>def build_wandb_table(self) -&gt; None:\n    \"\"\"Specifies the columns of the wandb table if not in weave mode. This function\n    is called automatically when the callback is initialized.\n    \"\"\"\n    self.table_columns = [\n        \"Prompt\",\n        \"Negative-Prompt\",\n        \"Generated-Image\",\n        \"Image-Size\",\n    ]\n</code></pre>"},{"location":"diffusers/base/#wandb_addons.diffusers.callbacks.base.BaseDiffusersCallback.end_experiment","title":"<code>end_experiment()</code>","text":"<p>Ends the experiment. This function is called automatically at the end of <code>__call__</code> the parameter <code>end_experiment</code> is set to <code>True</code>.</p> Source code in <code>wandb_addons/diffusers/callbacks/base/base_callback.py</code> <pre><code>def end_experiment(self):\n    \"\"\"Ends the experiment. This function is called automatically at the end of\n    `__call__` the parameter `end_experiment` is set to `True`.\n    \"\"\"\n    if self.weave_mode:\n        self.stream_table.log(self.table_row)\n        self.stream_table.finish()\n    elif wandb.run is not None:\n        wandb.log({self.table_name: self.wandb_table})\n        wandb.finish()\n</code></pre>"},{"location":"diffusers/base/#wandb_addons.diffusers.callbacks.base.BaseDiffusersCallback.generate","title":"<code>generate(latents)</code>  <code>abstractmethod</code>","text":"<p>Generate images from latents. This method must be implemented in the child class.</p> Source code in <code>wandb_addons/diffusers/callbacks/base/base_callback.py</code> <pre><code>@abstractmethod\ndef generate(self, latents: torch.FloatTensor) -&gt; List:\n    \"\"\"Generate images from latents. This method must be implemented in the child\n    class.\"\"\"\n    pass\n</code></pre>"},{"location":"diffusers/base/#wandb_addons.diffusers.callbacks.base.BaseDiffusersCallback.initialize_wandb","title":"<code>initialize_wandb(wandb_project, wandb_entity)</code>","text":"<p>Initializes the wandb run if not already initialized. If <code>weave_mode</code> is <code>True</code>, then a StreamTable is initialized instead of a wandb run. This function is called automatically when the callback is initialized.</p> <p>Parameters:</p> Name Type Description Default <code>wandb_project</code> <code>str</code> <p>The name of the W&amp;B project.</p> required <code>wandb_entity</code> <code>str</code> <p>The name of the W&amp;B entity.</p> required Source code in <code>wandb_addons/diffusers/callbacks/base/base_callback.py</code> <pre><code>def initialize_wandb(self, wandb_project: str, wandb_entity: str) -&gt; None:\n    \"\"\"Initializes the wandb run if not already initialized. If `weave_mode` is\n    `True`, then a [StreamTable](https://docs.wandb.ai/guides/weave/streamtable) is\n    initialized instead of a wandb run. This function is called automatically when\n    the callback is initialized.\n\n    Arguments:\n        wandb_project (str): The name of the W&amp;B project.\n        wandb_entity (str): The name of the W&amp;B entity.\n    \"\"\"\n    self.update_configs()\n    if self.weave_mode:\n        if self.wandb_entity is None:\n            wandb.termerror(\n                \"The parameter wandb_entity must be provided when weave_mode is enabled.\"\n            )\n        else:\n            self.stream_table = StreamTable(\n                f\"{self.wandb_entity}/{self.wandb_project}/{self.table_name}\"\n            )\n            self.table_row = []\n    else:\n        if wandb.run is None:\n            if wandb_project is not None:\n                wandb.init(\n                    project=wandb_project,\n                    entity=wandb_entity,\n                    job_type=self.job_type,\n                    config=self.configs,\n                )\n            else:\n                wandb.termerror(\"The parameter wandb_project must be provided.\")\n</code></pre>"},{"location":"diffusers/base/#wandb_addons.diffusers.callbacks.base.BaseDiffusersCallback.populate_table_row","title":"<code>populate_table_row(prompt, negative_prompt, image)</code>","text":"<p>Populates the table row with the prompt, negative prompt, the generated image, and the configs.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to guide the image generation.</p> required <code>negative_prompt</code> <code>str</code> <p>The prompt not to guide the image generation.</p> required <code>image</code> <code>Image</code> <p>The generated image.</p> required Source code in <code>wandb_addons/diffusers/callbacks/base/base_callback.py</code> <pre><code>def populate_table_row(\n    self, prompt: str, negative_prompt: str, image: Image\n) -&gt; None:\n    \"\"\"Populates the table row with the prompt, negative prompt, the generated\n    image, and the configs.\n\n    Arguments:\n        prompt (str): The prompt to guide the image generation.\n        negative_prompt (str): The prompt not to guide the image generation.\n        image (Image): The generated image.\n    \"\"\"\n    width, height = image.size\n    if self.weave_mode:\n        self.table_row += [\n            {\n                \"Generated-Image\": image,\n                \"Image-Size\": {\"Width\": width, \"Height\": height},\n                \"Configs\": self.configs,\n            }\n        ]\n    else:\n        self.table_row = [\n            prompt,\n            negative_prompt if negative_prompt is not None else \"\",\n            wandb.Image(image),\n            {\"Width\": width, \"Height\": height},\n        ]\n</code></pre>"},{"location":"diffusers/base/#wandb_addons.diffusers.callbacks.base.BaseDiffusersCallback.update_configs","title":"<code>update_configs()</code>","text":"<p>Update the configs as a state of the callback. This function is called inside <code>initialize_wandb()</code>.</p> Source code in <code>wandb_addons/diffusers/callbacks/base/base_callback.py</code> <pre><code>def update_configs(self) -&gt; None:\n    \"\"\"Update the configs as a state of the callback. This function is called inside\n    `initialize_wandb()`.\n    \"\"\"\n    pipeline_configs = dict(self.pipeline.config)\n    pipeline_configs[\"scheduler\"] = list(pipeline_configs[\"scheduler\"])\n    pipeline_configs[\"scheduler\"][1] = dict(self.pipeline.scheduler.config)\n    additional_configs = {\n        \"prompt\": self.prompt,\n        \"negative_prompt\": self.negative_prompt\n        if self.negative_prompt is not None\n        else \"\",\n        \"num_inference_steps\": self.num_inference_steps,\n        \"num_images_per_prompt\": self.num_images_per_prompt,\n        \"pipeline\": pipeline_configs,\n    }\n    self.configs = (\n        {**self.configs, **additional_configs}\n        if self.configs is not None\n        else additional_configs\n    )\n</code></pre>"},{"location":"diffusers/base/#wandb_addons.diffusers.callbacks.base.BaseImage2ImageCallback","title":"<code>BaseImage2ImageCallback</code>","text":"<p>             Bases: <code>BaseDiffusersCallback</code></p> <p>Base callback for \ud83e\udde8 Diffusers logging the results of a <code>DiffusionPipeline</code> for image2image translation task to Weights &amp; Biases.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>DiffusionPipeline</code> <p>The <code>DiffusionPipeline</code> from <code>diffusers</code>.</p> required <code>prompt</code> <code>Union[str, List[str]]</code> <p>The prompt or prompts to guide the image generation.</p> required <code>input_images</code> <code>PipelineImageInput</code> <p>The input image, numpy array or tensor representing an image batch to be used as the starting point. For both numpy array and pytorch tensor, the expected value range is between [0, 1] If it's a tensor or a list or tensors, the expected shape should be <code>(B, C, H, W)</code> or <code>(C, H, W)</code>. If it is a numpy array or a list of arrays, the expected shape should be (B, H, W, C) or (H, W, C) It can also accept image latents as image, but if passing latents directly it is not encoded again.</p> required <code>wandb_project</code> <code>Optional[str]</code> <p>The name of the project where you're sending the new run. The project is not necessary to be specified unless the run has automatically been initiatlized before the callback is defined.</p> required <code>wandb_entity</code> <code>Optional[str]</code> <p>An entity is a username or team name where you're sending runs. This entity must exist before you can send runs there, so make sure to create your account or team in the UI before starting to log runs. If you don't specify an entity, the run will be sent to your default entity, which is usually your username. Change your default entity in your settings under \"default location to create new projects\".</p> <code>None</code> <code>weave_mode</code> <code>bool</code> <p>Whether to use log to a weave board instead of W&amp;B dashboard or not. The weave mode logs the configs, generated images and timestamp in a <code>StreamTable</code> instead of a <code>wandb.Table</code> and does not require a wandb run to be initialized in order to start logging. This makes it possible to log muliple generations without having to initialize or terminate runs. Note that the parameter <code>wandb_entity</code> must be explicitly specified in order to use weave mode.</p> <code>False</code> <code>num_inference_steps</code> <code>int</code> <p>The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.</p> <code>50</code> <code>num_images_per_prompt</code> <code>Optional[int]</code> <p>The number of images to generate per prompt.</p> <code>1</code> <code>negative_prompt</code> <code>Optional[Union[str, List[str]]]</code> <p>The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored if <code>guidance_scale</code> is less than <code>1</code>).</p> <code>None</code> <code>configs</code> <code>Optional[Dict]</code> <p>Additional configs for the experiment you want to sync, for example, for example, <code>seed</code> could be a good config to be passed here.</p> <code>None</code> Source code in <code>wandb_addons/diffusers/callbacks/base/base_image_to_image_callback.py</code> <pre><code>class BaseImage2ImageCallback(BaseDiffusersCallback):\n    \"\"\"Base callback for [\ud83e\udde8 Diffusers](https://huggingface.co/docs/diffusers/index)\n    logging the results of a\n    [`DiffusionPipeline`](https://github.com/huggingface/diffusers/blob/v0.21.0/src/diffusers/pipelines/pipeline_utils.py#L480)\n    for image2image translation task to Weights &amp; Biases.\n\n    Arguments:\n        pipeline (diffusers.DiffusionPipeline): The `DiffusionPipeline` from\n            `diffusers`.\n        prompt (Union[str, List[str]]): The prompt or prompts to guide the image\n            generation.\n        input_images (PipelineImageInput): The input image, numpy array or tensor\n            representing an image batch to be used as the starting point. For both numpy\n            array and pytorch tensor, the expected value range is between [0, 1] If it's\n            a tensor or a list or tensors, the expected shape should be `(B, C, H, W)`\n            or `(C, H, W)`. If it is a numpy array or a list of arrays, the expected\n            shape should be (B, H, W, C) or (H, W, C) It can also accept image latents as\n            image, but if passing latents directly it is not encoded again.\n        wandb_project (Optional[str]): The name of the project where you're sending\n            the new run. The project is not necessary to be specified unless the run\n            has automatically been initiatlized before the callback is defined.\n        wandb_entity (Optional[str]): An entity is a username or team name where\n            you're sending runs. This entity must exist before you can send runs there,\n            so make sure to create your account or team in the UI before starting to\n            log runs. If you don't specify an entity, the run will be sent to your\n            default entity, which is usually your username. Change your default entity\n            in [your settings](https://wandb.ai/settings) under \"default location to\n            create new projects\".\n        weave_mode (bool): Whether to use log to a\n            [weave board](https://docs.wandb.ai/guides/weave) instead of W&amp;B dashboard\n            or not. The weave mode logs the configs, generated images and timestamp in a\n            [`StreamTable`](https://docs.wandb.ai/guides/weave/streamtable) instead of a\n            `wandb.Table` and does not require a wandb run to be initialized in order to\n            start logging. This makes it possible to log muliple generations without\n            having to initialize or terminate runs. Note that the parameter\n            `wandb_entity` must be explicitly specified in order to use weave mode.\n        num_inference_steps (int): The number of denoising steps. More denoising steps\n            usually lead to a higher quality image at the expense of slower inference.\n        num_images_per_prompt (Optional[int]): The number of images to generate per\n            prompt.\n        negative_prompt (Optional[Union[str, List[str]]]): The prompt or prompts not\n            to guide the image generation. Ignored when not using guidance\n            (i.e., ignored if `guidance_scale` is less than `1`).\n        configs (Optional[Dict]): Additional configs for the experiment you want to\n            sync, for example, for example, `seed` could be a good config to be passed\n            here.\n    \"\"\"\n\n    def __init__(\n        self,\n        pipeline: DiffusionPipeline,\n        prompt: Union[str, List[str]],\n        input_images: PipelineImageInput,\n        wandb_project: str,\n        wandb_entity: Optional[str] = None,\n        weave_mode: bool = False,\n        num_inference_steps: int = 50,\n        num_images_per_prompt: Optional[int] = 1,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        configs: Optional[Dict] = None,\n        **kwargs,\n    ) -&gt; None:\n        self.job_type = \"image-to-image\"\n        super().__init__(\n            pipeline=pipeline,\n            prompt=prompt,\n            wandb_project=wandb_project,\n            wandb_entity=wandb_entity,\n            weave_mode=weave_mode,\n            num_inference_steps=num_inference_steps,\n            num_images_per_prompt=num_images_per_prompt,\n            negative_prompt=negative_prompt,\n            configs=configs,\n            **kwargs,\n        )\n        self.input_images = self.postprocess_input_images(input_images)\n\n    def initialize_wandb(self, wandb_project, wandb_entity) -&gt; None:\n        \"\"\"Initializes the wandb run if not already initialized. If `weave_mode` is\n        `True`, then a [StreamTable](https://docs.wandb.ai/guides/weave/streamtable) is\n        initialized instead of a wandb run. This function is called automatically when\n        the callback is initialized.\n\n        Arguments:\n            wandb_project (str): The name of the W&amp;B project.\n            wandb_entity (str): The name of the W&amp;B entity.\n        \"\"\"\n        self.job_type = \"image-to-image\"\n        self.table_name = \"Image-To-Image\"\n        super().initialize_wandb(wandb_project, wandb_entity)\n\n    def postprocess_input_images(\n        self, input_images: Union[torch.Tensor, Image.Image, np.array]\n    ) -&gt; Image.Image:\n        \"\"\"Postprocess input images to be logged to the W&amp;B Table/StreamTable.\n\n        Arguments:\n            input_images (Union[torch.Tensor, Image.Image, np.array]): The input images\n                to be postprocessed.\n        \"\"\"\n        if isinstance(input_images, torch.Tensor):\n            input_images = self.pipeline.image_processor.pt_to_numpy(input_images)\n            input_images = self.pipeline.image_processor.numpy_to_pil(input_images)\n        elif isinstance(input_images, Image.Image):\n            input_images = [input_images]\n        elif isinstance(input_images, np.array):\n            input_images = self.pipeline.image_processor.numpy_to_pil(input_images)\n        return input_images\n\n    def build_wandb_table(self) -&gt; None:\n        \"\"\"Specifies the columns of the wandb table if not in weave mode. This function\n        is called automatically when the callback is initialized.\n        \"\"\"\n        super().build_wandb_table()\n        self.table_columns = [\"Input-Image\", \"Input-Image-Size\"] + self.table_columns\n\n    def populate_table_row(\n        self, input_image: Image.Image, prompt: str, negative_prompt: str, image: Any\n    ) -&gt; None:\n        \"\"\"Populates the table row with the input image, prompt, negative prompt, the\n        generated image, and the configs.\n\n        Arguments:\n            input_image (Image): The input image.s\n            prompt (str): The prompt to guide the image generation.\n            negative_prompt (str): The prompt not to guide the image generation.\n            image (Image): The generated image.\n        \"\"\"\n        input_width, input_height = input_image.size\n        generated_width, generated_height = image.size\n        self.table_row = (\n            {\n                \"Input-Image\": input_image,\n                \"Input-Image-Size\": {\"Width\": input_width, \"Height\": input_height},\n                \"Prompt\": prompt,\n                \"Negative-Prompt\": negative_prompt\n                if negative_prompt is not None\n                else \"\",\n                \"Generated-Image\": image,\n                \"Generated-Image-Size\": {\n                    \"Width\": generated_width,\n                    \"Height\": generated_height,\n                },\n                \"Configs\": self.configs,\n            }\n            if self.weave_mode\n            else [\n                wandb.Image(input_image),\n                {\"Width\": input_width, \"Height\": input_height},\n                prompt,\n                negative_prompt if negative_prompt is not None else \"\",\n                wandb.Image(image),\n                {\"Width\": generated_width, \"Height\": generated_height},\n            ]\n        )\n\n    def __call__(\n        self,\n        step: int,\n        timestep: int,\n        latents: torch.FloatTensor,\n        end_experiment: bool = True,\n    ):\n        \"\"\"A function that will be called every `callback_steps` steps during\n        inference with the `diffusers.DiffusionPipeline`.\n\n        Arguments:\n            step (int): The current step of the inference.\n            timestep (int): The current timestep of the inference.\n            latents (torch.FloatTensor): The latent tensor used to generate the image.\n            end_experiment (bool): Whether to end the experiment automatically or not\n                after the pipeline is executed.\n        \"\"\"\n        if step == self.starting_step:\n            self.at_initial_step()\n        if step == self.log_step:\n            images = self.generate(latents)\n            prompt_logging = (\n                self.prompt if isinstance(self.prompt, list) else [self.prompt]\n            )\n            negative_prompt_logging = (\n                self.negative_prompt\n                if isinstance(self.negative_prompt, list)\n                else [self.negative_prompt] * len(prompt_logging)\n            )\n            images = chunkify(images, len(prompt_logging))\n            for idx in range(len(prompt_logging)):\n                for image in images[idx]:\n                    self.populate_table_row(\n                        self.input_images[0],\n                        prompt_logging[idx],\n                        negative_prompt_logging[idx],\n                        image,\n                    )\n                    if self.weave_mode:\n                        self.stream_table.log(self.table_row)\n                    else:\n                        self.wandb_table.add_data(*self.table_row)\n                        wandb.log({\"Generated-Images\": wandb.Image(image)})\n            if end_experiment:\n                self.end_experiment()\n</code></pre>"},{"location":"diffusers/base/#wandb_addons.diffusers.callbacks.base.BaseImage2ImageCallback.__call__","title":"<code>__call__(step, timestep, latents, end_experiment=True)</code>","text":"<p>A function that will be called every <code>callback_steps</code> steps during inference with the <code>diffusers.DiffusionPipeline</code>.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>The current step of the inference.</p> required <code>timestep</code> <code>int</code> <p>The current timestep of the inference.</p> required <code>latents</code> <code>FloatTensor</code> <p>The latent tensor used to generate the image.</p> required <code>end_experiment</code> <code>bool</code> <p>Whether to end the experiment automatically or not after the pipeline is executed.</p> <code>True</code> Source code in <code>wandb_addons/diffusers/callbacks/base/base_image_to_image_callback.py</code> <pre><code>def __call__(\n    self,\n    step: int,\n    timestep: int,\n    latents: torch.FloatTensor,\n    end_experiment: bool = True,\n):\n    \"\"\"A function that will be called every `callback_steps` steps during\n    inference with the `diffusers.DiffusionPipeline`.\n\n    Arguments:\n        step (int): The current step of the inference.\n        timestep (int): The current timestep of the inference.\n        latents (torch.FloatTensor): The latent tensor used to generate the image.\n        end_experiment (bool): Whether to end the experiment automatically or not\n            after the pipeline is executed.\n    \"\"\"\n    if step == self.starting_step:\n        self.at_initial_step()\n    if step == self.log_step:\n        images = self.generate(latents)\n        prompt_logging = (\n            self.prompt if isinstance(self.prompt, list) else [self.prompt]\n        )\n        negative_prompt_logging = (\n            self.negative_prompt\n            if isinstance(self.negative_prompt, list)\n            else [self.negative_prompt] * len(prompt_logging)\n        )\n        images = chunkify(images, len(prompt_logging))\n        for idx in range(len(prompt_logging)):\n            for image in images[idx]:\n                self.populate_table_row(\n                    self.input_images[0],\n                    prompt_logging[idx],\n                    negative_prompt_logging[idx],\n                    image,\n                )\n                if self.weave_mode:\n                    self.stream_table.log(self.table_row)\n                else:\n                    self.wandb_table.add_data(*self.table_row)\n                    wandb.log({\"Generated-Images\": wandb.Image(image)})\n        if end_experiment:\n            self.end_experiment()\n</code></pre>"},{"location":"diffusers/base/#wandb_addons.diffusers.callbacks.base.BaseImage2ImageCallback.build_wandb_table","title":"<code>build_wandb_table()</code>","text":"<p>Specifies the columns of the wandb table if not in weave mode. This function is called automatically when the callback is initialized.</p> Source code in <code>wandb_addons/diffusers/callbacks/base/base_image_to_image_callback.py</code> <pre><code>def build_wandb_table(self) -&gt; None:\n    \"\"\"Specifies the columns of the wandb table if not in weave mode. This function\n    is called automatically when the callback is initialized.\n    \"\"\"\n    super().build_wandb_table()\n    self.table_columns = [\"Input-Image\", \"Input-Image-Size\"] + self.table_columns\n</code></pre>"},{"location":"diffusers/base/#wandb_addons.diffusers.callbacks.base.BaseImage2ImageCallback.initialize_wandb","title":"<code>initialize_wandb(wandb_project, wandb_entity)</code>","text":"<p>Initializes the wandb run if not already initialized. If <code>weave_mode</code> is <code>True</code>, then a StreamTable is initialized instead of a wandb run. This function is called automatically when the callback is initialized.</p> <p>Parameters:</p> Name Type Description Default <code>wandb_project</code> <code>str</code> <p>The name of the W&amp;B project.</p> required <code>wandb_entity</code> <code>str</code> <p>The name of the W&amp;B entity.</p> required Source code in <code>wandb_addons/diffusers/callbacks/base/base_image_to_image_callback.py</code> <pre><code>def initialize_wandb(self, wandb_project, wandb_entity) -&gt; None:\n    \"\"\"Initializes the wandb run if not already initialized. If `weave_mode` is\n    `True`, then a [StreamTable](https://docs.wandb.ai/guides/weave/streamtable) is\n    initialized instead of a wandb run. This function is called automatically when\n    the callback is initialized.\n\n    Arguments:\n        wandb_project (str): The name of the W&amp;B project.\n        wandb_entity (str): The name of the W&amp;B entity.\n    \"\"\"\n    self.job_type = \"image-to-image\"\n    self.table_name = \"Image-To-Image\"\n    super().initialize_wandb(wandb_project, wandb_entity)\n</code></pre>"},{"location":"diffusers/base/#wandb_addons.diffusers.callbacks.base.BaseImage2ImageCallback.populate_table_row","title":"<code>populate_table_row(input_image, prompt, negative_prompt, image)</code>","text":"<p>Populates the table row with the input image, prompt, negative prompt, the generated image, and the configs.</p> <p>Parameters:</p> Name Type Description Default <code>input_image</code> <code>Image</code> <p>The input image.s</p> required <code>prompt</code> <code>str</code> <p>The prompt to guide the image generation.</p> required <code>negative_prompt</code> <code>str</code> <p>The prompt not to guide the image generation.</p> required <code>image</code> <code>Image</code> <p>The generated image.</p> required Source code in <code>wandb_addons/diffusers/callbacks/base/base_image_to_image_callback.py</code> <pre><code>def populate_table_row(\n    self, input_image: Image.Image, prompt: str, negative_prompt: str, image: Any\n) -&gt; None:\n    \"\"\"Populates the table row with the input image, prompt, negative prompt, the\n    generated image, and the configs.\n\n    Arguments:\n        input_image (Image): The input image.s\n        prompt (str): The prompt to guide the image generation.\n        negative_prompt (str): The prompt not to guide the image generation.\n        image (Image): The generated image.\n    \"\"\"\n    input_width, input_height = input_image.size\n    generated_width, generated_height = image.size\n    self.table_row = (\n        {\n            \"Input-Image\": input_image,\n            \"Input-Image-Size\": {\"Width\": input_width, \"Height\": input_height},\n            \"Prompt\": prompt,\n            \"Negative-Prompt\": negative_prompt\n            if negative_prompt is not None\n            else \"\",\n            \"Generated-Image\": image,\n            \"Generated-Image-Size\": {\n                \"Width\": generated_width,\n                \"Height\": generated_height,\n            },\n            \"Configs\": self.configs,\n        }\n        if self.weave_mode\n        else [\n            wandb.Image(input_image),\n            {\"Width\": input_width, \"Height\": input_height},\n            prompt,\n            negative_prompt if negative_prompt is not None else \"\",\n            wandb.Image(image),\n            {\"Width\": generated_width, \"Height\": generated_height},\n        ]\n    )\n</code></pre>"},{"location":"diffusers/base/#wandb_addons.diffusers.callbacks.base.BaseImage2ImageCallback.postprocess_input_images","title":"<code>postprocess_input_images(input_images)</code>","text":"<p>Postprocess input images to be logged to the W&amp;B Table/StreamTable.</p> <p>Parameters:</p> Name Type Description Default <code>input_images</code> <code>Union[Tensor, Image, array]</code> <p>The input images to be postprocessed.</p> required Source code in <code>wandb_addons/diffusers/callbacks/base/base_image_to_image_callback.py</code> <pre><code>def postprocess_input_images(\n    self, input_images: Union[torch.Tensor, Image.Image, np.array]\n) -&gt; Image.Image:\n    \"\"\"Postprocess input images to be logged to the W&amp;B Table/StreamTable.\n\n    Arguments:\n        input_images (Union[torch.Tensor, Image.Image, np.array]): The input images\n            to be postprocessed.\n    \"\"\"\n    if isinstance(input_images, torch.Tensor):\n        input_images = self.pipeline.image_processor.pt_to_numpy(input_images)\n        input_images = self.pipeline.image_processor.numpy_to_pil(input_images)\n    elif isinstance(input_images, Image.Image):\n        input_images = [input_images]\n    elif isinstance(input_images, np.array):\n        input_images = self.pipeline.image_processor.numpy_to_pil(input_images)\n    return input_images\n</code></pre>"},{"location":"diffusers/base/#wandb_addons.diffusers.callbacks.base.BaseMultiPipelineCallback","title":"<code>BaseMultiPipelineCallback</code>","text":"<p>             Bases: <code>BaseDiffusersCallback</code></p> <p>Base callback for \ud83e\udde8 Diffusers logging the results of multiple <code>DiffusionPipeline</code>s to Weights &amp; Biases.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>DiffusionPipeline</code> <p>The <code>DiffusionPipeline</code> from <code>diffusers</code>.</p> required <code>prompt</code> <code>Union[str, List[str]]</code> <p>The prompt or prompts to guide the image generation.</p> required <code>wandb_project</code> <code>Optional[str]</code> <p>The name of the project where you're sending the new run. The project is not necessary to be specified unless the run has automatically been initiatlized before the callback is defined.</p> required <code>wandb_entity</code> <code>Optional[str]</code> <p>An entity is a username or team name where you're sending runs. This entity must exist before you can send runs there, so make sure to create your account or team in the UI before starting to log runs. If you don't specify an entity, the run will be sent to your default entity, which is usually your username. Change your default entity in your settings under \"default location to create new projects\".</p> <code>None</code> <code>weave_mode</code> <code>bool</code> <p>Whether to use log to a weave board instead of W&amp;B dashboard or not. The weave mode logs the configs, generated images and timestamp in a <code>StreamTable</code> instead of a <code>wandb.Table</code> and does not require a wandb run to be initialized in order to start logging. This makes it possible to log muliple generations without having to initialize or terminate runs. Note that the parameter <code>wandb_entity</code> must be explicitly specified in order to use weave mode.</p> <code>False</code> <code>num_inference_steps</code> <code>int</code> <p>The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.</p> <code>50</code> <code>num_images_per_prompt</code> <code>Optional[int]</code> <p>The number of images to generate per prompt.</p> <code>1</code> <code>negative_prompt</code> <code>Optional[Union[str, List[str]]]</code> <p>The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored if <code>guidance_scale</code> is less than <code>1</code>).</p> <code>None</code> <code>initial_stage_name</code> <code>Optional[str]</code> <p>The name of the initial stage. If not specified, it would be set to <code>\"stage_1\"</code>.</p> <code>None</code> <code>configs</code> <code>Optional[Dict]</code> <p>Additional configs for the experiment you want to sync, for example, for example, <code>seed</code> could be a good config to be passed here.</p> <code>None</code> Source code in <code>wandb_addons/diffusers/callbacks/base/base_multi_pipeline_callback.py</code> <pre><code>class BaseMultiPipelineCallback(BaseDiffusersCallback):\n    \"\"\"Base callback for [\ud83e\udde8 Diffusers](https://huggingface.co/docs/diffusers/index)\n    logging the results of multiple\n    [`DiffusionPipeline`](https://github.com/huggingface/diffusers/blob/v0.21.0/src/diffusers/pipelines/pipeline_utils.py#L480)s\n    to Weights &amp; Biases.\n\n    Arguments:\n        pipeline (diffusers.DiffusionPipeline): The `DiffusionPipeline` from\n            `diffusers`.\n        prompt (Union[str, List[str]]): The prompt or prompts to guide the image\n            generation.\n        wandb_project (Optional[str]): The name of the project where you're sending\n            the new run. The project is not necessary to be specified unless the run\n            has automatically been initiatlized before the callback is defined.\n        wandb_entity (Optional[str]): An entity is a username or team name where\n            you're sending runs. This entity must exist before you can send runs there,\n            so make sure to create your account or team in the UI before starting to\n            log runs. If you don't specify an entity, the run will be sent to your\n            default entity, which is usually your username. Change your default entity\n            in [your settings](https://wandb.ai/settings) under \"default location to\n            create new projects\".\n        weave_mode (bool): Whether to use log to a\n            [weave board](https://docs.wandb.ai/guides/weave) instead of W&amp;B dashboard\n            or not. The weave mode logs the configs, generated images and timestamp in a\n            [`StreamTable`](https://docs.wandb.ai/guides/weave/streamtable) instead of a\n            `wandb.Table` and does not require a wandb run to be initialized in order to\n            start logging. This makes it possible to log muliple generations without\n            having to initialize or terminate runs. Note that the parameter\n            `wandb_entity` must be explicitly specified in order to use weave mode.\n        num_inference_steps (int): The number of denoising steps. More denoising steps\n            usually lead to a higher quality image at the expense of slower inference.\n        num_images_per_prompt (Optional[int]): The number of images to generate per\n            prompt.\n        negative_prompt (Optional[Union[str, List[str]]]): The prompt or prompts not\n            to guide the image generation. Ignored when not using guidance\n            (i.e., ignored if `guidance_scale` is less than `1`).\n        initial_stage_name (Optional[str]): The name of the initial stage. If not\n            specified, it would be set to `\"stage_1\"`.\n        configs (Optional[Dict]): Additional configs for the experiment you want to\n            sync, for example, for example, `seed` could be a good config to be passed\n            here.\n    \"\"\"\n\n    def __init__(\n        self,\n        pipeline: DiffusionPipeline,\n        prompt: Union[str, List[str]],\n        wandb_project: str,\n        wandb_entity: Optional[str] = None,\n        weave_mode: bool = False,\n        num_inference_steps: int = 50,\n        num_images_per_prompt: Optional[int] = 1,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        initial_stage_name: Optional[str] = None,\n        configs: Optional[Dict] = None,\n        **kwargs,\n    ) -&gt; None:\n        self.stage_name = (\n            initial_stage_name if initial_stage_name is not None else \"stage_1\"\n        )\n        self.stage_counter = 1\n        self.original_configs = {} if configs is None else configs\n        super().__init__(\n            pipeline,\n            prompt,\n            wandb_project,\n            wandb_entity,\n            weave_mode,\n            num_inference_steps,\n            num_images_per_prompt,\n            negative_prompt,\n            configs,\n            **kwargs,\n        )\n        self.table_row = {}\n\n    def update_configs(self) -&gt; None:\n        \"\"\"Update the configs as a state of the callback. This function is called inside\n        `initialize_wandb()`. In this function, the configs regarding the base pipeline\n        are updated as well.\n        \"\"\"\n        pipeline_configs = dict(self.pipeline.config)\n        pipeline_configs[\"scheduler\"] = list(pipeline_configs[\"scheduler\"])\n        pipeline_configs[\"scheduler\"][1] = dict(self.pipeline.scheduler.config)\n        additional_configs = {\n            self.stage_name: {\n                \"pipeline\": pipeline_configs,\n                \"num_inference_steps\": self.num_inference_steps,\n                \"prompt\": self.prompt,\n                \"negative_prompt\": self.negative_prompt\n                if self.negative_prompt is not None\n                else \"\",\n                \"num_images_per_prompt\": self.num_images_per_prompt,\n                \"stage-name\": self.stage_name,\n                \"stage-sequence\": self.stage_counter,\n                **self.original_configs,\n            },\n        }\n        self.configs = additional_configs\n\n    def add_stage(\n        self,\n        pipeline: DiffusionPipeline,\n        num_inference_steps: Optional[int] = None,\n        stage_name: Optional[str] = None,\n        configs: Optional[Dict] = None,\n    ) -&gt; None:\n        \"\"\"Add a new stage to the callback to log the results of a new pipeline in a\n        multi-pipeline workflow.\n\n        Arguments:\n            pipeline (diffusers.DiffusionPipeline): The `DiffusionPipeline` from\n                for the new stage.\n            num_inference_steps (Optional[int]): The number of denoising steps for the\n                new stage. More denoising steps usually lead to a higher quality image\n                at the expense of slower inference.\n            stage_name (Optional[str]): The name of the new stage. If not specified,\n                it would be set to `\"stage_{stage_counter}\"`.\n            configs (Optional[Dict]): Additional configs for the new stage you want to\n                sync, for example, for example, `seed` could be a good config to be\n                passed here.\n        \"\"\"\n        self.pipeline = pipeline\n        self.num_inference_steps = (\n            num_inference_steps\n            if num_inference_steps is not None\n            else self.num_inference_steps\n        )\n        self.stage_counter += 1\n        self.stage_name = (\n            stage_name if stage_name is not None else f\"stage_{self.stage_counter}\"\n        )\n        pipeline_configs = dict(self.pipeline.config)\n        pipeline_configs[\"scheduler\"] = list(pipeline_configs[\"scheduler\"])\n        pipeline_configs[\"scheduler\"][1] = dict(self.pipeline.scheduler.config)\n        additional_configs = {\n            self.stage_name: {\n                \"pipeline\": pipeline_configs,\n                \"num_inference_steps\": self.num_inference_steps,\n                \"prompt\": self.prompt,\n                \"negative_prompt\": self.negative_prompt\n                if self.negative_prompt is not None\n                else \"\",\n                \"num_images_per_prompt\": self.num_images_per_prompt,\n                \"stage-name\": self.stage_name,\n                \"stage-sequence\": self.stage_counter,\n                **self.original_configs,\n            },\n        }\n        if configs is not None:\n            additional_configs[self.stage_name].update(configs)\n        self.configs.update(additional_configs)\n        if wandb.run is not None:\n            wandb.config.update(additional_configs)\n\n    def at_initial_step(self):\n        \"\"\"A function that will be called at the initial step of the denoising loop\n        during inference.\"\"\"\n        if self.stage_counter == 1:\n            super().at_initial_step()\n\n    def build_wandb_table(self) -&gt; None:\n        \"\"\"Specifies the columns of the wandb table if not in weave mode. This function\n        is called automatically when the callback is initialized.\n        \"\"\"\n        super().build_wandb_table()\n        self.table_columns = [\"Stage-Sequence\", \"Stage-Name\"] + self.table_columns\n\n    def populate_table_row(self, prompt: str, negative_prompt: str, image: Any) -&gt; None:\n        \"\"\"Populates the table row with the prompt, negative prompt, the generated\n        image, and the configs.\n\n        Arguments:\n            prompt (str): The prompt to guide the image generation.\n            negative_prompt (str): The prompt not to guide the image generation.\n            image (Image): The generated image.\n        \"\"\"\n        width, height = image.size\n        if self.weave_mode:\n            self.table_row.update(\n                {\n                    self.stage_name: {\n                        \"Generated-Image\": image,\n                        \"Image-Size\": {\"Width\": width, \"Height\": height},\n                        \"Configs\": self.configs[self.stage_name],\n                    }\n                }\n            )\n        else:\n            self.table_row = [\n                self.stage_counter,\n                self.stage_name,\n                prompt,\n                negative_prompt if negative_prompt is not None else \"\",\n                wandb.Image(image),\n                {\"Width\": width, \"Height\": height},\n            ]\n\n    def end_experiment(self):\n        \"\"\"Ends the experiment. This function is called automatically at the end of\n        `__call__` the parameter `end_experiment` is set to `True`.\n        \"\"\"\n        if self.weave_mode:\n            self.table_row = {\"Experiment\": self.table_row}\n            self.stream_table.log(self.table_row)\n            self.stream_table.finish()\n        elif wandb.run is not None:\n            wandb.log({self.table_name: self.wandb_table})\n            wandb.finish()\n</code></pre>"},{"location":"diffusers/base/#wandb_addons.diffusers.callbacks.base.BaseMultiPipelineCallback.add_stage","title":"<code>add_stage(pipeline, num_inference_steps=None, stage_name=None, configs=None)</code>","text":"<p>Add a new stage to the callback to log the results of a new pipeline in a multi-pipeline workflow.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>DiffusionPipeline</code> <p>The <code>DiffusionPipeline</code> from for the new stage.</p> required <code>num_inference_steps</code> <code>Optional[int]</code> <p>The number of denoising steps for the new stage. More denoising steps usually lead to a higher quality image at the expense of slower inference.</p> <code>None</code> <code>stage_name</code> <code>Optional[str]</code> <p>The name of the new stage. If not specified, it would be set to <code>\"stage_{stage_counter}\"</code>.</p> <code>None</code> <code>configs</code> <code>Optional[Dict]</code> <p>Additional configs for the new stage you want to sync, for example, for example, <code>seed</code> could be a good config to be passed here.</p> <code>None</code> Source code in <code>wandb_addons/diffusers/callbacks/base/base_multi_pipeline_callback.py</code> <pre><code>def add_stage(\n    self,\n    pipeline: DiffusionPipeline,\n    num_inference_steps: Optional[int] = None,\n    stage_name: Optional[str] = None,\n    configs: Optional[Dict] = None,\n) -&gt; None:\n    \"\"\"Add a new stage to the callback to log the results of a new pipeline in a\n    multi-pipeline workflow.\n\n    Arguments:\n        pipeline (diffusers.DiffusionPipeline): The `DiffusionPipeline` from\n            for the new stage.\n        num_inference_steps (Optional[int]): The number of denoising steps for the\n            new stage. More denoising steps usually lead to a higher quality image\n            at the expense of slower inference.\n        stage_name (Optional[str]): The name of the new stage. If not specified,\n            it would be set to `\"stage_{stage_counter}\"`.\n        configs (Optional[Dict]): Additional configs for the new stage you want to\n            sync, for example, for example, `seed` could be a good config to be\n            passed here.\n    \"\"\"\n    self.pipeline = pipeline\n    self.num_inference_steps = (\n        num_inference_steps\n        if num_inference_steps is not None\n        else self.num_inference_steps\n    )\n    self.stage_counter += 1\n    self.stage_name = (\n        stage_name if stage_name is not None else f\"stage_{self.stage_counter}\"\n    )\n    pipeline_configs = dict(self.pipeline.config)\n    pipeline_configs[\"scheduler\"] = list(pipeline_configs[\"scheduler\"])\n    pipeline_configs[\"scheduler\"][1] = dict(self.pipeline.scheduler.config)\n    additional_configs = {\n        self.stage_name: {\n            \"pipeline\": pipeline_configs,\n            \"num_inference_steps\": self.num_inference_steps,\n            \"prompt\": self.prompt,\n            \"negative_prompt\": self.negative_prompt\n            if self.negative_prompt is not None\n            else \"\",\n            \"num_images_per_prompt\": self.num_images_per_prompt,\n            \"stage-name\": self.stage_name,\n            \"stage-sequence\": self.stage_counter,\n            **self.original_configs,\n        },\n    }\n    if configs is not None:\n        additional_configs[self.stage_name].update(configs)\n    self.configs.update(additional_configs)\n    if wandb.run is not None:\n        wandb.config.update(additional_configs)\n</code></pre>"},{"location":"diffusers/base/#wandb_addons.diffusers.callbacks.base.BaseMultiPipelineCallback.at_initial_step","title":"<code>at_initial_step()</code>","text":"<p>A function that will be called at the initial step of the denoising loop during inference.</p> Source code in <code>wandb_addons/diffusers/callbacks/base/base_multi_pipeline_callback.py</code> <pre><code>def at_initial_step(self):\n    \"\"\"A function that will be called at the initial step of the denoising loop\n    during inference.\"\"\"\n    if self.stage_counter == 1:\n        super().at_initial_step()\n</code></pre>"},{"location":"diffusers/base/#wandb_addons.diffusers.callbacks.base.BaseMultiPipelineCallback.build_wandb_table","title":"<code>build_wandb_table()</code>","text":"<p>Specifies the columns of the wandb table if not in weave mode. This function is called automatically when the callback is initialized.</p> Source code in <code>wandb_addons/diffusers/callbacks/base/base_multi_pipeline_callback.py</code> <pre><code>def build_wandb_table(self) -&gt; None:\n    \"\"\"Specifies the columns of the wandb table if not in weave mode. This function\n    is called automatically when the callback is initialized.\n    \"\"\"\n    super().build_wandb_table()\n    self.table_columns = [\"Stage-Sequence\", \"Stage-Name\"] + self.table_columns\n</code></pre>"},{"location":"diffusers/base/#wandb_addons.diffusers.callbacks.base.BaseMultiPipelineCallback.end_experiment","title":"<code>end_experiment()</code>","text":"<p>Ends the experiment. This function is called automatically at the end of <code>__call__</code> the parameter <code>end_experiment</code> is set to <code>True</code>.</p> Source code in <code>wandb_addons/diffusers/callbacks/base/base_multi_pipeline_callback.py</code> <pre><code>def end_experiment(self):\n    \"\"\"Ends the experiment. This function is called automatically at the end of\n    `__call__` the parameter `end_experiment` is set to `True`.\n    \"\"\"\n    if self.weave_mode:\n        self.table_row = {\"Experiment\": self.table_row}\n        self.stream_table.log(self.table_row)\n        self.stream_table.finish()\n    elif wandb.run is not None:\n        wandb.log({self.table_name: self.wandb_table})\n        wandb.finish()\n</code></pre>"},{"location":"diffusers/base/#wandb_addons.diffusers.callbacks.base.BaseMultiPipelineCallback.populate_table_row","title":"<code>populate_table_row(prompt, negative_prompt, image)</code>","text":"<p>Populates the table row with the prompt, negative prompt, the generated image, and the configs.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to guide the image generation.</p> required <code>negative_prompt</code> <code>str</code> <p>The prompt not to guide the image generation.</p> required <code>image</code> <code>Image</code> <p>The generated image.</p> required Source code in <code>wandb_addons/diffusers/callbacks/base/base_multi_pipeline_callback.py</code> <pre><code>def populate_table_row(self, prompt: str, negative_prompt: str, image: Any) -&gt; None:\n    \"\"\"Populates the table row with the prompt, negative prompt, the generated\n    image, and the configs.\n\n    Arguments:\n        prompt (str): The prompt to guide the image generation.\n        negative_prompt (str): The prompt not to guide the image generation.\n        image (Image): The generated image.\n    \"\"\"\n    width, height = image.size\n    if self.weave_mode:\n        self.table_row.update(\n            {\n                self.stage_name: {\n                    \"Generated-Image\": image,\n                    \"Image-Size\": {\"Width\": width, \"Height\": height},\n                    \"Configs\": self.configs[self.stage_name],\n                }\n            }\n        )\n    else:\n        self.table_row = [\n            self.stage_counter,\n            self.stage_name,\n            prompt,\n            negative_prompt if negative_prompt is not None else \"\",\n            wandb.Image(image),\n            {\"Width\": width, \"Height\": height},\n        ]\n</code></pre>"},{"location":"diffusers/base/#wandb_addons.diffusers.callbacks.base.BaseMultiPipelineCallback.update_configs","title":"<code>update_configs()</code>","text":"<p>Update the configs as a state of the callback. This function is called inside <code>initialize_wandb()</code>. In this function, the configs regarding the base pipeline are updated as well.</p> Source code in <code>wandb_addons/diffusers/callbacks/base/base_multi_pipeline_callback.py</code> <pre><code>def update_configs(self) -&gt; None:\n    \"\"\"Update the configs as a state of the callback. This function is called inside\n    `initialize_wandb()`. In this function, the configs regarding the base pipeline\n    are updated as well.\n    \"\"\"\n    pipeline_configs = dict(self.pipeline.config)\n    pipeline_configs[\"scheduler\"] = list(pipeline_configs[\"scheduler\"])\n    pipeline_configs[\"scheduler\"][1] = dict(self.pipeline.scheduler.config)\n    additional_configs = {\n        self.stage_name: {\n            \"pipeline\": pipeline_configs,\n            \"num_inference_steps\": self.num_inference_steps,\n            \"prompt\": self.prompt,\n            \"negative_prompt\": self.negative_prompt\n            if self.negative_prompt is not None\n            else \"\",\n            \"num_images_per_prompt\": self.num_images_per_prompt,\n            \"stage-name\": self.stage_name,\n            \"stage-sequence\": self.stage_counter,\n            **self.original_configs,\n        },\n    }\n    self.configs = additional_configs\n</code></pre>"},{"location":"diffusers/deepfloyd_if/","title":"Callbacks for \ud83e\udde8 Diffusers pipelines for DeepFloyd IF","text":"<p>Callbacks for logging experiment details, configs and generated images for the DeepFloyd IF pipelines from Diffusers \ud83e\udde8 pipelines to your Weights &amp; Biases workspace.</p> Callback Run-in-Colab WandB Run DeepFloyd IF"},{"location":"diffusers/deepfloyd_if/#wandb_addons.diffusers.callbacks.deepfloyd_if.IFCallback","title":"<code>IFCallback</code>","text":"<p>             Bases: <code>BaseMultiPipelineCallback</code></p> <p>Callback for logging the resulst of a text-to-image workflow with DeepFloyd IF to Weights &amp; Biases.</p> <p>Features:</p> <ul> <li>The callback automatically logs basic configs like prompt, negative prompt,     etc. along with the generated image in a     <code>wandb.Table</code>.</li> <li>The callback also logs configs for both the experiment as well as pipelines     with the wandb run.</li> <li>No need to initialize a run, the callback automatically initialized and ends     runs gracefully.</li> <li>Supports logging multiple stages of a workflow using a single callback.</li> </ul> <p>Example usage:</p> <p>You can fine an example notebook here.</p> <pre><code>import gc\nfrom functools import partial\n\nimport torch\n\nfrom diffusers import IFPipeline, IFSuperResolutionPipeline, StableDiffusionUpscalePipeline\nfrom wandb_addons.diffusers import IFCallback\n\n# Stage 1\npipeline_1 = IFPipeline.from_pretrained(\n    \"DeepFloyd/IF-I-XL-v1.0\", variant=\"fp16\", torch_dtype=torch.float16\n)\npipeline_1.enable_model_cpu_offload()\n\nprompt = 'a photo of a smiling bee wearing a yellow hoodie and blue sunglasses standing in front of the eiffel tower holding a sign that says \"Weights and Biases\"'\nprompt_embeds, negative_embeds = pipeline_1.encode_prompt(prompt)\nnum_images_per_prompt = 2\nnum_inference_steps = 100\nconfigs = {\"guidance_scale\": 7.0}\n\ncallback = IFCallback(\n    pipeline=pipeline_1,\n    prompt=prompt,\n    wandb_project=\"diffusers-2\",\n    wandb_entity=\"geekyrakshit\",\n    weave_mode=False,\n    num_inference_steps=num_inference_steps,\n    num_images_per_prompt=num_images_per_prompt,\n    configs=configs\n)\n\nimage = pipeline_1(\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=negative_embeds,\n    output_type=\"pt\",\n    # Do not end the experiment after the first stage\n    callback=partial(callback, end_experiment=False),\n    **configs,\n).images\n\n# Stage 2\npipeline_2 = IFSuperResolutionPipeline.from_pretrained(\n    \"DeepFloyd/IF-II-L-v1.0\",\n    text_encoder=None,\n    variant=\"fp16\",\n    torch_dtype=torch.float16\n)\npipeline_2.enable_model_cpu_offload()\n\nnum_inference_steps = 50\n\ncallback.add_stage(pipeline_2, num_inference_steps=num_inference_steps)\n\nimage = pipeline_2(\n    image=image,\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=negative_embeds,\n    num_inference_steps=num_inference_steps,\n    output_type=\"pt\",\n    callback=partial(callback, end_experiment=False),\n).images\n\n# Upscale stage\nsafety_modules = {\n    \"feature_extractor\": pipeline_1.feature_extractor,\n    \"safety_checker\": pipeline_1.safety_checker,\n    \"watermarker\": pipeline_1.watermarker,\n}\npipeline_3 = StableDiffusionUpscalePipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-x4-upscaler\",\n    **safety_modules,\n    torch_dtype=torch.float16\n)\npipeline_3.enable_model_cpu_offload()\n\nnum_inference_steps = 75\n\ncallback.add_stage(\n    pipeline_3, num_inference_steps=num_inference_steps, stage_name=\"Upscale\"\n)\n\nimage = pipeline_3(\n    prompt=prompt,\n    image=image,\n    noise_level=100,\n    num_inference_steps=num_inference_steps,\n    callback=callback,\n).images\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>DiffusionPipeline</code> <p>The <code>DiffusionPipeline</code> from <code>diffusers</code>.</p> required <code>prompt</code> <code>Union[str, List[str]]</code> <p>The prompt or prompts to guide the image generation.</p> required <code>wandb_project</code> <code>Optional[str]</code> <p>The name of the project where you're sending the new run. The project is not necessary to be specified unless the run has automatically been initiatlized before the callback is defined.</p> required <code>wandb_entity</code> <code>Optional[str]</code> <p>An entity is a username or team name where you're sending runs. This entity must exist before you can send runs there, so make sure to create your account or team in the UI before starting to log runs. If you don't specify an entity, the run will be sent to your default entity, which is usually your username. Change your default entity in your settings under \"default location to create new projects\".</p> <code>None</code> <code>weave_mode</code> <code>bool</code> <p>Whether to use log to a weave board instead of W&amp;B dashboard or not. The weave mode logs the configs, generated images and timestamp in a <code>StreamTable</code> instead of a <code>wandb.Table</code> and does not require a wandb run to be initialized in order to start logging. This makes it possible to log muliple generations without having to initialize or terminate runs. Note that the parameter <code>wandb_entity</code> must be explicitly specified in order to use weave mode.</p> <code>False</code> <code>num_inference_steps</code> <code>int</code> <p>The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.</p> <code>100</code> <code>num_images_per_prompt</code> <code>Optional[int]</code> <p>The number of images to generate per prompt.</p> <code>1</code> <code>negative_prompt</code> <code>Optional[Union[str, List[str]]]</code> <p>The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored if <code>guidance_scale</code> is less than <code>1</code>).</p> <code>None</code> <code>initial_stage_name</code> <code>Optional[str]</code> <p>The name of the initial stage. If not specified, it would be set to <code>\"stage_1\"</code>.</p> <code>None</code> <code>configs</code> <code>Optional[Dict]</code> <p>Additional configs for the experiment you want to sync, for example, for example, <code>seed</code> could be a good config to be passed here.</p> <code>None</code> Source code in <code>wandb_addons/diffusers/callbacks/deepfloyd_if.py</code> <pre><code>class IFCallback(BaseMultiPipelineCallback):\n    \"\"\"Callback for logging the resulst of a text-to-image workflow with\n    [DeepFloyd IF](https://huggingface.co/docs/diffusers/v0.21.0/en/api/pipelines/deepfloyd_if#texttoimage-generation)\n    to Weights &amp; Biases.\n\n    !!! note \"Features:\"\n        - The callback automatically logs basic configs like prompt, negative prompt,\n            etc. along with the generated image in a\n            [`wandb.Table`](https://docs.wandb.ai/guides/tables).\n        - The callback also logs configs for both the experiment as well as pipelines\n            with the wandb run.\n        - No need to initialize a run, the callback automatically initialized and ends\n            runs gracefully.\n        - Supports logging multiple stages of a workflow using a single callback.\n\n    !!! example \"Example usage:\"\n        You can fine an example notebook [here](../examples/deepfloyd_if).\n\n        ```python\n        import gc\n        from functools import partial\n\n        import torch\n\n        from diffusers import IFPipeline, IFSuperResolutionPipeline, StableDiffusionUpscalePipeline\n        from wandb_addons.diffusers import IFCallback\n\n        # Stage 1\n        pipeline_1 = IFPipeline.from_pretrained(\n            \"DeepFloyd/IF-I-XL-v1.0\", variant=\"fp16\", torch_dtype=torch.float16\n        )\n        pipeline_1.enable_model_cpu_offload()\n\n        prompt = 'a photo of a smiling bee wearing a yellow hoodie and blue sunglasses standing in front of the eiffel tower holding a sign that says \"Weights and Biases\"'\n        prompt_embeds, negative_embeds = pipeline_1.encode_prompt(prompt)\n        num_images_per_prompt = 2\n        num_inference_steps = 100\n        configs = {\"guidance_scale\": 7.0}\n\n        callback = IFCallback(\n            pipeline=pipeline_1,\n            prompt=prompt,\n            wandb_project=\"diffusers-2\",\n            wandb_entity=\"geekyrakshit\",\n            weave_mode=False,\n            num_inference_steps=num_inference_steps,\n            num_images_per_prompt=num_images_per_prompt,\n            configs=configs\n        )\n\n        image = pipeline_1(\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_embeds,\n            output_type=\"pt\",\n            # Do not end the experiment after the first stage\n            callback=partial(callback, end_experiment=False),\n            **configs,\n        ).images\n\n        # Stage 2\n        pipeline_2 = IFSuperResolutionPipeline.from_pretrained(\n            \"DeepFloyd/IF-II-L-v1.0\",\n            text_encoder=None,\n            variant=\"fp16\",\n            torch_dtype=torch.float16\n        )\n        pipeline_2.enable_model_cpu_offload()\n\n        num_inference_steps = 50\n\n        callback.add_stage(pipeline_2, num_inference_steps=num_inference_steps)\n\n        image = pipeline_2(\n            image=image,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_embeds,\n            num_inference_steps=num_inference_steps,\n            output_type=\"pt\",\n            callback=partial(callback, end_experiment=False),\n        ).images\n\n        # Upscale stage\n        safety_modules = {\n            \"feature_extractor\": pipeline_1.feature_extractor,\n            \"safety_checker\": pipeline_1.safety_checker,\n            \"watermarker\": pipeline_1.watermarker,\n        }\n        pipeline_3 = StableDiffusionUpscalePipeline.from_pretrained(\n            \"stabilityai/stable-diffusion-x4-upscaler\",\n            **safety_modules,\n            torch_dtype=torch.float16\n        )\n        pipeline_3.enable_model_cpu_offload()\n\n        num_inference_steps = 75\n\n        callback.add_stage(\n            pipeline_3, num_inference_steps=num_inference_steps, stage_name=\"Upscale\"\n        )\n\n        image = pipeline_3(\n            prompt=prompt,\n            image=image,\n            noise_level=100,\n            num_inference_steps=num_inference_steps,\n            callback=callback,\n        ).images\n        ```\n\n    Arguments:\n        pipeline (diffusers.DiffusionPipeline): The `DiffusionPipeline` from\n            `diffusers`.\n        prompt (Union[str, List[str]]): The prompt or prompts to guide the image\n            generation.\n        wandb_project (Optional[str]): The name of the project where you're sending\n            the new run. The project is not necessary to be specified unless the run\n            has automatically been initiatlized before the callback is defined.\n        wandb_entity (Optional[str]): An entity is a username or team name where\n            you're sending runs. This entity must exist before you can send runs there,\n            so make sure to create your account or team in the UI before starting to\n            log runs. If you don't specify an entity, the run will be sent to your\n            default entity, which is usually your username. Change your default entity\n            in [your settings](https://wandb.ai/settings) under \"default location to\n            create new projects\".\n        weave_mode (bool): Whether to use log to a\n            [weave board](https://docs.wandb.ai/guides/weave) instead of W&amp;B dashboard\n            or not. The weave mode logs the configs, generated images and timestamp in a\n            [`StreamTable`](https://docs.wandb.ai/guides/weave/streamtable) instead of a\n            `wandb.Table` and does not require a wandb run to be initialized in order to\n            start logging. This makes it possible to log muliple generations without\n            having to initialize or terminate runs. Note that the parameter\n            `wandb_entity` must be explicitly specified in order to use weave mode.\n        num_inference_steps (int): The number of denoising steps. More denoising steps\n            usually lead to a higher quality image at the expense of slower inference.\n        num_images_per_prompt (Optional[int]): The number of images to generate per\n            prompt.\n        negative_prompt (Optional[Union[str, List[str]]]): The prompt or prompts not\n            to guide the image generation. Ignored when not using guidance\n            (i.e., ignored if `guidance_scale` is less than `1`).\n        initial_stage_name (Optional[str]): The name of the initial stage. If not\n            specified, it would be set to `\"stage_1\"`.\n        configs (Optional[Dict]): Additional configs for the experiment you want to\n            sync, for example, for example, `seed` could be a good config to be passed\n            here.\n    \"\"\"\n\n    def __init__(\n        self,\n        pipeline: Union[\n            IFPipeline, IFSuperResolutionPipeline, StableDiffusionUpscalePipeline\n        ],\n        prompt: Union[str, List[str]],\n        wandb_project: str,\n        wandb_entity: Optional[str] = None,\n        weave_mode: bool = False,\n        num_inference_steps: int = 100,\n        num_images_per_prompt: Optional[int] = 1,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        initial_stage_name: Optional[str] = None,\n        configs: Optional[Dict] = None,\n        **kwargs\n    ) -&gt; None:\n        super().__init__(\n            pipeline,\n            prompt,\n            wandb_project,\n            wandb_entity,\n            weave_mode,\n            num_inference_steps,\n            num_images_per_prompt,\n            negative_prompt,\n            initial_stage_name,\n            configs,\n            **kwargs\n        )\n        self.starting_step = 0\n        self.log_step = self.num_inference_steps - 1\n\n    def generate(self, latents: torch.FloatTensor) -&gt; List:\n        if isinstance(self.pipeline, IFPipeline) or isinstance(\n            self.pipeline, IFSuperResolutionPipeline\n        ):\n            images = (latents / 2 + 0.5).clamp(0, 1)\n            images = images.cpu().permute(0, 2, 3, 1).float().numpy()\n            (\n                images,\n                nsfw_detected,\n                watermark_detected,\n            ) = self.pipeline.run_safety_checker(\n                images, self.pipeline._execution_device, latents.dtype\n            )\n            images = self.pipeline.numpy_to_pil(images)\n            return images\n        elif isinstance(self.pipeline, StableDiffusionUpscalePipeline):\n            needs_upcasting = (\n                self.pipeline.vae.dtype == torch.float16\n                and self.pipeline.vae.config.force_upcast\n            )\n            if needs_upcasting:\n                self.pipeline.upcast_vae()\n                latents = latents.to(\n                    next(iter(self.pipeline.vae.post_quant_conv.parameters())).dtype\n                )\n            image = self.pipeline.vae.decode(\n                latents / self.pipeline.vae.config.scaling_factor, return_dict=False\n            )[0]\n            if needs_upcasting:\n                self.pipeline.vae.to(dtype=torch.float16)\n            image, nsfw_detected, watermark_detected = self.pipeline.run_safety_checker(\n                image, self.pipeline._execution_device, latents.dtype\n            )\n            do_denormalize = (\n                [True] * image.shape[0]\n                if nsfw_detected is None\n                else [not has_nsfw for has_nsfw in nsfw_detected]\n            )\n            image = self.pipeline.image_processor.postprocess(\n                image, output_type=\"pil\", do_denormalize=do_denormalize\n            )\n            image = (\n                self.pipeline.watermarker.apply_watermark(image)\n                if self.pipeline.watermarker is not None\n                else image\n            )\n            if (\n                hasattr(self, \"final_offload_hook\")\n                and self.pipeline.final_offload_hook is not None\n            ):\n                self.pipeline.final_offload_hook.offload()\n            return image\n\n    def add_stage(\n        self,\n        pipeline: Union[IFPipeline, IFSuperResolutionPipeline],\n        num_inference_steps: Optional[int] = None,\n        stage_name: Optional[str] = None,\n        configs: Optional[Dict] = None,\n    ) -&gt; None:\n        \"\"\"Add a new stage to the callback to log the results of a new pipeline in a\n        multi-pipeline workflow.\n\n        Arguments:\n            pipeline (diffusers.DiffusionPipeline): The `DiffusionPipeline` from\n                for the new stage.\n            num_inference_steps (Optional[int]): The number of denoising steps for the\n                new stage. More denoising steps usually lead to a higher quality image\n                at the expense of slower inference.\n            stage_name (Optional[str]): The name of the new stage. If not specified,\n                it would be set to `\"stage_{stage_counter}\"`.\n            configs (Optional[Dict]): Additional configs for the new stage you want to\n                sync, for example, for example, `seed` could be a good config to be\n                passed here.\n        \"\"\"\n        assert (\n            isinstance(pipeline, IFPipeline)\n            or isinstance(pipeline, IFSuperResolutionPipeline)\n            or isinstance(pipeline, StableDiffusionUpscalePipeline)\n        ), \"IFCallback only supports IFPipeline and IFSuperResolutionPipeline\"\n        super().add_stage(pipeline, num_inference_steps, stage_name, configs)\n        if isinstance(pipeline, IFSuperResolutionPipeline) or isinstance(\n            pipeline, StableDiffusionUpscalePipeline\n        ):\n            self.starting_step = 0\n            self.log_step = self.starting_step + self.num_inference_steps - 1\n</code></pre>"},{"location":"diffusers/deepfloyd_if/#wandb_addons.diffusers.callbacks.deepfloyd_if.IFCallback.add_stage","title":"<code>add_stage(pipeline, num_inference_steps=None, stage_name=None, configs=None)</code>","text":"<p>Add a new stage to the callback to log the results of a new pipeline in a multi-pipeline workflow.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>DiffusionPipeline</code> <p>The <code>DiffusionPipeline</code> from for the new stage.</p> required <code>num_inference_steps</code> <code>Optional[int]</code> <p>The number of denoising steps for the new stage. More denoising steps usually lead to a higher quality image at the expense of slower inference.</p> <code>None</code> <code>stage_name</code> <code>Optional[str]</code> <p>The name of the new stage. If not specified, it would be set to <code>\"stage_{stage_counter}\"</code>.</p> <code>None</code> <code>configs</code> <code>Optional[Dict]</code> <p>Additional configs for the new stage you want to sync, for example, for example, <code>seed</code> could be a good config to be passed here.</p> <code>None</code> Source code in <code>wandb_addons/diffusers/callbacks/deepfloyd_if.py</code> <pre><code>def add_stage(\n    self,\n    pipeline: Union[IFPipeline, IFSuperResolutionPipeline],\n    num_inference_steps: Optional[int] = None,\n    stage_name: Optional[str] = None,\n    configs: Optional[Dict] = None,\n) -&gt; None:\n    \"\"\"Add a new stage to the callback to log the results of a new pipeline in a\n    multi-pipeline workflow.\n\n    Arguments:\n        pipeline (diffusers.DiffusionPipeline): The `DiffusionPipeline` from\n            for the new stage.\n        num_inference_steps (Optional[int]): The number of denoising steps for the\n            new stage. More denoising steps usually lead to a higher quality image\n            at the expense of slower inference.\n        stage_name (Optional[str]): The name of the new stage. If not specified,\n            it would be set to `\"stage_{stage_counter}\"`.\n        configs (Optional[Dict]): Additional configs for the new stage you want to\n            sync, for example, for example, `seed` could be a good config to be\n            passed here.\n    \"\"\"\n    assert (\n        isinstance(pipeline, IFPipeline)\n        or isinstance(pipeline, IFSuperResolutionPipeline)\n        or isinstance(pipeline, StableDiffusionUpscalePipeline)\n    ), \"IFCallback only supports IFPipeline and IFSuperResolutionPipeline\"\n    super().add_stage(pipeline, num_inference_steps, stage_name, configs)\n    if isinstance(pipeline, IFSuperResolutionPipeline) or isinstance(\n        pipeline, StableDiffusionUpscalePipeline\n    ):\n        self.starting_step = 0\n        self.log_step = self.starting_step + self.num_inference_steps - 1\n</code></pre>"},{"location":"diffusers/kandinsky/","title":"Callbacks for \ud83e\udde8 Diffusers pipelines for Kandinsly","text":"<p>Callbacks for logging experiment details, configs and generated images for the Kandinsly family of pipelines from Diffusers \ud83e\udde8 pipelines to your Weights &amp; Biases workspace.</p> Callback Run-in-Colab WandB Run Kandinsky v2.1"},{"location":"diffusers/kandinsky/#wandb_addons.diffusers.callbacks.kandinsky.KandinskyCallback","title":"<code>KandinskyCallback</code>","text":"<p>             Bases: <code>BaseDiffusersCallback</code></p> <p>Callback for \ud83e\udde8 Diffusers logging the results of a <code>KandinskyCombinedPipeline</code> generation to Weights &amp; Biases.</p> <p>Features:</p> <ul> <li>The callback automatically logs basic configs like prompt, negative prompt,     etc. along with the generated image in a     <code>wandb.Table</code>.</li> <li>The callback also logs configs for both the experiment as well as pipelines     with the wandb run.</li> <li>No need to initialize a run, the callback automatically initialized and ends     runs gracefully.</li> </ul> <p>Example usage:</p> <p>You can fine an example notebook here.</p> <pre><code>import torch\nfrom diffusers import KandinskyCombinedPipeline\n\nfrom wandb_addons.diffusers import KandinskyCallback\n\n\npipe = KandinskyCombinedPipeline.from_pretrained(\n    \"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16\n)\npipe = pipe.to(\"cuda\")\n\nprompt = [\n    \"a photograph of an astronaut riding a horse\",\n    \"a photograph of a dragon\"\n]\nnegative_prompt = [\"ugly, deformed\", \"ugly, deformed\"]\nnum_images_per_prompt = 2\n\nconfigs = {\n    \"guidance_scale\": 4.0,\n    \"height\": 512,\n    \"width\": 512,\n    \"prior_guidance_scale\": 4.0,\n    \"prior_num_inference_steps\": 25,\n}\n\n# Create the WandB callback for StableDiffusionPipeline\ncallback = KandinskyCallback(\n    pipe,\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    wandb_project=\"diffusers\",\n    num_images_per_prompt=num_images_per_prompt,\n    configs=configs,\n)\n\n# Add the callback to the pipeline\nimage = pipe(\n    prompt,\n    negative_prompt=negative_prompt,\n    callback=callback,\n    num_images_per_prompt=num_images_per_prompt,\n    **configs,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>Union[DiffusionPipeline, KandinskyCombinedPipeline, KandinskyPipeline]</code> <p>The <code>KandinskyCombinedPipeline</code> or <code>KandinskyPipeline</code> from <code>diffusers</code>.</p> required <code>prompt</code> <code>Union[str, List[str]]</code> <p>The prompt or prompts to guide the image generation.</p> required <code>wandb_project</code> <code>Optional[str]</code> <p>The name of the project where you're sending the new run. The project is not necessary to be specified unless the run has automatically been initiatlized before the callback is defined.</p> required <code>wandb_entity</code> <code>Optional[str]</code> <p>An entity is a username or team name where you're sending runs. This entity must exist before you can send runs there, so make sure to create your account or team in the UI before starting to log runs. If you don't specify an entity, the run will be sent to your default entity, which is usually your username. Change your default entity in your settings under \"default location to create new projects\".</p> <code>None</code> <code>weave_mode</code> <code>bool</code> <p>Whether to use log to a weave board instead of W&amp;B dashboard or not. The weave mode logs the configs, generated images and timestamp in a <code>StreamTable</code> instead of a <code>wandb.Table</code> and does not require a wandb run to be initialized in order to start logging. This makes it possible to log muliple generations without having to initialize or terminate runs. Note that the parameter <code>wandb_entity</code> must be explicitly specified in order to use weave mode.</p> <code>False</code> <code>num_inference_steps</code> <code>int</code> <p>The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.</p> <code>100</code> <code>num_images_per_prompt</code> <code>Optional[int]</code> <p>The number of images to generate per prompt.</p> <code>1</code> <code>negative_prompt</code> <code>Optional[Union[str, List[str]]]</code> <p>The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored if <code>guidance_scale</code> is less than <code>1</code>).</p> <code>None</code> <code>configs</code> <code>Optional[Dict]</code> <p>Additional configs for the experiment you want to sync, for example, for example, <code>seed</code> could be a good config to be passed here.</p> <code>None</code> Source code in <code>wandb_addons/diffusers/callbacks/kandinsky.py</code> <pre><code>class KandinskyCallback(BaseDiffusersCallback):\n    \"\"\"Callback for [\ud83e\udde8 Diffusers](https://huggingface.co/docs/diffusers/index) logging\n    the results of a\n    [`KandinskyCombinedPipeline`](https://huggingface.co/docs/diffusers/api/pipelines/kandinsky#diffusers.KandinskyCombinedPipeline)\n    generation to Weights &amp; Biases.\n\n    !!! note \"Features:\"\n        - The callback automatically logs basic configs like prompt, negative prompt,\n            etc. along with the generated image in a\n            [`wandb.Table`](https://docs.wandb.ai/guides/tables).\n        - The callback also logs configs for both the experiment as well as pipelines\n            with the wandb run.\n        - No need to initialize a run, the callback automatically initialized and ends\n            runs gracefully.\n\n    !!! example \"Example usage:\"\n        You can fine an example notebook [here](../examples/kandinsky).\n\n        ```python\n        import torch\n        from diffusers import KandinskyCombinedPipeline\n\n        from wandb_addons.diffusers import KandinskyCallback\n\n\n        pipe = KandinskyCombinedPipeline.from_pretrained(\n            \"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16\n        )\n        pipe = pipe.to(\"cuda\")\n\n        prompt = [\n            \"a photograph of an astronaut riding a horse\",\n            \"a photograph of a dragon\"\n        ]\n        negative_prompt = [\"ugly, deformed\", \"ugly, deformed\"]\n        num_images_per_prompt = 2\n\n        configs = {\n            \"guidance_scale\": 4.0,\n            \"height\": 512,\n            \"width\": 512,\n            \"prior_guidance_scale\": 4.0,\n            \"prior_num_inference_steps\": 25,\n        }\n\n        # Create the WandB callback for StableDiffusionPipeline\n        callback = KandinskyCallback(\n            pipe,\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            wandb_project=\"diffusers\",\n            num_images_per_prompt=num_images_per_prompt,\n            configs=configs,\n        )\n\n        # Add the callback to the pipeline\n        image = pipe(\n            prompt,\n            negative_prompt=negative_prompt,\n            callback=callback,\n            num_images_per_prompt=num_images_per_prompt,\n            **configs,\n        )\n        ```\n\n    Arguments:\n        pipeline (Union[DiffusionPipeline, KandinskyCombinedPipeline, KandinskyPipeline]):\n            The `KandinskyCombinedPipeline` or `KandinskyPipeline` from `diffusers`.\n        prompt (Union[str, List[str]]): The prompt or prompts to guide the image\n            generation.\n        wandb_project (Optional[str]): The name of the project where you're sending\n            the new run. The project is not necessary to be specified unless the run\n            has automatically been initiatlized before the callback is defined.\n        wandb_entity (Optional[str]): An entity is a username or team name where\n            you're sending runs. This entity must exist before you can send runs there,\n            so make sure to create your account or team in the UI before starting to\n            log runs. If you don't specify an entity, the run will be sent to your\n            default entity, which is usually your username. Change your default entity\n            in [your settings](https://wandb.ai/settings) under \"default location to\n            create new projects\".\n        weave_mode (bool): Whether to use log to a\n            [weave board](https://docs.wandb.ai/guides/weave) instead of W&amp;B dashboard\n            or not. The weave mode logs the configs, generated images and timestamp in a\n            [`StreamTable`](https://docs.wandb.ai/guides/weave/streamtable) instead of a\n            `wandb.Table` and does not require a wandb run to be initialized in order to\n            start logging. This makes it possible to log muliple generations without\n            having to initialize or terminate runs. Note that the parameter\n            `wandb_entity` must be explicitly specified in order to use weave mode.\n        num_inference_steps (int): The number of denoising steps. More denoising steps\n            usually lead to a higher quality image at the expense of slower inference.\n        num_images_per_prompt (Optional[int]): The number of images to generate per\n            prompt.\n        negative_prompt (Optional[Union[str, List[str]]]): The prompt or prompts not\n            to guide the image generation. Ignored when not using guidance\n            (i.e., ignored if `guidance_scale` is less than `1`).\n        configs (Optional[Dict]): Additional configs for the experiment you want to\n            sync, for example, for example, `seed` could be a good config to be passed\n            here.\n    \"\"\"\n\n    def __init__(\n        self,\n        pipeline: Union[\n            DiffusionPipeline, KandinskyCombinedPipeline, KandinskyPipeline\n        ],\n        prompt: Union[str, List[str]],\n        wandb_project: str,\n        wandb_entity: Optional[str] = None,\n        weave_mode: bool = False,\n        num_inference_steps: int = 100,\n        num_images_per_prompt: Optional[int] = 1,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        configs: Optional[Dict] = None,\n        **kwargs\n    ) -&gt; None:\n        super().__init__(\n            pipeline,\n            prompt,\n            wandb_project,\n            wandb_entity,\n            weave_mode,\n            num_inference_steps,\n            num_images_per_prompt,\n            negative_prompt,\n            configs,\n            **kwargs\n        )\n        self.starting_step = 0\n        self.log_step = num_inference_steps - 1\n\n    def generate(self, latents: torch.FloatTensor) -&gt; List:\n        images = self.pipeline.movq.decode(latents, force_not_quantize=True)[\"sample\"]\n        images = images * 0.5 + 0.5\n        images = images.clamp(0, 1)\n        images = images.cpu().permute(0, 2, 3, 1).float().numpy()\n        images = self.pipeline.numpy_to_pil(images)\n        return images\n</code></pre>"},{"location":"diffusers/overview/","title":"Callbacks for \ud83e\udde8 Diffusers pipelines","text":"<p>Callbacks for logging experiment details, configs and generated images for multi-modal diffusion pipelines from Diffusers \ud83e\udde8 to your Weights &amp; Biases workspace or Weave Dashboard.</p> <p>In order to install the depensencies to use the integration, you can run:</p> <pre><code>git clone https://github.com/soumik12345/wandb-addons\npip install ./wandb-addons[huggingface]\n</code></pre> <p>For detailed documentation, check out the following:</p> <ul> <li>Auto-integrate</li> <li>DeepFloyd IF Callbacks</li> <li>Kandinsky Callback</li> <li>Stable Diffusion Callbacks</li> </ul> Callback Run-in-Colab WandB Run Stable Diffusion Stable Diffusion XL Stable Diffusion Image2Image Kandinsky v2.1 DeepFloyd IF Text-to-Image on Weights &amp; Biases Image-to-Image on Weights &amp; Biases Multi-pipeline Text-to-Image Experiments using DeepFloydIF on Weights &amp; Biases"},{"location":"diffusers/stable_diffusion/","title":"Callbacks for \ud83e\udde8 Diffusers pipelines for Stable Diffusion","text":"<p>Callbacks for logging experiment details, configs and generated images for the Stable Diffusion family of pipelines from Diffusers \ud83e\udde8 pipelines to your Weights &amp; Biases workspace.</p> Callback Run in Colab WandB Run Stable Diffusion Stable Diffusion XL Stable Diffusion Image2Image"},{"location":"diffusers/stable_diffusion/#wandb_addons.diffusers.callbacks.stable_diffusion.StableDiffusionCallback","title":"<code>StableDiffusionCallback</code>","text":"<p>             Bases: <code>BaseDiffusersCallback</code></p> <p>Callback for \ud83e\udde8 Diffusers logging the results of a <code>StableDiffusionPipeline</code> generation to Weights &amp; Biases.</p> <p>Features:</p> <ul> <li>The callback automatically logs basic configs like prompt, negative prompt,     etc. along with the generated image in a     <code>wandb.Table</code>.</li> <li>The callback also logs configs for both the experiment as well as pipelines     with the wandb run.</li> <li>No need to initialize a run, the callback automatically initialized and ends     runs gracefully.</li> </ul> <p>Example usage:</p> <p>You can fine an example notebook here.</p> <pre><code>import torch\nfrom diffusers import StableDiffusionPipeline\n\nfrom wandb_addons.diffusers import StableDiffusionCallback\n\n\npipeline = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16\n)\npipeline = pipeline.to(\"cuda\")\n\nprompt = [\n    \"a photograph of an astronaut riding a horse\",\n    \"a photograph of a dragon\"\n]\nnegative_prompt = [\"ugly, deformed\", \"ugly, deformed\"]\nnum_images_per_prompt = 2\nconfigs = {\n    \"eta\": 0.0,\n    \"guidance_rescale\": 0.0,\n}\n\n# Create the WandB callback for StableDiffusionPipeline\ncallback = StableDiffusionCallback(\n    pipe,\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    wandb_project=\"diffusers\",\n    num_images_per_prompt=num_images_per_prompt,\n    configs=configs,\n)\n\n# Add the callback to the pipeline\nimage = pipe(\n    prompt,\n    negative_prompt=negative_prompt,\n    callback=callback,\n    num_images_per_prompt=num_images_per_prompt,\n    **configs,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>StableDiffusionPipeline</code> <p>The <code>StableDiffusionPipeline</code> from <code>diffusers</code>.</p> required <code>prompt</code> <code>Union[str, List[str]]</code> <p>The prompt or prompts to guide the image generation.</p> required <code>wandb_project</code> <code>Optional[str]</code> <p>The name of the project where you're sending the new run. The project is not necessary to be specified unless the run has automatically been initiatlized before the callback is defined.</p> required <code>wandb_entity</code> <code>Optional[str]</code> <p>An entity is a username or team name where you're sending runs. This entity must exist before you can send runs there, so make sure to create your account or team in the UI before starting to log runs. If you don't specify an entity, the run will be sent to your default entity, which is usually your username. Change your default entity in your settings under \"default location to create new projects\".</p> <code>None</code> <code>weave_mode</code> <code>bool</code> <p>Whether to use log to a weave board instead of W&amp;B dashboard or not. The weave mode logs the configs, generated images and timestamp in a <code>StreamTable</code> instead of a <code>wandb.Table</code> and does not require a wandb run to be initialized in order to start logging. This makes it possible to log muliple generations without having to initialize or terminate runs. Note that the parameter <code>wandb_entity</code> must be explicitly specified in order to use weave mode.</p> <code>False</code> <code>num_inference_steps</code> <code>int</code> <p>The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.</p> <code>50</code> <code>num_images_per_prompt</code> <code>Optional[int]</code> <p>The number of images to generate per prompt.</p> <code>1</code> <code>guidance_scale</code> <code>float</code> <p>Guidance scale as defined in Classifier-Free Diffusion Guidance. <code>guidance_scale</code> is defined as <code>w</code> of equation 2. of Imagen Paper. Guidance scale is enabled by setting <code>guidance_scale &gt; 1</code>. Higher guidance scale encourages to generate images that are closely linked to the text <code>prompt</code>, usually at the expense of lower image quality.</p> <code>7.5</code> <code>negative_prompt</code> <code>Optional[Union[str, List[str]]]</code> <p>The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored if <code>guidance_scale</code> is less than <code>1</code>).</p> <code>None</code> <code>configs</code> <code>Optional[Dict]</code> <p>Additional configs for the experiment you want to sync, for example, for example, <code>seed</code> could be a good config to be passed here.</p> <code>None</code> Source code in <code>wandb_addons/diffusers/callbacks/stable_diffusion/text_to_image.py</code> <pre><code>class StableDiffusionCallback(BaseDiffusersCallback):\n    \"\"\"Callback for [\ud83e\udde8 Diffusers](https://huggingface.co/docs/diffusers/index) logging\n    the results of a\n    [`StableDiffusionPipeline`](https://huggingface.co/docs/diffusers/v0.9.0/en/api/pipelines/stable_diffusion#diffusers.StableDiffusionPipeline)\n    generation to Weights &amp; Biases.\n\n    !!! note \"Features:\"\n        - The callback automatically logs basic configs like prompt, negative prompt,\n            etc. along with the generated image in a\n            [`wandb.Table`](https://docs.wandb.ai/guides/tables).\n        - The callback also logs configs for both the experiment as well as pipelines\n            with the wandb run.\n        - No need to initialize a run, the callback automatically initialized and ends\n            runs gracefully.\n\n    !!! example \"Example usage:\"\n        You can fine an example notebook [here](../examples/stable_diffusion).\n\n        ```python\n        import torch\n        from diffusers import StableDiffusionPipeline\n\n        from wandb_addons.diffusers import StableDiffusionCallback\n\n\n        pipeline = StableDiffusionPipeline.from_pretrained(\n            \"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16\n        )\n        pipeline = pipeline.to(\"cuda\")\n\n        prompt = [\n            \"a photograph of an astronaut riding a horse\",\n            \"a photograph of a dragon\"\n        ]\n        negative_prompt = [\"ugly, deformed\", \"ugly, deformed\"]\n        num_images_per_prompt = 2\n        configs = {\n            \"eta\": 0.0,\n            \"guidance_rescale\": 0.0,\n        }\n\n        # Create the WandB callback for StableDiffusionPipeline\n        callback = StableDiffusionCallback(\n            pipe,\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            wandb_project=\"diffusers\",\n            num_images_per_prompt=num_images_per_prompt,\n            configs=configs,\n        )\n\n        # Add the callback to the pipeline\n        image = pipe(\n            prompt,\n            negative_prompt=negative_prompt,\n            callback=callback,\n            num_images_per_prompt=num_images_per_prompt,\n            **configs,\n        )\n        ```\n\n    Arguments:\n        pipeline (diffusers.StableDiffusionPipeline): The `StableDiffusionPipeline`\n            from `diffusers`.\n        prompt (Union[str, List[str]]): The prompt or prompts to guide the image\n            generation.\n        wandb_project (Optional[str]): The name of the project where you're sending\n            the new run. The project is not necessary to be specified unless the run\n            has automatically been initiatlized before the callback is defined.\n        wandb_entity (Optional[str]): An entity is a username or team name where\n            you're sending runs. This entity must exist before you can send runs there,\n            so make sure to create your account or team in the UI before starting to\n            log runs. If you don't specify an entity, the run will be sent to your\n            default entity, which is usually your username. Change your default entity\n            in [your settings](https://wandb.ai/settings) under \"default location to\n            create new projects\".\n        weave_mode (bool): Whether to use log to a\n            [weave board](https://docs.wandb.ai/guides/weave) instead of W&amp;B dashboard\n            or not. The weave mode logs the configs, generated images and timestamp in a\n            [`StreamTable`](https://docs.wandb.ai/guides/weave/streamtable) instead of a\n            `wandb.Table` and does not require a wandb run to be initialized in order to\n            start logging. This makes it possible to log muliple generations without\n            having to initialize or terminate runs. Note that the parameter\n            `wandb_entity` must be explicitly specified in order to use weave mode.\n        num_inference_steps (int): The number of denoising steps. More denoising steps\n            usually lead to a higher quality image at the expense of slower inference.\n        num_images_per_prompt (Optional[int]): The number of images to generate per\n            prompt.\n        guidance_scale (float): Guidance scale as defined in\n            [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n            `guidance_scale` is defined as `w` of equation 2. of\n            [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is\n            enabled by setting `guidance_scale &gt; 1`. Higher guidance scale encourages\n            to generate images that are closely linked to the text `prompt`, usually\n            at the expense of lower image quality.\n        negative_prompt (Optional[Union[str, List[str]]]): The prompt or prompts not\n            to guide the image generation. Ignored when not using guidance\n            (i.e., ignored if `guidance_scale` is less than `1`).\n        configs (Optional[Dict]): Additional configs for the experiment you want to\n            sync, for example, for example, `seed` could be a good config to be passed\n            here.\n    \"\"\"\n\n    def __init__(\n        self,\n        pipeline: Union[DiffusionPipeline, StableDiffusionPipeline],\n        prompt: Union[str, List[str]],\n        wandb_project: str,\n        wandb_entity: Optional[str] = None,\n        weave_mode: bool = False,\n        guidance_scale: float = 7.5,\n        num_inference_steps: int = 50,\n        num_images_per_prompt: Optional[int] = 1,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        configs: Optional[Dict] = None,\n        **kwargs,\n    ) -&gt; None:\n        super().__init__(\n            pipeline,\n            prompt,\n            wandb_project,\n            wandb_entity,\n            weave_mode,\n            num_inference_steps,\n            num_images_per_prompt,\n            negative_prompt,\n            configs,\n            **kwargs,\n        )\n        self.starting_step = 0\n        self.log_step = self.num_inference_steps - 1\n        self.guidance_scale = guidance_scale\n        self.do_classifier_free_guidance = guidance_scale &gt; 1.0\n        additional_configs = {\n            \"guidance_scale\": self.guidance_scale,\n            \"do_classifier_free_guidance\": self.do_classifier_free_guidance,\n        }\n        if not self.weave_mode:\n            wandb.config.update(additional_configs)\n        else:\n            self.configs.update(additional_configs)\n\n    def build_wandb_table(self) -&gt; None:\n        super().build_wandb_table()\n        self.table_columns += [\"Guidance-Scale\", \"Do-Classifier-Free-Guidance\"]\n\n    def populate_table_row(\n        self, prompt: str, negative_prompt: str, image: Image\n    ) -&gt; None:\n        super().populate_table_row(prompt, negative_prompt, image)\n        if not self.weave_mode:\n            self.table_row += [self.guidance_scale, self.do_classifier_free_guidance]\n\n    def generate(self, latents: torch.FloatTensor) -&gt; List:\n        images = self.pipeline.decode_latents(latents)\n        images, _ = self.pipeline.run_safety_checker(\n            images, self.pipeline._execution_device, latents.dtype\n        )\n        images = self.pipeline.numpy_to_pil(images)\n        return images\n</code></pre>"},{"location":"diffusers/stable_diffusion/#wandb_addons.diffusers.callbacks.stable_diffusion.StableDiffusionImg2ImgCallback","title":"<code>StableDiffusionImg2ImgCallback</code>","text":"<p>             Bases: <code>BaseImage2ImageCallback</code></p> <p>Callback for \ud83e\udde8 Diffusers logging the results of a <code>StableDiffusionImg2ImgPipeline</code> generation to Weights &amp; Biases.</p> <p>Features:</p> <ul> <li>The callback automatically logs basic configs like input image, prompt,     negative prompt, etc. along with the generated image in a     <code>wandb.Table</code>.</li> <li>The callback also logs configs for both the experiment as well as pipelines     with the wandb run.</li> <li>No need to initialize a run, the callback automatically initialized and ends     runs gracefully.</li> </ul> <p>Example usage:</p> <p>You can fine an example notebook here.</p> <pre><code>import requests\nimport torch\nfrom PIL import Image\nfrom io import BytesIO\n\nfrom diffusers import StableDiffusionImg2ImgPipeline\n\nfrom wandb_addons.diffusers import StableDiffusionImg2ImgCallback\n\n\npipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16\n)\npipe = pipe.to(\"cuda\")\n\nurl = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n\nresponse = requests.get(url)\ninit_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\ninit_image = init_image.resize((768, 512))\n\nprompt = \"A fantasy landscape, trending on artstation\"\n\nnum_images_per_prompt = 1\nstrength = 0.75\n\ncallback = StableDiffusionImg2ImgCallback(\n    pipeline=pipe, prompt=prompt,\n    input_images=init_image,\n    wandb_project=\"diffusers\",\n    num_images_per_prompt=num_images_per_prompt,\n    strength=strength,\n)\n\nresults = pipe(\n    prompt=prompt,\n    image=init_image,\n    strength=strength,\n    num_images_per_prompt=num_images_per_prompt,\n    callback=callback\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>StableDiffusionPipeline</code> <p>The <code>StableDiffusionPipeline</code> from <code>diffusers</code>.</p> required <code>prompt</code> <code>Union[str, List[str]]</code> <p>The prompt or prompts to guide the image generation.</p> required <code>input_images</code> <code>PipelineImageInput</code> <p>The input image, numpy array or tensor representing an image batch to be used as the starting point. For both numpy array and pytorch tensor, the expected value range is between [0, 1] If it's a tensor or a list or tensors, the expected shape should be <code>(B, C, H, W)</code> or <code>(C, H, W)</code>. If it is a numpy array or a list of arrays, the expected shape should be (B, H, W, C) or (H, W, C) It can also accept image latents as image, but if passing latents directly it is not encoded again.</p> required <code>wandb_project</code> <code>Optional[str]</code> <p>The name of the project where you're sending the new run. The project is not necessary to be specified unless the run has automatically been initiatlized before the callback is defined.</p> required <code>wandb_entity</code> <code>Optional[str]</code> <p>An entity is a username or team name where you're sending runs. This entity must exist before you can send runs there, so make sure to create your account or team in the UI before starting to log runs. If you don't specify an entity, the run will be sent to your default entity, which is usually your username. Change your default entity in your settings under \"default location to create new projects\".</p> <code>None</code> <code>weave_mode</code> <code>bool</code> <p>Whether to use log to a weave board instead of W&amp;B dashboard or not. The weave mode logs the configs, generated images and timestamp in a <code>StreamTable</code> instead of a <code>wandb.Table</code> and does not require a wandb run to be initialized in order to start logging. This makes it possible to log muliple generations without having to initialize or terminate runs. Note that the parameter <code>wandb_entity</code> must be explicitly specified in order to use weave mode.</p> <code>False</code> <code>num_inference_steps</code> <code>int</code> <p>The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.</p> <code>50</code> <code>strength</code> <code>Optional[float]</code> <p>Indicates extent to transform the reference image. Must be between 0 and 1. image is used as a starting point and more noise is added the higher the strength. The number of denoising steps depends on the amount of noise initially added. When strength is 1, added noise is maximum and the denoising process runs for the full number of iterations specified in <code>num_inference_steps</code>. A value of 1 essentially ignores image.</p> <code>0.8</code> <code>guidance_scale</code> <code>float</code> <p>Guidance scale as defined in Classifier-Free Diffusion Guidance. <code>guidance_scale</code> is defined as <code>w</code> of equation 2. of Imagen Paper. Guidance scale is enabled by setting <code>guidance_scale &gt; 1</code>. Higher guidance scale encourages to generate images that are closely linked to the text <code>prompt</code>, usually at the expense of lower image quality.</p> <code>7.5</code> <code>num_images_per_prompt</code> <code>Optional[int]</code> <p>The number of images to generate per prompt.</p> <code>1</code> <code>negative_prompt</code> <code>Optional[Union[str, List[str]]]</code> <p>The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored if <code>guidance_scale</code> is less than <code>1</code>).</p> <code>None</code> <code>configs</code> <code>Optional[Dict]</code> <p>Additional configs for the experiment you want to sync, for example, for example, <code>seed</code> could be a good config to be passed here.</p> <code>None</code> Source code in <code>wandb_addons/diffusers/callbacks/stable_diffusion/image_to_image.py</code> <pre><code>class StableDiffusionImg2ImgCallback(BaseImage2ImageCallback):\n    \"\"\"Callback for [\ud83e\udde8 Diffusers](https://huggingface.co/docs/diffusers/index) logging\n    the results of a\n    [`StableDiffusionImg2ImgPipeline`](https://huggingface.co/docs/diffusers/v0.21.0/en/api/pipelines/stable_diffusion/img2img#diffusers.StableDiffusionImg2ImgPipeline)\n    generation to Weights &amp; Biases.\n\n    !!! note \"Features:\"\n        - The callback automatically logs basic configs like input image, prompt,\n            negative prompt, etc. along with the generated image in a\n            [`wandb.Table`](https://docs.wandb.ai/guides/tables).\n        - The callback also logs configs for both the experiment as well as pipelines\n            with the wandb run.\n        - No need to initialize a run, the callback automatically initialized and ends\n            runs gracefully.\n\n    !!! example \"Example usage:\"\n        You can fine an example notebook [here](../examples/stable_diffusion_img2img).\n\n        ```python\n        import requests\n        import torch\n        from PIL import Image\n        from io import BytesIO\n\n        from diffusers import StableDiffusionImg2ImgPipeline\n\n        from wandb_addons.diffusers import StableDiffusionImg2ImgCallback\n\n\n        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n            \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16\n        )\n        pipe = pipe.to(\"cuda\")\n\n        url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n\n        response = requests.get(url)\n        init_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n        init_image = init_image.resize((768, 512))\n\n        prompt = \"A fantasy landscape, trending on artstation\"\n\n        num_images_per_prompt = 1\n        strength = 0.75\n\n        callback = StableDiffusionImg2ImgCallback(\n            pipeline=pipe, prompt=prompt,\n            input_images=init_image,\n            wandb_project=\"diffusers\",\n            num_images_per_prompt=num_images_per_prompt,\n            strength=strength,\n        )\n\n        results = pipe(\n            prompt=prompt,\n            image=init_image,\n            strength=strength,\n            num_images_per_prompt=num_images_per_prompt,\n            callback=callback\n        )\n        ```\n\n    Arguments:\n        pipeline (diffusers.StableDiffusionPipeline): The `StableDiffusionPipeline` from\n            `diffusers`.\n        prompt (Union[str, List[str]]): The prompt or prompts to guide the image\n            generation.\n        input_images (PipelineImageInput): The input image, numpy array or tensor\n            representing an image batch to be used as the starting point. For both numpy\n            array and pytorch tensor, the expected value range is between [0, 1] If it's\n            a tensor or a list or tensors, the expected shape should be `(B, C, H, W)`\n            or `(C, H, W)`. If it is a numpy array or a list of arrays, the expected\n            shape should be (B, H, W, C) or (H, W, C) It can also accept image latents\n            as image, but if passing latents directly it is not encoded again.\n        wandb_project (Optional[str]): The name of the project where you're sending\n            the new run. The project is not necessary to be specified unless the run\n            has automatically been initiatlized before the callback is defined.\n        wandb_entity (Optional[str]): An entity is a username or team name where\n            you're sending runs. This entity must exist before you can send runs there,\n            so make sure to create your account or team in the UI before starting to\n            log runs. If you don't specify an entity, the run will be sent to your\n            default entity, which is usually your username. Change your default entity\n            in [your settings](https://wandb.ai/settings) under \"default location to\n            create new projects\".\n        weave_mode (bool): Whether to use log to a\n            [weave board](https://docs.wandb.ai/guides/weave) instead of W&amp;B dashboard\n            or not. The weave mode logs the configs, generated images and timestamp in a\n            [`StreamTable`](https://docs.wandb.ai/guides/weave/streamtable) instead of a\n            `wandb.Table` and does not require a wandb run to be initialized in order to\n            start logging. This makes it possible to log muliple generations without\n            having to initialize or terminate runs. Note that the parameter\n            `wandb_entity` must be explicitly specified in order to use weave mode.\n        num_inference_steps (int): The number of denoising steps. More denoising steps\n            usually lead to a higher quality image at the expense of slower inference.\n        strength (Optional[float]): Indicates extent to transform the reference image.\n            Must be between 0 and 1. image is used as a starting point and more noise\n            is added the higher the strength. The number of denoising steps depends on\n            the amount of noise initially added. When strength is 1, added noise is\n            maximum and the denoising process runs for the full number of iterations\n            specified in `num_inference_steps`. A value of 1 essentially ignores image.\n        guidance_scale (float): Guidance scale as defined in\n            [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n            `guidance_scale` is defined as `w` of equation 2. of\n            [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is\n            enabled by setting `guidance_scale &gt; 1`. Higher guidance scale encourages\n            to generate images that are closely linked to the text `prompt`, usually\n            at the expense of lower image quality.\n        num_images_per_prompt (Optional[int]): The number of images to generate per\n            prompt.\n        negative_prompt (Optional[Union[str, List[str]]]): The prompt or prompts not\n            to guide the image generation. Ignored when not using guidance\n            (i.e., ignored if `guidance_scale` is less than `1`).\n        configs (Optional[Dict]): Additional configs for the experiment you want to\n            sync, for example, for example, `seed` could be a good config to be passed\n            here.\n    \"\"\"\n\n    def __init__(\n        self,\n        pipeline: Union[DiffusionPipeline, StableDiffusionImg2ImgPipeline],\n        prompt: Union[str, List[str]],\n        input_images: PipelineImageInput,\n        wandb_project: str,\n        wandb_entity: Optional[str] = None,\n        weave_mode: bool = False,\n        num_inference_steps: int = 50,\n        strength: Optional[float] = 0.8,\n        guidance_scale: Optional[float] = 7.5,\n        num_images_per_prompt: Optional[int] = 1,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        configs: Optional[Dict] = None,\n        **kwargs,\n    ) -&gt; None:\n        super().__init__(\n            pipeline=pipeline,\n            prompt=prompt,\n            input_images=input_images,\n            wandb_project=wandb_project,\n            wandb_entity=wandb_entity,\n            weave_mode=weave_mode,\n            num_inference_steps=num_inference_steps,\n            num_images_per_prompt=num_images_per_prompt,\n            negative_prompt=negative_prompt,\n            configs=configs,\n            **kwargs,\n        )\n        self.strength = strength\n        self.guidance_scale = guidance_scale\n        self.do_classifier_free_guidance = guidance_scale &gt; 1.0\n        additional_configs = {\n            \"strength\": self.strength,\n            \"guidance_scale\": self.guidance_scale,\n            \"do_classifier_free_guidance\": self.do_classifier_free_guidance,\n        }\n        if self.weave_mode:\n            self.configs.update(additional_configs)\n        else:\n            wandb.config.update(additional_configs)\n\n    def at_initial_step(self):\n        super().at_initial_step()\n        _, self.log_step = self.pipeline.get_timesteps(\n            self.num_inference_steps, self.strength, self.pipeline._execution_device\n        )\n\n    def generate(self, latents: torch.FloatTensor) -&gt; List:\n        images = self.pipeline.vae.decode(\n            latents / self.pipeline.vae.config.scaling_factor, return_dict=False\n        )[0]\n        images, has_nsfw_concept = self.pipeline.run_safety_checker(\n            images, self.pipeline._execution_device, latents.dtype\n        )\n        do_denormalize = (\n            [True] * images.shape[0]\n            if has_nsfw_concept is None\n            else [not has_nsfw for has_nsfw in has_nsfw_concept]\n        )\n        images = self.pipeline.image_processor.postprocess(\n            images, output_type=\"pil\", do_denormalize=do_denormalize\n        )\n        return images\n</code></pre>"},{"location":"diffusers/stable_diffusion/#wandb_addons.diffusers.callbacks.stable_diffusion.StableDiffusionXLCallback","title":"<code>StableDiffusionXLCallback</code>","text":"<p>             Bases: <code>BaseMultiPipelineCallback</code></p> <p>Callback for logging the resulst of a text-to-image workflow with Stable Diffusion XL to Weights &amp; Biases.</p> <p>Features:</p> <ul> <li>The callback automatically logs basic configs like prompt, negative prompt,     etc. along with the generated image in a     <code>wandb.Table</code>.</li> <li>The callback also logs configs for both the experiment as well as pipelines     with the wandb run.</li> <li>No need to initialize a run, the callback automatically initialized and ends     runs gracefully.</li> <li>Supports logging base and refinement stages of a workflow using a single     callback.</li> </ul> <p>Example usage:</p> <p>You can fine an example notebook here.</p> <pre><code>from functools import partial\n\nimport torch\n\nfrom diffusers import StableDiffusionXLPipeline, StableDiffusionXLImg2ImgPipeline\nfrom wandb_addons.diffusers import StableDiffusionXLCallback\n\n\nbase_pipeline = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n    use_safetensors=True,\n)\n\nbase_pipeline.enable_model_cpu_offload()\n\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nnum_inference_steps = 50\n\ncallback = StableDiffusionXLCallback(\n    pipeline=base_pipeline,\n    prompt=prompt,\n    wandb_project=\"diffusers-sdxl\",\n    wandb_entity=\"geekyrakshit\",\n    weave_mode=True,\n    num_inference_steps=num_inference_steps,\n    initial_stage_name=\"base\",\n)\n\nimage = base_pipeline(\n    prompt=prompt,\n    output_type=\"latent\",\n    num_inference_steps=num_inference_steps,\n    callback=partial(callback, end_experiment=False)\n).images[0]\n\nrefiner_pipeline = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n    text_encoder_2=base_pipeline.text_encoder_2,\n    vae=base_pipeline.vae,\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n    variant=\"fp16\",\n)\nrefiner_pipeline.enable_model_cpu_offload()\n\nnum_inference_steps = 50\nstrength = 0.3\n\ncallback.add_refiner_stage(\n    refiner_pipeline, num_inference_steps=num_inference_steps, strength=strength\n)\n\nimage = refiner_pipeline(\n    prompt=prompt, image=image[None, :], callback=callback\n).images[0]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>DiffusionPipeline</code> <p>The <code>DiffusionPipeline</code> from <code>diffusers</code>.</p> required <code>prompt</code> <code>Union[str, List[str]]</code> <p>The prompt or prompts to guide the image generation.</p> required <code>wandb_project</code> <code>Optional[str]</code> <p>The name of the project where you're sending the new run. The project is not necessary to be specified unless the run has automatically been initiatlized before the callback is defined.</p> required <code>wandb_entity</code> <code>Optional[str]</code> <p>An entity is a username or team name where you're sending runs. This entity must exist before you can send runs there, so make sure to create your account or team in the UI before starting to log runs. If you don't specify an entity, the run will be sent to your default entity, which is usually your username. Change your default entity in your settings under \"default location to create new projects\".</p> <code>None</code> <code>weave_mode</code> <code>bool</code> <p>Whether to use log to a weave board instead of W&amp;B dashboard or not. The weave mode logs the configs, generated images and timestamp in a <code>StreamTable</code> instead of a <code>wandb.Table</code> and does not require a wandb run to be initialized in order to start logging. This makes it possible to log muliple generations without having to initialize or terminate runs. Note that the parameter <code>wandb_entity</code> must be explicitly specified in order to use weave mode.</p> <code>False</code> <code>num_inference_steps</code> <code>int</code> <p>The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.</p> <code>50</code> <code>num_images_per_prompt</code> <code>Optional[int]</code> <p>The number of images to generate per prompt.</p> <code>1</code> <code>negative_prompt</code> <code>Optional[Union[str, List[str]]]</code> <p>The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored if <code>guidance_scale</code> is less than <code>1</code>).</p> <code>None</code> <code>initial_stage_name</code> <code>Optional[str]</code> <p>The name of the initial stage. If not specified, it would be set to <code>\"stage_1\"</code>.</p> <code>'Base-Pipeline'</code> <code>configs</code> <code>Optional[Dict]</code> <p>Additional configs for the experiment you want to sync, for example, for example, <code>seed</code> could be a good config to be passed here.</p> <code>None</code> Source code in <code>wandb_addons/diffusers/callbacks/stable_diffusion/sdxl_t2i.py</code> <pre><code>class StableDiffusionXLCallback(BaseMultiPipelineCallback):\n    \"\"\"Callback for logging the resulst of a text-to-image workflow with\n    [Stable Diffusion XL](https://huggingface.co/docs/diffusers/v0.21.0/en/using-diffusers/sdxl#texttoimage)\n    to Weights &amp; Biases.\n\n    !!! note \"Features:\"\n        - The callback automatically logs basic configs like prompt, negative prompt,\n            etc. along with the generated image in a\n            [`wandb.Table`](https://docs.wandb.ai/guides/tables).\n        - The callback also logs configs for both the experiment as well as pipelines\n            with the wandb run.\n        - No need to initialize a run, the callback automatically initialized and ends\n            runs gracefully.\n        - Supports logging base and refinement stages of a workflow using a single\n            callback.\n\n    !!! example \"Example usage:\"\n        You can fine an example notebook [here](../examples/sdxl).\n\n        ```python\n        from functools import partial\n\n        import torch\n\n        from diffusers import StableDiffusionXLPipeline, StableDiffusionXLImg2ImgPipeline\n        from wandb_addons.diffusers import StableDiffusionXLCallback\n\n\n        base_pipeline = StableDiffusionXLPipeline.from_pretrained(\n            \"stabilityai/stable-diffusion-xl-base-1.0\",\n            torch_dtype=torch.float16,\n            variant=\"fp16\",\n            use_safetensors=True,\n        )\n\n        base_pipeline.enable_model_cpu_offload()\n\n        prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\n        num_inference_steps = 50\n\n        callback = StableDiffusionXLCallback(\n            pipeline=base_pipeline,\n            prompt=prompt,\n            wandb_project=\"diffusers-sdxl\",\n            wandb_entity=\"geekyrakshit\",\n            weave_mode=True,\n            num_inference_steps=num_inference_steps,\n            initial_stage_name=\"base\",\n        )\n\n        image = base_pipeline(\n            prompt=prompt,\n            output_type=\"latent\",\n            num_inference_steps=num_inference_steps,\n            callback=partial(callback, end_experiment=False)\n        ).images[0]\n\n        refiner_pipeline = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n            \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n            text_encoder_2=base_pipeline.text_encoder_2,\n            vae=base_pipeline.vae,\n            torch_dtype=torch.float16,\n            use_safetensors=True,\n            variant=\"fp16\",\n        )\n        refiner_pipeline.enable_model_cpu_offload()\n\n        num_inference_steps = 50\n        strength = 0.3\n\n        callback.add_refiner_stage(\n            refiner_pipeline, num_inference_steps=num_inference_steps, strength=strength\n        )\n\n        image = refiner_pipeline(\n            prompt=prompt, image=image[None, :], callback=callback\n        ).images[0]\n        ```\n\n    Arguments:\n        pipeline (diffusers.DiffusionPipeline): The `DiffusionPipeline` from\n            `diffusers`.\n        prompt (Union[str, List[str]]): The prompt or prompts to guide the image\n            generation.\n        wandb_project (Optional[str]): The name of the project where you're sending\n            the new run. The project is not necessary to be specified unless the run\n            has automatically been initiatlized before the callback is defined.\n        wandb_entity (Optional[str]): An entity is a username or team name where\n            you're sending runs. This entity must exist before you can send runs there,\n            so make sure to create your account or team in the UI before starting to\n            log runs. If you don't specify an entity, the run will be sent to your\n            default entity, which is usually your username. Change your default entity\n            in [your settings](https://wandb.ai/settings) under \"default location to\n            create new projects\".\n        weave_mode (bool): Whether to use log to a\n            [weave board](https://docs.wandb.ai/guides/weave) instead of W&amp;B dashboard\n            or not. The weave mode logs the configs, generated images and timestamp in a\n            [`StreamTable`](https://docs.wandb.ai/guides/weave/streamtable) instead of a\n            `wandb.Table` and does not require a wandb run to be initialized in order to\n            start logging. This makes it possible to log muliple generations without\n            having to initialize or terminate runs. Note that the parameter\n            `wandb_entity` must be explicitly specified in order to use weave mode.\n        num_inference_steps (int): The number of denoising steps. More denoising steps\n            usually lead to a higher quality image at the expense of slower inference.\n        num_images_per_prompt (Optional[int]): The number of images to generate per\n            prompt.\n        negative_prompt (Optional[Union[str, List[str]]]): The prompt or prompts not\n            to guide the image generation. Ignored when not using guidance\n            (i.e., ignored if `guidance_scale` is less than `1`).\n        initial_stage_name (Optional[str]): The name of the initial stage. If not\n            specified, it would be set to `\"stage_1\"`.\n        configs (Optional[Dict]): Additional configs for the experiment you want to\n            sync, for example, for example, `seed` could be a good config to be passed\n            here.\n    \"\"\"\n\n    def __init__(\n        self,\n        pipeline: Union[DiffusionPipeline, StableDiffusionXLPipeline],\n        prompt: Union[str, List[str]],\n        wandb_project: str,\n        wandb_entity: Optional[str] = None,\n        weave_mode: bool = False,\n        num_inference_steps: int = 50,\n        num_images_per_prompt: Optional[int] = 1,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        configs: Optional[Dict] = None,\n        initial_stage_name: Optional[str] = \"Base-Pipeline\",\n        **kwargs\n    ) -&gt; None:\n        super().__init__(\n            pipeline,\n            prompt,\n            wandb_project,\n            wandb_entity,\n            weave_mode,\n            num_inference_steps,\n            num_images_per_prompt,\n            negative_prompt,\n            initial_stage_name,\n            configs,\n            **kwargs\n        )\n        self.starting_step = 0\n        self.log_step = self.num_inference_steps - 1\n\n    def generate(self, latents: torch.FloatTensor) -&gt; List:\n        needs_upcasting = (\n            self.pipeline.vae.dtype == torch.float16\n            and self.pipeline.vae.config.force_upcast\n        )\n        if needs_upcasting:\n            self.pipeline.upcast_vae()\n            latents = latents.to(\n                next(iter(self.pipeline.vae.post_quant_conv.parameters())).dtype\n            )\n        images = self.pipeline.vae.decode(\n            latents / self.pipeline.vae.config.scaling_factor, return_dict=False\n        )[0]\n        if needs_upcasting:\n            self.pipeline.vae.to(dtype=torch.float16)\n        if self.pipeline.watermark is not None:\n            images = self.pipeline.watermark.apply_watermark(images)\n        images = self.pipeline.image_processor.postprocess(images, output_type=\"pil\")\n        self.pipeline.maybe_free_model_hooks()\n        return images\n\n    def add_refiner_stage(\n        self,\n        pipeline: StableDiffusionXLImg2ImgPipeline,\n        num_inference_steps: Optional[int] = None,\n        strength: Optional[float] = 0.3,\n        configs: Optional[Dict] = None,\n    ):\n        \"\"\"Add the refinement stage to the callback to log the results of the refiner\n        pipeline.\n\n        Arguments:\n            pipeline (diffusers.DiffusionPipeline): The `DiffusionPipeline` from\n                for the new stage.\n            num_inference_steps (Optional[int]): The number of denoising steps for the\n                new stage. More denoising steps usually lead to a higher quality image\n                at the expense of slower inference.\n            strength (Optional[float]): Conceptually, indicates how much to transform\n                the reference image. Must be between 0 and 1. image will be used as a\n                starting point, adding more noise to it the larger the strength. The\n                number of denoising steps depends on the amount of noise initially\n                added. When strength is 1, added noise will be maximum and the\n                denoising process will run for the full number of iterations specified\n                in num_inference_steps. A value of 1, therefore, essentially ignores\n                image. Note that in the case of `denoising_start` being declared as an\n                integer, the value of strength will be ignored.\n            configs (Optional[Dict]): Additional configs for the new stage you want to\n                sync, for example, for example, `seed` could be a good config to be\n                passed here.\n        \"\"\"\n        self.strength = strength\n        super().add_stage(pipeline, num_inference_steps, \"Refiner-Pipeline\", configs)\n        self.starting_step = 0\n        _, self.log_step = self.pipeline.get_timesteps(\n            self.num_inference_steps, self.strength, self.pipeline._execution_device\n        )\n        self.log_step = self.starting_step + self.log_step - 1\n</code></pre>"},{"location":"diffusers/stable_diffusion/#wandb_addons.diffusers.callbacks.stable_diffusion.StableDiffusionXLCallback.add_refiner_stage","title":"<code>add_refiner_stage(pipeline, num_inference_steps=None, strength=0.3, configs=None)</code>","text":"<p>Add the refinement stage to the callback to log the results of the refiner pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>DiffusionPipeline</code> <p>The <code>DiffusionPipeline</code> from for the new stage.</p> required <code>num_inference_steps</code> <code>Optional[int]</code> <p>The number of denoising steps for the new stage. More denoising steps usually lead to a higher quality image at the expense of slower inference.</p> <code>None</code> <code>strength</code> <code>Optional[float]</code> <p>Conceptually, indicates how much to transform the reference image. Must be between 0 and 1. image will be used as a starting point, adding more noise to it the larger the strength. The number of denoising steps depends on the amount of noise initially added. When strength is 1, added noise will be maximum and the denoising process will run for the full number of iterations specified in num_inference_steps. A value of 1, therefore, essentially ignores image. Note that in the case of <code>denoising_start</code> being declared as an integer, the value of strength will be ignored.</p> <code>0.3</code> <code>configs</code> <code>Optional[Dict]</code> <p>Additional configs for the new stage you want to sync, for example, for example, <code>seed</code> could be a good config to be passed here.</p> <code>None</code> Source code in <code>wandb_addons/diffusers/callbacks/stable_diffusion/sdxl_t2i.py</code> <pre><code>def add_refiner_stage(\n    self,\n    pipeline: StableDiffusionXLImg2ImgPipeline,\n    num_inference_steps: Optional[int] = None,\n    strength: Optional[float] = 0.3,\n    configs: Optional[Dict] = None,\n):\n    \"\"\"Add the refinement stage to the callback to log the results of the refiner\n    pipeline.\n\n    Arguments:\n        pipeline (diffusers.DiffusionPipeline): The `DiffusionPipeline` from\n            for the new stage.\n        num_inference_steps (Optional[int]): The number of denoising steps for the\n            new stage. More denoising steps usually lead to a higher quality image\n            at the expense of slower inference.\n        strength (Optional[float]): Conceptually, indicates how much to transform\n            the reference image. Must be between 0 and 1. image will be used as a\n            starting point, adding more noise to it the larger the strength. The\n            number of denoising steps depends on the amount of noise initially\n            added. When strength is 1, added noise will be maximum and the\n            denoising process will run for the full number of iterations specified\n            in num_inference_steps. A value of 1, therefore, essentially ignores\n            image. Note that in the case of `denoising_start` being declared as an\n            integer, the value of strength will be ignored.\n        configs (Optional[Dict]): Additional configs for the new stage you want to\n            sync, for example, for example, `seed` could be a good config to be\n            passed here.\n    \"\"\"\n    self.strength = strength\n    super().add_stage(pipeline, num_inference_steps, \"Refiner-Pipeline\", configs)\n    self.starting_step = 0\n    _, self.log_step = self.pipeline.get_timesteps(\n        self.num_inference_steps, self.strength, self.pipeline._execution_device\n    )\n    self.log_step = self.starting_step + self.log_step - 1\n</code></pre>"},{"location":"diffusers/examples/deepfloyd_if/","title":"Deepfloyd if","text":"In\u00a0[\u00a0]: Copied! <pre>!git clone https://github.com/soumik12345/wandb-addons\n!pip install ./wandb-addons[huggingface]\n</pre> !git clone https://github.com/soumik12345/wandb-addons !pip install ./wandb-addons[huggingface] In\u00a0[\u00a0]: Copied! <pre>import gc\nfrom functools import partial\n\nimport torch\n\nfrom diffusers import IFPipeline, IFSuperResolutionPipeline, StableDiffusionUpscalePipeline\nfrom wandb_addons.diffusers import IFCallback\n</pre> import gc from functools import partial  import torch  from diffusers import IFPipeline, IFSuperResolutionPipeline, StableDiffusionUpscalePipeline from wandb_addons.diffusers import IFCallback In\u00a0[\u00a0]: Copied! <pre>pipeline_1 = IFPipeline.from_pretrained(\n    \"DeepFloyd/IF-I-XL-v1.0\", variant=\"fp16\", torch_dtype=torch.float16\n)\npipeline_1.enable_model_cpu_offload()\n</pre> pipeline_1 = IFPipeline.from_pretrained(     \"DeepFloyd/IF-I-XL-v1.0\", variant=\"fp16\", torch_dtype=torch.float16 ) pipeline_1.enable_model_cpu_offload() In\u00a0[\u00a0]: Copied! <pre>prompt = 'a photo of a smiling bee wearing a yellow hoodie and blue sunglasses standing in front of the eiffel tower holding a sign that says \"Weights and Biases\"'\nprompt_embeds, negative_embeds = pipeline_1.encode_prompt(prompt)\nnum_images_per_prompt = 2\nnum_inference_steps = 100\nconfigs = {\"guidance_scale\": 7.0}\n\ncallback = IFCallback(\n    pipeline=pipeline_1,\n    prompt=prompt,\n    wandb_project=\"diffusers-2\",\n    wandb_entity=\"geekyrakshit\",\n    weave_mode=True,\n    num_inference_steps=num_inference_steps,\n    num_images_per_prompt=num_images_per_prompt,\n    configs=configs\n)\n\nimage = pipeline_1(\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=negative_embeds,\n    output_type=\"pt\",\n    callback=partial(callback, end_experiment=False),\n    **configs,\n).images\n</pre> prompt = 'a photo of a smiling bee wearing a yellow hoodie and blue sunglasses standing in front of the eiffel tower holding a sign that says \"Weights and Biases\"' prompt_embeds, negative_embeds = pipeline_1.encode_prompt(prompt) num_images_per_prompt = 2 num_inference_steps = 100 configs = {\"guidance_scale\": 7.0}  callback = IFCallback(     pipeline=pipeline_1,     prompt=prompt,     wandb_project=\"diffusers-2\",     wandb_entity=\"geekyrakshit\",     weave_mode=True,     num_inference_steps=num_inference_steps,     num_images_per_prompt=num_images_per_prompt,     configs=configs )  image = pipeline_1(     prompt_embeds=prompt_embeds,     negative_prompt_embeds=negative_embeds,     output_type=\"pt\",     callback=partial(callback, end_experiment=False),     **configs, ).images In\u00a0[\u00a0]: Copied! <pre>pipeline_2 = IFSuperResolutionPipeline.from_pretrained(\n    \"DeepFloyd/IF-II-L-v1.0\", text_encoder=None, variant=\"fp16\", torch_dtype=torch.float16\n)\npipeline_2.enable_model_cpu_offload()\n</pre> pipeline_2 = IFSuperResolutionPipeline.from_pretrained(     \"DeepFloyd/IF-II-L-v1.0\", text_encoder=None, variant=\"fp16\", torch_dtype=torch.float16 ) pipeline_2.enable_model_cpu_offload() In\u00a0[\u00a0]: Copied! <pre>num_inference_steps = 50\n\ncallback.add_stage(pipeline_2, num_inference_steps=num_inference_steps)\n\nimage = pipeline_2(\n    image=image,\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=negative_embeds,\n    num_inference_steps=num_inference_steps,\n    output_type=\"pt\",\n    callback=partial(callback, end_experiment=False),\n).images\n</pre> num_inference_steps = 50  callback.add_stage(pipeline_2, num_inference_steps=num_inference_steps)  image = pipeline_2(     image=image,     prompt_embeds=prompt_embeds,     negative_prompt_embeds=negative_embeds,     num_inference_steps=num_inference_steps,     output_type=\"pt\",     callback=partial(callback, end_experiment=False), ).images In\u00a0[\u00a0]: Copied! <pre>del pipeline_2\ngc.collect()\ntorch.cuda.empty_cache()\n</pre> del pipeline_2 gc.collect() torch.cuda.empty_cache() In\u00a0[\u00a0]: Copied! <pre>safety_modules = {\n    \"feature_extractor\": pipeline_1.feature_extractor,\n    \"safety_checker\": pipeline_1.safety_checker,\n    \"watermarker\": pipeline_1.watermarker,\n}\npipeline_3 = StableDiffusionUpscalePipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-x4-upscaler\", **safety_modules, torch_dtype=torch.float16\n)\npipeline_3.enable_model_cpu_offload()\n</pre> safety_modules = {     \"feature_extractor\": pipeline_1.feature_extractor,     \"safety_checker\": pipeline_1.safety_checker,     \"watermarker\": pipeline_1.watermarker, } pipeline_3 = StableDiffusionUpscalePipeline.from_pretrained(     \"stabilityai/stable-diffusion-x4-upscaler\", **safety_modules, torch_dtype=torch.float16 ) pipeline_3.enable_model_cpu_offload() In\u00a0[\u00a0]: Copied! <pre>num_inference_steps = 75\n\ncallback.add_stage(pipeline_3, num_inference_steps=num_inference_steps, stage_name=\"Upscale\")\n\nimage = pipeline_3(\n    prompt=prompt,\n    image=image,\n    noise_level=100,\n    num_inference_steps=num_inference_steps,\n    callback=callback,\n).images\n</pre> num_inference_steps = 75  callback.add_stage(pipeline_3, num_inference_steps=num_inference_steps, stage_name=\"Upscale\")  image = pipeline_3(     prompt=prompt,     image=image,     noise_level=100,     num_inference_steps=num_inference_steps,     callback=callback, ).images"},{"location":"diffusers/examples/kandinsky/","title":"Kandinsky","text":"In\u00a0[\u00a0]: Copied! <pre># !git clone https://github.com/soumik12345/wandb-addons\n# !pip install ./wandb-addons[huggingface]\n</pre> # !git clone https://github.com/soumik12345/wandb-addons # !pip install ./wandb-addons[huggingface] In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom diffusers import KandinskyCombinedPipeline\n\nfrom wandb_addons.diffusers import KandinskyCallback\n</pre> import torch from diffusers import KandinskyCombinedPipeline  from wandb_addons.diffusers import KandinskyCallback In\u00a0[\u00a0]: Copied! <pre>pipe = KandinskyCombinedPipeline.from_pretrained(\n    \"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16\n)\npipe = pipe.to(\"cuda\")\n</pre> pipe = KandinskyCombinedPipeline.from_pretrained(     \"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16 ) pipe = pipe.to(\"cuda\") In\u00a0[\u00a0]: Copied! <pre>prompt = [\"a photograph of an astronaut riding a horse\", \"a photograph of a dragon\"]\nnegative_prompt = [\"ugly, deformed\", \"ugly, deformed\"]\nnum_images_per_prompt = 2\n\nconfigs = {\n    \"guidance_scale\": 4.0,\n    \"height\": 512,\n    \"width\": 512,\n    \"prior_guidance_scale\": 4.0,\n    \"prior_num_inference_steps\": 25,\n}\n\n# Create the WandB callback for StableDiffusionPipeline\ncallback = KandinskyCallback(\n    pipe,\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    wandb_project=\"diffusers\",\n    wandb_entity=\"geekyrakshit\",\n    weave_mode=False,\n    num_images_per_prompt=num_images_per_prompt,\n    configs=configs,\n)\n\n# Add the callback to the pipeline\nimage = pipe(\n    prompt,\n    negative_prompt=negative_prompt,\n    callback=callback,\n    num_images_per_prompt=num_images_per_prompt,\n    **configs,\n)\n</pre> prompt = [\"a photograph of an astronaut riding a horse\", \"a photograph of a dragon\"] negative_prompt = [\"ugly, deformed\", \"ugly, deformed\"] num_images_per_prompt = 2  configs = {     \"guidance_scale\": 4.0,     \"height\": 512,     \"width\": 512,     \"prior_guidance_scale\": 4.0,     \"prior_num_inference_steps\": 25, }  # Create the WandB callback for StableDiffusionPipeline callback = KandinskyCallback(     pipe,     prompt=prompt,     negative_prompt=negative_prompt,     wandb_project=\"diffusers\",     wandb_entity=\"geekyrakshit\",     weave_mode=False,     num_images_per_prompt=num_images_per_prompt,     configs=configs, )  # Add the callback to the pipeline image = pipe(     prompt,     negative_prompt=negative_prompt,     callback=callback,     num_images_per_prompt=num_images_per_prompt,     **configs, )"},{"location":"diffusers/examples/sdxl/","title":"Sdxl","text":"In\u00a0[\u00a0]: Copied! <pre>!git clone https://github.com/soumik12345/wandb-addons\n!pip install ./wandb-addons[huggingface] &gt; install.log\n</pre> !git clone https://github.com/soumik12345/wandb-addons !pip install ./wandb-addons[huggingface] &gt; install.log In\u00a0[\u00a0]: Copied! <pre>from functools import partial\n\nimport torch\ntorch.cuda.empty_cache()\n\nfrom diffusers import StableDiffusionXLPipeline, StableDiffusionXLImg2ImgPipeline\nfrom wandb_addons.diffusers import StableDiffusionXLCallback\n</pre> from functools import partial  import torch torch.cuda.empty_cache()  from diffusers import StableDiffusionXLPipeline, StableDiffusionXLImg2ImgPipeline from wandb_addons.diffusers import StableDiffusionXLCallback In\u00a0[\u00a0]: Copied! <pre>base_pipeline = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n    use_safetensors=True,\n)\n\nbase_pipeline.enable_model_cpu_offload()\n</pre> base_pipeline = StableDiffusionXLPipeline.from_pretrained(     \"stabilityai/stable-diffusion-xl-base-1.0\",     torch_dtype=torch.float16,     variant=\"fp16\",     use_safetensors=True, )  base_pipeline.enable_model_cpu_offload() In\u00a0[\u00a0]: Copied! <pre>prompt = \"seascape by Ray Collins and artgerm, front view of a perfect wave, sunny background, ultra detailed water, 4k resolution\"\nnegative_prompt = \"low resolution, low details, blurry, clouds\"\nnum_inference_steps = 50\n\ncallback = StableDiffusionXLCallback(\n    pipeline=base_pipeline,\n    prompt=prompt,\n    wandb_project=\"diffusers-new\",\n    wandb_entity=\"geekyrakshit\",\n    weave_mode=True,\n    num_inference_steps=num_inference_steps,\n    negative_prompt=negative_prompt,\n)\n\nimage = base_pipeline(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    output_type=\"latent\",\n    num_inference_steps=num_inference_steps,\n    callback=partial(callback, end_experiment=False)\n).images[0]\n</pre> prompt = \"seascape by Ray Collins and artgerm, front view of a perfect wave, sunny background, ultra detailed water, 4k resolution\" negative_prompt = \"low resolution, low details, blurry, clouds\" num_inference_steps = 50  callback = StableDiffusionXLCallback(     pipeline=base_pipeline,     prompt=prompt,     wandb_project=\"diffusers-new\",     wandb_entity=\"geekyrakshit\",     weave_mode=True,     num_inference_steps=num_inference_steps,     negative_prompt=negative_prompt, )  image = base_pipeline(     prompt=prompt,     negative_prompt=negative_prompt,     output_type=\"latent\",     num_inference_steps=num_inference_steps,     callback=partial(callback, end_experiment=False) ).images[0] In\u00a0[\u00a0]: Copied! <pre>refiner_pipeline = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n    text_encoder_2=base_pipeline.text_encoder_2,\n    vae=base_pipeline.vae,\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n    variant=\"fp16\",\n)\nrefiner_pipeline.enable_model_cpu_offload()\n</pre> refiner_pipeline = StableDiffusionXLImg2ImgPipeline.from_pretrained(     \"stabilityai/stable-diffusion-xl-refiner-1.0\",     text_encoder_2=base_pipeline.text_encoder_2,     vae=base_pipeline.vae,     torch_dtype=torch.float16,     use_safetensors=True,     variant=\"fp16\", ) refiner_pipeline.enable_model_cpu_offload() In\u00a0[\u00a0]: Copied! <pre>num_inference_steps = 50\nstrength = 0.3\n\ncallback.add_refiner_stage(\n    refiner_pipeline, num_inference_steps=num_inference_steps, strength=strength\n)\n\nimage = refiner_pipeline(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    image=image[None, :],\n    callback=callback\n).images[0]\n</pre> num_inference_steps = 50 strength = 0.3  callback.add_refiner_stage(     refiner_pipeline, num_inference_steps=num_inference_steps, strength=strength )  image = refiner_pipeline(     prompt=prompt,     negative_prompt=negative_prompt,     image=image[None, :],     callback=callback ).images[0]"},{"location":"diffusers/examples/stable_diffusion/","title":"Stable diffusion","text":"In\u00a0[\u00a0]: Copied! <pre>!git clone https://github.com/soumik12345/wandb-addons\n!pip install ./wandb-addons[huggingface]\n</pre> !git clone https://github.com/soumik12345/wandb-addons !pip install ./wandb-addons[huggingface] In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom diffusers import StableDiffusionPipeline\n\nfrom wandb_addons.diffusers import get_wandb_callback\n</pre> import torch from diffusers import StableDiffusionPipeline  from wandb_addons.diffusers import get_wandb_callback In\u00a0[\u00a0]: Copied! <pre>pipe = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16\n)\npipe = pipe.to(\"cuda\")\n</pre> pipe = StableDiffusionPipeline.from_pretrained(     \"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16 ) pipe = pipe.to(\"cuda\") In\u00a0[\u00a0]: Copied! <pre>prompt = [\"a photograph of an astronaut riding a horse\", \"a photograph of a dragon\"]\nnegative_prompt = [\"ugly, deformed\", \"ugly, deformed\"]\nnum_images_per_prompt = 2\nnum_inference_steps = 50\nconfigs = {\n    \"eta\": 0.0,\n    \"guidance_rescale\": 0.0,\n}\n\n# Create the WandB callback for StableDiffusionPipeline\ncallback = get_wandb_callback(\n    pipe,\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    wandb_project=\"diffusers-new\",\n    wandb_entity=\"geekyrakshit\",\n    weave_mode=True,\n    num_inference_steps=num_inference_steps,\n    num_images_per_prompt=num_images_per_prompt,\n    configs=configs,\n)\n\n# Add the callback to the pipeline\nimage = pipe(\n    prompt,\n    negative_prompt=negative_prompt,\n    callback=callback,\n    num_inference_steps=num_inference_steps,\n    num_images_per_prompt=num_images_per_prompt,\n    **configs,\n)\n</pre> prompt = [\"a photograph of an astronaut riding a horse\", \"a photograph of a dragon\"] negative_prompt = [\"ugly, deformed\", \"ugly, deformed\"] num_images_per_prompt = 2 num_inference_steps = 50 configs = {     \"eta\": 0.0,     \"guidance_rescale\": 0.0, }  # Create the WandB callback for StableDiffusionPipeline callback = get_wandb_callback(     pipe,     prompt=prompt,     negative_prompt=negative_prompt,     wandb_project=\"diffusers-new\",     wandb_entity=\"geekyrakshit\",     weave_mode=True,     num_inference_steps=num_inference_steps,     num_images_per_prompt=num_images_per_prompt,     configs=configs, )  # Add the callback to the pipeline image = pipe(     prompt,     negative_prompt=negative_prompt,     callback=callback,     num_inference_steps=num_inference_steps,     num_images_per_prompt=num_images_per_prompt,     **configs, ) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"diffusers/examples/stable_diffusion_img2img/","title":"Stable diffusion img2img","text":"In\u00a0[\u00a0]: Copied! <pre>!git clone https://github.com/soumik12345/wandb-addons\n!pip install ./wandb-addons[huggingface]\n</pre> !git clone https://github.com/soumik12345/wandb-addons !pip install ./wandb-addons[huggingface] In\u00a0[\u00a0]: Copied! <pre>import requests\nimport torch\nfrom PIL import Image\nfrom io import BytesIO\n\nfrom diffusers import StableDiffusionImg2ImgPipeline\n\nfrom wandb_addons.diffusers import get_wandb_callback\n</pre> import requests import torch from PIL import Image from io import BytesIO  from diffusers import StableDiffusionImg2ImgPipeline  from wandb_addons.diffusers import get_wandb_callback In\u00a0[\u00a0]: Copied! <pre>pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16\n)\npipe = pipe.to(\"cuda\")\n</pre> pipe = StableDiffusionImg2ImgPipeline.from_pretrained(     \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16 ) pipe = pipe.to(\"cuda\") In\u00a0[\u00a0]: Copied! <pre>url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n\nresponse = requests.get(url)\ninit_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\ninit_image = init_image.resize((768, 512))\n\nprompt = \"A fantasy landscape, trending on artstation\"\n</pre> url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"  response = requests.get(url) init_image = Image.open(BytesIO(response.content)).convert(\"RGB\") init_image = init_image.resize((768, 512))  prompt = \"A fantasy landscape, trending on artstation\" In\u00a0[\u00a0]: Copied! <pre>num_images_per_prompt = 1\nnum_inference_steps = 50\nstrength = 0.75\n\ncallback = get_wandb_callback(\n    pipeline=pipe, prompt=prompt,\n    input_images=init_image,\n    wandb_project=\"diffusers-2\",\n    wandb_entity=\"geekyrakshit\",\n    weave_mode=True,\n    num_images_per_prompt=num_images_per_prompt,\n    num_inference_steps=num_inference_steps,\n    strength=strength,\n)\n\nresults = pipe(\n    prompt=prompt,\n    image=init_image,\n    strength=strength,\n    num_images_per_prompt=num_images_per_prompt,\n    num_inference_steps=num_inference_steps,\n    callback=callback\n)\n</pre> num_images_per_prompt = 1 num_inference_steps = 50 strength = 0.75  callback = get_wandb_callback(     pipeline=pipe, prompt=prompt,     input_images=init_image,     wandb_project=\"diffusers-2\",     wandb_entity=\"geekyrakshit\",     weave_mode=True,     num_images_per_prompt=num_images_per_prompt,     num_inference_steps=num_inference_steps,     strength=strength, )  results = pipe(     prompt=prompt,     image=init_image,     strength=strength,     num_images_per_prompt=num_images_per_prompt,     num_inference_steps=num_inference_steps,     callback=callback )"},{"location":"keras/keras/","title":"Keras Core Callbacks","text":"<p>Backend-agnostic callbacks integrating Weights &amp; Biases with Keras-Core.</p> Examples Link WandB Run Image Classification using KerasCore"},{"location":"keras/keras/#wandb_addons.keras.metrics_logger.WandbMetricsLogger","title":"<code>WandbMetricsLogger</code>","text":"<p>             Bases: <code>Callback</code></p> <p>Logger that sends system metrics to W&amp;B.</p> <p><code>WandbMetricsLogger</code> automatically logs the <code>logs</code> dictionary that callback methods take as argument to wandb.</p> <p>Warning</p> <p>This callback will be deprecated soon in favor of <code>wandb.keras.WandbMetricsLogger</code> which will be updated to support backend-agnostic Keras3.</p> <p>This callback automatically logs the following to a W&amp;B run page</p> <ul> <li>system (CPU/GPU/TPU) metrics</li> <li>train and validation metrics defined in <code>model.compile</code>,</li> <li>learning rate (both for a fixed value or a learning rate scheduler)</li> </ul> <p>Notes</p> <p>If you resume training by passing <code>initial_epoch</code> to <code>model.fit</code> and you are using a learning rate scheduler, make sure to pass <code>initial_global_step</code> to <code>WandbMetricsLogger</code>. The <code>initial_global_step</code> is <code>step_size * initial_step</code>, where <code>step_size</code> is number of training steps per epoch. <code>step_size</code> can be calculated as the product of the cardinality of the training dataset and the batch size.</p> <p>Example notebooks:</p> <ul> <li>Image Classification using Keras Core.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>log_freq</code> <code>Union[LogStrategy, int]</code> <p>(\"epoch\", \"batch\", or int) if \"epoch\", logs metrics at the end of each epoch. If \"batch\", logs metrics at the end of each batch. If an integer, logs metrics at the end of that many batches. Defaults to \"epoch\".</p> <code>'epoch'</code> <code>initial_global_step</code> <code>int</code> <p>Use this argument to correcly log the learning rate when you resume training from some <code>initial_epoch</code>, and a learning rate scheduler is used. This can be computed as <code>step_size * initial_step</code>. Defaults to 0.</p> <code>0</code> Source code in <code>wandb_addons/keras/metrics_logger.py</code> <pre><code>class WandbMetricsLogger(Callback):\n    \"\"\"Logger that sends system metrics to W&amp;B.\n\n    `WandbMetricsLogger` automatically logs the `logs` dictionary that callback methods\n    take as argument to wandb.\n\n    !!! warning\n        This callback will be deprecated soon in favor of\n        [`wandb.keras.WandbMetricsLogger`](https://docs.wandb.ai/guides/integrations/keras#experiment-tracking-with-wandbmetricslogger)\n        which will be updated to support backend-agnostic Keras3.\n\n    !!! note \"This callback automatically logs the following to a W&amp;B run page\"\n        - system (CPU/GPU/TPU) metrics\n        - train and validation metrics defined in `model.compile`,\n        - learning rate (both for a fixed value or a learning rate scheduler)\n\n    !!! note \"Notes\"\n        If you resume training by passing `initial_epoch` to `model.fit` and you are\n        using a learning rate scheduler, make sure to pass `initial_global_step` to\n        `WandbMetricsLogger`. The `initial_global_step` is `step_size * initial_step`,\n        where `step_size` is number of training steps per epoch. `step_size` can be\n        calculated as the product of the cardinality of the training dataset and the\n        batch size.\n\n    !!! example \"Example notebooks:\"\n        - [Image Classification using Keras Core](../examples/image_classification).\n\n    Arguments:\n        log_freq (Union[LogStrategy, int]): (\"epoch\", \"batch\", or int) if \"epoch\",\n            logs metrics at the end of each epoch. If \"batch\", logs metrics at the end\n            of each batch. If an integer, logs metrics at the end of that\n            many batches. Defaults to \"epoch\".\n        initial_global_step (int): Use this argument to correcly log the\n            learning rate when you resume training from some `initial_epoch`,\n            and a learning rate scheduler is used. This can be computed as\n            `step_size * initial_step`. Defaults to 0.\n    \"\"\"\n\n    def __init__(\n        self,\n        log_freq: Union[LogStrategy, int] = \"epoch\",\n        initial_global_step: int = 0,\n        *args: Any,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(*args, **kwargs)\n\n        if wandb.run is None:\n            raise wandb.Error(\n                \"You must call `wandb.init()` before WandbMetricsLogger()\"\n            )\n\n        with telemetry.context(run=wandb.run) as tel:\n            tel.feature.keras_metrics_logger = True\n\n        if log_freq == \"batch\":\n            log_freq = 1\n\n        self.logging_batch_wise = isinstance(log_freq, int)\n        self.log_freq: Any = log_freq if self.logging_batch_wise else None\n        self.global_batch = 0\n        self.global_step = initial_global_step\n\n        if self.logging_batch_wise:\n            # define custom x-axis for batch logging.\n            wandb.define_metric(\"batch/batch_step\")\n            # set all batch metrics to be logged against batch_step.\n            wandb.define_metric(\"batch/*\", step_metric=\"batch/batch_step\")\n        else:\n            # define custom x-axis for epoch-wise logging.\n            wandb.define_metric(\"epoch/epoch\")\n            # set all epoch-wise metrics to be logged against epoch.\n            wandb.define_metric(\"epoch/*\", step_metric=\"epoch/epoch\")\n\n    def _get_lr(self) -&gt; Union[float, None]:\n        try:\n            if isinstance(self.model.optimizer, keras.optimizers.Optimizer):\n                return float(self.model.optimizer.learning_rate.numpy().item())\n        except Exception:\n            if tf_backend_available:\n                if isinstance(self.model.optimizer.learning_rate, tf.Tensor):\n                    return float(self.model.optimizer.learning_rate.numpy().item())\n                else:\n                    wandb.termerror(\"Unable to log learning rate.\", repeat=False)\n                    return None\n            elif torch_backend_available:\n                if isinstance(self.model.optimizer.learning_rate, torch.Tensor):\n                    lr = self.model.optimizer.learning_rate.to(\"cpu\")\n                    return float(lr.numpy().item())\n                else:\n                    wandb.termerror(\"Unable to log learning rate.\", repeat=False)\n                    return None\n            elif jax_backend_available:\n                try:\n                    return float(np.array(self.model.optimizer.learning_rate).item())\n                except Exception as e:\n                    wandb.termerror(\"Unable to log learning rate.\", repeat=False)\n                    return None\n\n    def on_epoch_end(self, epoch: int, logs: Optional[Dict[str, Any]] = None) -&gt; None:\n        logs = dict() if logs is None else {f\"epoch/{k}\": v for k, v in logs.items()}\n\n        logs[\"epoch/epoch\"] = epoch\n\n        lr = self._get_lr()\n        if lr is not None:\n            logs[\"epoch/learning_rate\"] = lr\n\n        wandb.log(logs)\n\n    def on_batch_end(self, batch: int, logs: Optional[Dict[str, Any]] = None) -&gt; None:\n        self.global_step += 1\n        if self.logging_batch_wise and batch % self.log_freq == 0:\n            logs = {f\"batch/{k}\": v for k, v in logs.items()} if logs else {}\n            logs[\"batch/batch_step\"] = self.global_batch\n\n            lr = self._get_lr()\n            if lr is not None:\n                logs[\"batch/learning_rate\"] = lr\n\n            wandb.log(logs)\n\n            self.global_batch += self.log_freq\n\n    def on_train_batch_end(\n        self, batch: int, logs: Optional[Dict[str, Any]] = None\n    ) -&gt; None:\n        self.on_batch_end(batch, logs if logs else {})\n</code></pre>"},{"location":"keras/keras/#wandb_addons.keras.model_checkpoint.WandbModelCheckpoint","title":"<code>WandbModelCheckpoint</code>","text":"<p>             Bases: <code>ModelCheckpoint</code></p> <p><code>WandbModelCheckpoint</code> automatically logs model checkpoints to W&amp;B and versions them as W&amp;B artifacts.</p> <p>Warning</p> <p>This callback will be deprecated soon in favor of <code>wandb.keras.WandbModelCheckpoint</code> which will be updated to support backend-agnostic Keras3.</p> <p>Since this callback is subclassed from <code>keras.callbacks.ModelCheckpoint</code>, the checkpointing logic is taken care of by the parent callback.</p> <p>This callback is to be used in conjunction with training using <code>model.fit()</code> to save a model or weights (in a checkpoint file) at some interval. The model checkpoints will be logged as W&amp;B Artifacts. You can learn more here: https://docs.wandb.ai/guides/artifacts</p> This callback provides the following features <ul> <li>Save the model that has achieved \"best performance\" based on \"monitor\".</li> <li>Save the model at the end of every epoch regardless of the performance.</li> <li>Save the model at the end of epoch or after a fixed number of training     batches.</li> <li>Save only model weights, or save the whole model.</li> <li>Save the model either in SavedModel format or in <code>.h5</code> format.</li> </ul> <p>Example notebooks:</p> <ul> <li>Image Classification using Keras Core.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>StrPath</code> <p>path to save the model file. <code>filepath</code> can contain named formatting options, which will be filled by the value of <code>epoch</code> and keys in <code>logs</code> (passed in <code>on_epoch_end</code>). For example: if <code>filepath</code> is <code>model-{epoch:02d}-{val_loss:.2f}</code>, then the model checkpoints will be saved with the epoch number and the validation loss in the filename.</p> required <code>monitor</code> <code>str</code> <p>The metric name to monitor. Default to \"val_loss\".</p> <code>'val_loss'</code> <code>verbose</code> <code>int</code> <p>Verbosity mode, 0 or 1. Mode 0 is silent, and mode 1 displays messages when the callback takes an action.</p> <code>0</code> <code>save_best_only</code> <code>bool</code> <p>if <code>save_best_only=True</code>, it only saves when the model is considered the \"best\" and the latest best model according to the quantity monitored will not be overwritten. If <code>filepath</code> doesn't contain formatting options like <code>{epoch}</code> then <code>filepath</code> will be overwritten by each new better model locally. The model logged as an artifact will still be associated with the correct <code>monitor</code>.  Artifacts will be uploaded continuously and versioned separately as a new best model is found.</p> <code>False</code> <code>save_weights_only</code> <code>bool</code> <p>if True, then only the model's weights will be saved.</p> <code>False</code> <code>mode</code> <code>Literal['auto', 'min', 'max']</code> <p>one of {'auto', 'min', 'max'}. For <code>val_acc</code>, this should be <code>max</code>, for <code>val_loss</code> this should be <code>min</code>, etc.</p> <code>'auto'</code> <code>save_freq</code> <code>Union[SaveStrategy, int]</code> <p><code>epoch</code> or integer. When using <code>'epoch'</code>, the callback saves the model after each epoch. When using an integer, the callback saves the model at end of this many batches. Note that when monitoring validation metrics such as <code>val_acc</code> or <code>val_loss</code>, save_freq must be set to \"epoch\" as those metrics are only available at the end of an epoch.</p> <code>'epoch'</code> <code>initial_value_threshold</code> <code>Optional[float]</code> <p>Floating point initial \"best\" value of the metric to be monitored.</p> <code>None</code> Source code in <code>wandb_addons/keras/model_checkpoint.py</code> <pre><code>class WandbModelCheckpoint(ModelCheckpoint):\n    \"\"\"`WandbModelCheckpoint` automatically logs model checkpoints to W&amp;B and versions\n    them as [W&amp;B artifacts](https://docs.wandb.ai/guides/artifacts).\n\n    !!! warning\n        This callback will be deprecated soon in favor of\n        [`wandb.keras.WandbModelCheckpoint`](https://docs.wandb.ai/guides/integrations/keras#model-checkpointing-using-wandbmodelcheckpoint)\n        which will be updated to support backend-agnostic Keras3.\n\n    Since this callback is subclassed from\n    [`keras.callbacks.ModelCheckpoint`](https://keras.io/keras_core/api/callbacks/model_checkpoint/),\n    the checkpointing logic is taken care of by the parent callback.\n\n    This callback is to be used in conjunction with training using `model.fit()` to save\n    a model or weights (in a checkpoint file) at some interval. The model checkpoints\n    will be logged as W&amp;B Artifacts. You can learn more here:\n    https://docs.wandb.ai/guides/artifacts\n\n    This callback provides the following features:\n        - Save the model that has achieved \"best performance\" based on \"monitor\".\n        - Save the model at the end of every epoch regardless of the performance.\n        - Save the model at the end of epoch or after a fixed number of training\n            batches.\n        - Save only model weights, or save the whole model.\n        - Save the model either in SavedModel format or in `.h5` format.\n\n    !!! example \"Example notebooks:\"\n        - [Image Classification using Keras Core](../examples/image_classification).\n\n    Arguments:\n        filepath (wandb.sdk.lib.paths.StrPath): path to save the model file. `filepath`\n            can contain named formatting options, which will be filled by the value\n            of `epoch` and keys in `logs` (passed in `on_epoch_end`). For example:\n            if `filepath` is `model-{epoch:02d}-{val_loss:.2f}`, then the\n            model checkpoints will be saved with the epoch number and the\n            validation loss in the filename.\n        monitor (str): The metric name to monitor. Default to \"val_loss\".\n        verbose (int): Verbosity mode, 0 or 1. Mode 0 is silent, and mode 1\n            displays messages when the callback takes an action.\n        save_best_only (bool): if `save_best_only=True`, it only saves when the model\n            is considered the \"best\" and the latest best model according to the\n            quantity monitored will not be overwritten. If `filepath` doesn't contain\n            formatting options like `{epoch}` then `filepath` will be overwritten by\n            each new better model locally. The model logged as an artifact will still\n            be associated with the correct `monitor`.  Artifacts will be uploaded\n            continuously and versioned separately as a new best model is found.\n        save_weights_only (bool): if True, then only the model's weights will be saved.\n        mode (Literal[\"auto\", \"min\", \"max\"]): one of {'auto', 'min', 'max'}. For\n            `val_acc`, this should be `max`, for `val_loss` this should be `min`, etc.\n        save_freq (Union[SaveStrategy, int]): `epoch` or integer. When using `'epoch'`,\n            the callback saves the model after each epoch. When using an integer, the\n            callback saves the model at end of this many batches.\n            Note that when monitoring validation metrics such as `val_acc` or\n            `val_loss`, save_freq must be set to \"epoch\" as those metrics are only\n            available at the end of an epoch.\n        initial_value_threshold (Optional[float]): Floating point initial \"best\" value\n            of the metric to be monitored.\n    \"\"\"\n\n    def __init__(\n        self,\n        filepath: StrPath,\n        monitor: str = \"val_loss\",\n        verbose: int = 0,\n        save_best_only: bool = False,\n        save_weights_only: bool = False,\n        mode: Mode = \"auto\",\n        save_freq: Union[SaveStrategy, int] = \"epoch\",\n        initial_value_threshold: Optional[float] = None,\n    ):\n        if wandb.run is None:\n            raise wandb.Error(\n                \"You must call `wandb.init()` before `WandbModelCheckpoint()`\"\n            )\n        with telemetry.context(run=wandb.run) as tel:\n            tel.feature.keras_model_checkpoint = True\n\n        super().__init__(\n            filepath,\n            monitor,\n            verbose,\n            save_best_only,\n            save_weights_only,\n            mode,\n            save_freq,\n            initial_value_threshold,\n        )\n\n    def on_train_batch_end(self, batch, logs=None):\n        super().on_train_batch_end(batch, logs)\n        if self._should_save_on_batch(batch):\n            _log_artifact(self.filepath, aliases=[f\"batch_{batch}\"])\n\n    def on_epoch_end(self, epoch, logs=None):\n        super().on_epoch_end(epoch, logs)\n        if self.save_freq == \"epoch\":\n            _log_artifact(self.filepath, aliases=[f\"epoch_{epoch}\"])\n</code></pre>"},{"location":"keras/keras/#wandb_addons.keras.image_classification.WandBImageClassificationCallback","title":"<code>WandBImageClassificationCallback</code>","text":"<p>             Bases: <code>Callback</code></p> <p>Callback that logs the images and results of an image-classification task including ground-truth and predicted labels and the class-wise probabilities in a wandb.Table in an epoch-wise manner.</p> <p>Example notebooks:</p> <ul> <li>Image Classification using Keras Core.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[Dataset, Tuple[array, array]]</code> <p>The dataset that is to be visualized. This is ideally the validation dataset for the image classification task in the form of a <code>tf.data.Dataset</code> that has an <code>element_spec</code> like (image_tensor, label_tensor) or a tuple of numpy arrays in the form of (image_tensor, label_tensor).</p> required <code>class_labels</code> <code>Optional[List[str]]</code> <p>The list of class names such that the index of the class names list corrspond to the labels in the dataset.</p> required <code>unbatch_dataset</code> <code>bool</code> <p>This should be set to <code>True</code> if your dataset is batched and needs to be unbatched.</p> <code>True</code> <code>labels_from_logits</code> <code>bool</code> <p>Whether the labels in the dataset are one-hot-encoded or from logits.</p> <code>False</code> <code>max_items_for_visualization</code> <code>Optional[int]</code> <p>Maximum number of items from the dataset to be visualized every epoch.</p> <code>None</code> Source code in <code>wandb_addons/keras/image_classification.py</code> <pre><code>class WandBImageClassificationCallback(Callback):\n    \"\"\"Callback that logs the images and results of an image-classification task\n    including ground-truth and predicted labels and the class-wise probabilities\n    in a [wandb.Table](https://docs.wandb.ai/guides/data-vis) in an epoch-wise\n    manner.\n\n    !!! example \"Example notebooks:\"\n        - [Image Classification using Keras Core](../examples/image_classification).\n\n    Arguments:\n        dataset (Union[tf_data.Dataset, Tuple[np.array, np.array]]): The dataset that\n            is to be visualized. This is ideally the validation dataset for the image\n            classification task in the form of a\n            [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)\n            that has an `element_spec` like (image_tensor, label_tensor) or a tuple of\n            numpy arrays in the form of (image_tensor, label_tensor).\n        class_labels (Optional[List[str]]): The list of class names such that the index\n            of the class names list corrspond to the labels in the dataset.\n        unbatch_dataset (bool): This should be set to `True` if your dataset is batched\n            and needs to be unbatched.\n        labels_from_logits (bool): Whether the labels in the dataset are\n            one-hot-encoded or from logits.\n        max_items_for_visualization (Optional[int]): Maximum number of items from the\n            dataset to be visualized every epoch.\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset: Union[tf_data.Dataset, Tuple[np.array, np.array]],\n        class_labels: Optional[List[str]],\n        unbatch_dataset: bool = True,\n        labels_from_logits: bool = False,\n        max_items_for_visualization: Optional[int] = None,\n        title: Optional[str] = None,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n        self.dataset = dataset\n        self.class_labels = class_labels\n        self.unbatch_dataset = unbatch_dataset\n        self.labels_from_logits = labels_from_logits\n        self.max_items_for_visualization = max_items_for_visualization\n        self.title = title if title is not None else \"Evaluation-Table\"\n\n        if self.unbatch_dataset:\n            self.dataset = self.dataset.unbatch()\n\n        if self.max_items_for_visualization:\n            if isinstance(self.dataset, tf_data.Dataset):\n                dataset_size = (\n                    tf_data.experimental.cardinality(self.dataset).numpy().item()\n                )\n                if self.max_items_for_visualization &lt; dataset_size:\n                    self.dataset = self.dataset.take(self.max_items_for_visualization)\n            else:\n                assert self.dataset[0].shape[0] == self.dataset[0].shape[1]\n                dataset_size = self.dataset[0].shape[0]\n                if self.max_items_for_visualization &lt; dataset_size:\n                    self.dataset = (\n                        self.dataset[0][: self.max_items_for_visualization],\n                        self.dataset[1][: self.max_items_for_visualization],\n                    )\n        else:\n            if isinstance(self.dataset, tf_data.Dataset):\n                self.max_items_for_visualization = (\n                    tf_data.experimental.cardinality(self.dataset).numpy().item()\n                )\n            else:\n                assert self.dataset[0].shape[0] == self.dataset[0].shape[1]\n                self.max_items_for_visualization = self.dataset[0].shape[0]\n\n        wandb.termlog(\n            f\"Logging {self.max_items_for_visualization} items per epoch to the table.\"\n        )\n\n        self.table = wandb.Table(\n            columns=[\n                \"Epoch\",\n                \"Image\",\n                \"Ground-Truth-Label\",\n                \"Predicted-Label\",\n                \"Top-5-Classes\",\n                \"Top-5-Probabilities\",\n                \"Predicted-Probability\",\n            ]\n        )\n\n    def get_predicted_probabilities(self, predictions: np.array):\n        predictions = ops.convert_to_numpy(ops.squeeze(predictions)).tolist()\n        predicted_probabilities = {\n            self.class_labels[idx]: predictions[idx] for idx in range(len(predictions))\n        }\n        sorted_probabilities = sorted(\n            predicted_probabilities.items(), key=lambda item: item[1], reverse=True\n        )\n        top_5_classes = [item[0] for item in sorted_probabilities]\n        top_5_probabilities = [item[1] for item in sorted_probabilities]\n        return predicted_probabilities, top_5_classes, top_5_probabilities\n\n    def on_epoch_end(self, epoch, logs=None):\n        data_iterator = (\n            iter(self.dataset)\n            if isinstance(self.dataset, tf_data.Dataset)\n            else zip(self.dataset)\n        )\n        data_iterator = tqdm(\n            data_iterator,\n            total=self.max_items_for_visualization,\n            desc=\"Populating W&amp;B Table\",\n        )\n        for image, label in data_iterator:\n            if backend.backend() == \"jax\":\n                predictions = self.model(\n                    ops.expand_dims(ops.convert_to_numpy(image), axis=0)\n                )\n            else:\n                predictions = self.model(ops.expand_dims(image, axis=0))\n            predicted_label = self.class_labels[\n                int(ops.convert_to_numpy(ops.argmax(predictions, axis=-1)).item())\n            ]\n            (\n                predicted_probabilities,\n                top_5_classes,\n                top_5_probabilities,\n            ) = self.get_predicted_probabilities(predictions)\n            image = ops.convert_to_numpy(image)\n\n            if self.labels_from_logits:\n                label = self.class_labels[int(ops.convert_to_numpy(label).item())]\n            else:\n                if backend.backend() == \"jax\":\n                    label = self.class_labels[\n                        int(ops.argmax(ops.convert_to_numpy(label), axis=-1).item())\n                    ]\n                else:\n                    label = self.class_labels[\n                        int(ops.convert_to_numpy(ops.argmax(label, axis=-1)).item())\n                    ]\n\n            self.table.add_data(\n                epoch,\n                wandb.Image(image),\n                label,\n                predicted_label,\n                top_5_classes,\n                top_5_probabilities,\n                predicted_probabilities,\n            )\n\n    def on_train_end(self, logs=None):\n        wandb.log({self.title: self.table})\n</code></pre>"},{"location":"keras/object_detection/","title":"KerasCV Object Detection Integrations","text":"<p>Utilities and callbacks integrating Weights &amp; Biases with the object detection systems present in KerasCV.</p> Examples Link WandB Run Easy and Simple Object-detection using KerasCV and Weights &amp; Biases Visualizing Object-detection datasets using Weights &amp; Biases Training an object detetction model using KerasCV and Weights &amp; Biases"},{"location":"keras/object_detection/#wandb_addons.keras.detection.WandBDetectionVisualizationCallback","title":"<code>WandBDetectionVisualizationCallback</code>","text":"<p>             Bases: <code>Callback</code></p> <p>Callback for visualizing ground-truth and predicted bounding boxes in an epoch-wise manner for an object-detection task for KerasCV. The callback logs a <code>wandb.Table</code> with columns for the epoch, the images overlayed with an interactive bounding box overlay corresponding to the ground-truth and predicted boudning boxes, the number of ground-truth bounding boxes and the predicted mean-confidence for each class.</p> <p>Examples:</p> <ul> <li>Fine-tuning an Object Detection Model using KerasCV.</li> <li>Sample Results for Fine-tuning an Object Detection Model using KerasCV</li> </ul> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>A batched dataset consisting of Ragged Tensors. This can be obtained by applying <code>ragged_batch()</code> on a <code>tf.data.Dataset</code>.</p> required <code>class_mapping</code> <code>Dict[int, str]</code> <p>A dictionary that maps the index of the classes to the corresponding class names.</p> required <code>max_batches_to_visualize</code> <code>Optional[int]</code> <p>Maximum number of batches from the dataset to be visualized.</p> <code>1</code> <code>iou_threshold</code> <code>float</code> <p>IoU threshold for non-max suppression during prediction.</p> <code>0.01</code> <code>confidence_threshold</code> <code>float</code> <p>Confidence threshold for non-max suppression during prediction.</p> <code>0.01</code> <code>source_bounding_box_format</code> <code>str</code> <p>Format of the source bounding box, one of <code>\"xyxy\"</code> or <code>\"xywh\"</code>.</p> <code>'xywh'</code> <code>title</code> <code>str</code> <p>Title under which the table will be logged to the Weights &amp; Biases workspace.</p> <code>'Evaluation-Table'</code> Source code in <code>wandb_addons/keras/detection/callback.py</code> <pre><code>class WandBDetectionVisualizationCallback(keras.callbacks.Callback):\n    \"\"\"Callback for visualizing ground-truth and predicted bounding boxes in an\n    epoch-wise manner for an object-detection task for\n    [KerasCV](https://github.com/keras-team/keras-cv). The callback logs a\n    [`wandb.Table`](https://docs.wandb.ai/guides/tables) with columns for the epoch,\n    the images overlayed with an interactive bounding box overlay corresponding to the\n    ground-truth and predicted boudning boxes, the number of ground-truth bounding\n    boxes and the predicted mean-confidence for each class.\n\n    !!! example \"Examples:\"\n        - [Fine-tuning an Object Detection Model using KerasCV](../examples/train_retinanet).\n        - [Sample Results for Fine-tuning an Object Detection Model using KerasCV](https://wandb.ai/geekyrakshit/keras-cv-callbacks/reports/Keras-CV-Integration--Vmlldzo1MTU4Nzk3)\n\n    Arguments:\n        dataset (tf.data.Dataset): A batched dataset consisting of Ragged Tensors.\n            This can be obtained by applying `ragged_batch()` on a `tf.data.Dataset`.\n        class_mapping (Dict[int, str]): A dictionary that maps the index of the classes\n            to the corresponding class names.\n        max_batches_to_visualize (Optional[int]): Maximum number of batches from the\n            dataset to be visualized.\n        iou_threshold (float): IoU threshold for non-max suppression during prediction.\n        confidence_threshold (float): Confidence threshold for non-max suppression\n            during prediction.\n        source_bounding_box_format (str): Format of the source bounding box, one of\n            `\"xyxy\"` or `\"xywh\"`.\n        title (str): Title under which the table will be logged to the Weights &amp; Biases\n            workspace.\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset: tf_data.Dataset,\n        class_mapping: dict,\n        max_batches_to_visualize: Optional[Union[int, None]] = 1,\n        iou_threshold: float = 0.01,\n        confidence_threshold: float = 0.01,\n        source_bounding_box_format: str = \"xywh\",\n        title: str = \"Evaluation-Table\",\n        *args,\n        **kwargs\n    ):\n        super().__init__(*args, **kwargs)\n        self.dataset = dataset.take(max_batches_to_visualize)\n        self.class_mapping = class_mapping\n        self.max_batches_to_visualize = max_batches_to_visualize\n        self.iou_threshold = iou_threshold\n        self.confidence_threshold = confidence_threshold\n        self.source_bounding_box_format = source_bounding_box_format\n        self.title = title\n        self.prediction_decoder = keras_cv.layers.MultiClassNonMaxSuppression(\n            bounding_box_format=self.source_bounding_box_format,\n            from_logits=True,\n            iou_threshold=self.iou_threshold,\n            confidence_threshold=self.confidence_threshold,\n        )\n        self.table = wandb.Table(\n            columns=[\n                \"Epoch\",\n                \"Image\",\n                \"Number-of-Ground-Truth-Boxes\",\n                \"Mean-Confidence\",\n            ]\n        )\n\n    def plot_prediction(self, epoch, image_batch, y_true_batch):\n        y_pred_batch = self.model.predict(image_batch, verbose=0)\n        y_pred = keras_cv.bounding_box.to_ragged(y_pred_batch)\n        image_batch = keras_cv.utils.to_numpy(image_batch).astype(np.uint8)\n        ground_truth_bounding_boxes = keras_cv.utils.to_numpy(\n            keras_cv.bounding_box.convert_format(\n                y_true_batch[\"boxes\"],\n                source=self.source_bounding_box_format,\n                target=\"xyxy\",\n                images=image_batch,\n            )\n        )\n        ground_truth_classes = keras_cv.utils.to_numpy(y_true_batch[\"classes\"])\n        predicted_bounding_boxes = keras_cv.utils.to_numpy(\n            keras_cv.bounding_box.convert_format(\n                y_pred[\"boxes\"],\n                source=self.source_bounding_box_format,\n                target=\"xyxy\",\n                images=image_batch,\n            )\n        )\n        for idx in range(image_batch.shape[0]):\n            num_detections = y_pred[\"num_detections\"][idx].item()\n            predicted_boxes = predicted_bounding_boxes[idx][:num_detections]\n            confidences = keras_cv.utils.to_numpy(\n                y_pred[\"confidence\"][idx][:num_detections]\n            )\n            predicted_classes = keras_cv.utils.to_numpy(\n                y_pred[\"classes\"][idx][:num_detections]\n            )\n\n            gt_classes = [\n                int(class_idx) for class_idx in ground_truth_classes[idx].tolist()\n            ]\n            gt_boxes = ground_truth_bounding_boxes[idx]\n            if -1 in gt_classes:\n                gt_classes = gt_classes[: gt_classes.index(-1)]\n\n            wandb_prediction_boxes = []\n            for box_idx in range(num_detections):\n                wandb_prediction_boxes.append(\n                    {\n                        \"position\": {\n                            \"minX\": predicted_boxes[box_idx][0]\n                            / image_batch[idx].shape[0],\n                            \"minY\": predicted_boxes[box_idx][1]\n                            / image_batch[idx].shape[1],\n                            \"maxX\": predicted_boxes[box_idx][2]\n                            / image_batch[idx].shape[0],\n                            \"maxY\": predicted_boxes[box_idx][3]\n                            / image_batch[idx].shape[1],\n                        },\n                        \"class_id\": int(predicted_classes[box_idx]),\n                        \"box_caption\": self.class_mapping[\n                            int(predicted_classes[box_idx])\n                        ],\n                        \"scores\": {\"confidence\": float(confidences[box_idx])},\n                    }\n                )\n\n            wandb_ground_truth_boxes = []\n            for box_idx in range(len(gt_classes)):\n                wandb_ground_truth_boxes.append(\n                    {\n                        \"position\": {\n                            \"minX\": int(gt_boxes[box_idx][0]),\n                            \"minY\": int(gt_boxes[box_idx][1]),\n                            \"maxX\": int(gt_boxes[box_idx][2]),\n                            \"maxY\": int(gt_boxes[box_idx][3]),\n                        },\n                        \"class_id\": gt_classes[box_idx],\n                        \"box_caption\": self.class_mapping[int(gt_classes[box_idx])],\n                        \"domain\": \"pixel\",\n                    }\n                )\n            wandb_image = wandb.Image(\n                image_batch[idx],\n                boxes={\n                    \"ground-truth\": {\n                        \"box_data\": wandb_ground_truth_boxes,\n                        \"class_labels\": self.class_mapping,\n                    },\n                    \"predictions\": {\n                        \"box_data\": wandb_prediction_boxes,\n                        \"class_labels\": self.class_mapping,\n                    },\n                },\n            )\n            mean_confidence_dict = get_mean_confidence_per_class(\n                confidences, predicted_classes, self.class_mapping\n            )\n            self.table.add_data(\n                epoch, wandb_image, len(gt_classes), mean_confidence_dict\n            )\n\n    def on_epoch_end(self, epoch, logs):\n        original_prediction_decoder = self.model._prediction_decoder\n        self.model.prediction_decoder = self.prediction_decoder\n        for _ in tqdm(range(self.max_batches_to_visualize)):\n            image_batch, y_true_batch = next(iter(self.dataset))\n            self.plot_prediction(epoch, image_batch, y_true_batch)\n        self.model.prediction_decoder = original_prediction_decoder\n\n    def on_train_end(self, logs):\n        wandb.log({self.title: self.table})\n</code></pre>"},{"location":"keras/object_detection/#wandb_addons.keras.detection.log_predictions_to_wandb","title":"<code>log_predictions_to_wandb(image_batch, prediction_batch, class_mapping, source_bbox_format='xywh')</code>","text":"<p>Function to log inference results to a wandb.Table with images overlayed with an interactive bounding box overlay corresponding to the predicted boxes.</p> <p>Example notebooks:</p> <ul> <li>Object Detection using KerasCV.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>image_batch</code> <code>Union[KerasTensor, array]</code> <p>The batch of resized and batched images that is also passed to the model.</p> required <code>prediction_batch</code> <code>Union[KerasTensor, array]</code> <p>The prediction batch that is the output of the detection model.</p> required <code>class_mapping</code> <code>Dict[int, str]</code> <p>A dictionary that maps the index of the classes to the corresponding class names.</p> required <code>source_bbox_format</code> <code>str</code> <p>Format of the source bounding box, one of <code>\"xyxy\"</code> or <code>\"xywh\"</code>.</p> <code>'xywh'</code> Source code in <code>wandb_addons/keras/detection/inference.py</code> <pre><code>def log_predictions_to_wandb(\n    image_batch: np.array,\n    prediction_batch: np.array,\n    class_mapping: Dict[int, str],\n    source_bbox_format: str = \"xywh\",\n):\n    \"\"\"Function to log inference results to a\n    [wandb.Table](https://docs.wandb.ai/guides/data-vis) with images overlayed with an\n    interactive bounding box overlay corresponding to the predicted boxes.\n\n    !!! example \"Example notebooks:\"\n        - [Object Detection using KerasCV](../examples/object_detection_inference).\n\n    Arguments:\n        image_batch (Union[backend.KerasTensor, np.array]): The batch of resized and\n            batched images that is also passed to the model.\n        prediction_batch (Union[backend.KerasTensor, np.array]): The prediction batch\n            that is the output of the detection model.\n        class_mapping (Dict[int, str]): A dictionary that maps the index of the classes\n            to the corresponding class names.\n        source_bbox_format (str): Format of the source bounding box, one of `\"xyxy\"`\n            or `\"xywh\"`.\n    \"\"\"\n    batch_size = prediction_batch[\"boxes\"].shape[0]\n    image_batch = keras_cv.utils.to_numpy(image_batch).astype(np.uint8)\n    bounding_boxes = keras_cv.utils.to_numpy(\n        keras_cv.bounding_box.convert_format(\n            prediction_batch[\"boxes\"],\n            source=source_bbox_format,\n            target=\"xyxy\",\n            images=image_batch,\n        )\n    )\n    table = wandb.Table(columns=[\"Predictions\", \"Mean-Confidence\"])\n    for idx in tqdm(range(batch_size)):\n        num_detections = prediction_batch[\"num_detections\"][idx].item()\n        predicted_boxes = bounding_boxes[idx][:num_detections]\n        confidences = prediction_batch[\"confidence\"][idx][:num_detections]\n        classes = prediction_batch[\"classes\"][idx][:num_detections]\n        wandb_boxes = []\n        for box_idx in range(num_detections):\n            wandb_boxes.append(\n                {\n                    \"position\": {\n                        \"minX\": predicted_boxes[box_idx][0] / image_batch[idx].shape[0],\n                        \"minY\": predicted_boxes[box_idx][1] / image_batch[idx].shape[1],\n                        \"maxX\": predicted_boxes[box_idx][2] / image_batch[idx].shape[0],\n                        \"maxY\": predicted_boxes[box_idx][3] / image_batch[idx].shape[1],\n                    },\n                    \"class_id\": int(classes[box_idx]),\n                    \"box_caption\": class_mapping[int(classes[box_idx])],\n                    \"scores\": {\"confidence\": float(confidences[box_idx])},\n                }\n            )\n        wandb_image = wandb.Image(\n            image_batch[idx],\n            boxes={\n                \"predictions\": {\n                    \"box_data\": wandb_boxes,\n                    \"class_labels\": class_mapping,\n                },\n            },\n        )\n        mean_confidence_dict = get_mean_confidence_per_class(\n            confidences, classes, class_mapping\n        )\n        table.add_data(wandb_image, mean_confidence_dict)\n    wandb.log({\"Prediction-Table\": table})\n</code></pre>"},{"location":"keras/object_detection/#wandb_addons.keras.detection.visualize_dataset","title":"<code>visualize_dataset(dataset, class_mapping, title, max_batches_to_visualize=1, source_bbox_format='xywh')</code>","text":"<p>Function to visualize a dataset using a wandb.Table with 2 columns, one with the images overlayed with an interactive bounding box overlay corresponding to the predicted boxes and another showing the number of bounding boxes corresponding to that image.</p> <p>Example notebooks:</p> <ul> <li>Object Detection using KerasCV.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>A batched dataset consisting of Ragged Tensors. This can be obtained by applying <code>ragged_batch()</code> on a <code>tf.data.Dataset</code>.</p> required <code>class_mapping</code> <code>Dict[int, str]</code> <p>A dictionary that maps the index of the classes to the corresponding class names.</p> required <code>title</code> <code>str</code> <p>Title under which the table will be logged to the Weights &amp; Biases workspace.</p> required <code>max_batches_to_visualize</code> <code>Optional[int]</code> <p>Maximum number of batches from the dataset to be visualized.</p> <code>1</code> <code>source_bbox_format</code> <code>str</code> <p>Format of the source bounding box, one of <code>\"xyxy\"</code> or <code>\"xywh\"</code>.</p> <code>'xywh'</code> Source code in <code>wandb_addons/keras/detection/dataset.py</code> <pre><code>def visualize_dataset(\n    dataset: tf_data.Dataset,\n    class_mapping: Dict[int, str],\n    title: str,\n    max_batches_to_visualize: Optional[int] = 1,\n    source_bbox_format: str = \"xywh\",\n):\n    \"\"\"Function to visualize a dataset using a\n    [wandb.Table](https://docs.wandb.ai/guides/data-vis) with 2 columns, one with the\n    images overlayed with an interactive bounding box overlay corresponding to the\n    predicted boxes and another showing the number of bounding boxes corresponding to\n    that image.\n\n    !!! example \"Example notebooks:\"\n        - [Object Detection using KerasCV](../examples/visualize_dataset).\n\n    Arguments:\n        dataset (tf.data.Dataset): A batched dataset consisting of Ragged Tensors.\n            This can be obtained by applying `ragged_batch()` on a `tf.data.Dataset`.\n        class_mapping (Dict[int, str]): A dictionary that maps the index of the classes\n            to the corresponding class names.\n        title (str): Title under which the table will be logged to the Weights &amp; Biases\n            workspace.\n        max_batches_to_visualize (Optional[int]): Maximum number of batches from the\n            dataset to be visualized.\n        source_bbox_format (str): Format of the source bounding box, one of `\"xyxy\"`\n            or `\"xywh\"`.\n    \"\"\"\n    table = wandb.Table(columns=[\"Images\", \"Number-of-Objects\"])\n    if max_batches_to_visualize is not None:\n        dataset = iter(dataset.take(max_batches_to_visualize))\n    else:\n        dataset = iter(dataset)\n        max_batches_to_visualize = tf_data.experimental.cardinality(dataset).numpy()\n\n    for _ in tqdm(range(max_batches_to_visualize)):\n        sample = next(dataset)\n        images, bounding_boxes = sample[\"images\"], sample[\"bounding_boxes\"]\n        images = keras_cv.utils.to_numpy(images)\n        images = keras_cv.utils.transform_value_range(\n            images, original_range=(0, 255), target_range=(0, 255)\n        )\n        for key, val in bounding_boxes.items():\n            bounding_boxes[key] = keras_cv.utils.to_numpy(val)\n        bounding_boxes[\"boxes\"] = keras_cv.bounding_box.convert_format(\n            bounding_boxes[\"boxes\"],\n            source=source_bbox_format,\n            target=\"xyxy\",\n            images=images,\n        )\n        bounding_boxes[\"boxes\"] = keras_cv.utils.to_numpy(bounding_boxes[\"boxes\"])\n        for idx in range(images.shape[0]):\n            classes = [\n                int(class_idx) for class_idx in bounding_boxes[\"classes\"][idx].tolist()\n            ]\n            bboxes = bounding_boxes[\"boxes\"][idx]\n            if -1 in classes:\n                classes = classes[: classes.index(-1)]\n            wandb_boxes = []\n            for object_idx in range(len(classes)):\n                wandb_boxes.append(\n                    {\n                        \"position\": {\n                            \"minX\": int(bboxes[object_idx][0]),\n                            \"minY\": int(bboxes[object_idx][1]),\n                            \"maxX\": int(bboxes[object_idx][2]),\n                            \"maxY\": int(bboxes[object_idx][3]),\n                        },\n                        \"class_id\": classes[object_idx],\n                        \"box_caption\": class_mapping[int(classes[object_idx])],\n                        \"domain\": \"pixel\",\n                    }\n                )\n            wandb_image = wandb.Image(\n                images[idx],\n                boxes={\n                    \"gorund-truth\": {\n                        \"box_data\": wandb_boxes,\n                        \"class_labels\": class_mapping,\n                    },\n                },\n            )\n            table.add_data(wandb_image, len(classes))\n\n    wandb.log({title: table})\n</code></pre>"},{"location":"keras/examples/image_classification/","title":"Image Classification using Keras","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install -q namex\n!apt install python3.10-venv\n!git clone https://github.com/keras-team/keras-core.git &amp;&amp; cd keras-core\n!python pip_build.py --install\n!pip install -q git+https://github.com/keras-team/keras-cv\n!pip install --upgrade -q git+https://github.com/soumik12345/wandb-addons\n</pre> !pip install -q namex !apt install python3.10-venv !git clone https://github.com/keras-team/keras-core.git &amp;&amp; cd keras-core !python pip_build.py --install !pip install -q git+https://github.com/keras-team/keras-cv !pip install --upgrade -q git+https://github.com/soumik12345/wandb-addons In\u00a0[\u00a0]: Copied! <pre>import os\n\n# works with tensorflow, jax and torch backends\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.image as tf_image\nimport tensorflow.data as tf_data\nimport tensorflow_datasets as tfds\n\nimport keras as keras\nfrom keras import ops\nfrom keras import layers\nfrom keras import applications\n\nimport wandb\nfrom wandb_addons.keras import WandbMetricsLogger\nfrom wandb_addons.keras import WandbModelCheckpoint\nfrom wandb_addons.keras import WandBImageClassificationCallback\n</pre> import os  # works with tensorflow, jax and torch backends os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  import numpy as np import tensorflow as tf import tensorflow.image as tf_image import tensorflow.data as tf_data import tensorflow_datasets as tfds  import keras as keras from keras import ops from keras import layers from keras import applications  import wandb from wandb_addons.keras import WandbMetricsLogger from wandb_addons.keras import WandbModelCheckpoint from wandb_addons.keras import WandBImageClassificationCallback In\u00a0[\u00a0]: Copied! <pre>wandb.init(project=\"keras-community-days\", job_type=\"classification\")\n\nconfig = wandb.config\nconfig.batch_size = 64\nconfig.dataset_name = \"stanford_dogs\"\nconfig.image_size = 224\nconfig.epochs = 10\nconfig.top_dropout_rate = 0.2\n</pre> wandb.init(project=\"keras-community-days\", job_type=\"classification\")  config = wandb.config config.batch_size = 64 config.dataset_name = \"stanford_dogs\" config.image_size = 224 config.epochs = 10 config.top_dropout_rate = 0.2 In\u00a0[\u00a0]: Copied! <pre>(train_dataset, test_dataset), info = tfds.load(\n    config.dataset_name,\n    split=[\"train\", \"test\"],\n    with_info=True,\n    as_supervised=True\n)\ntrain_dataset = train_dataset.take(100)\ntest_dataset = test_dataset.take(100)\nconfig.classes = [name.split(\"-\")[-1] for name in info.features[\"label\"].names]\n</pre> (train_dataset, test_dataset), info = tfds.load(     config.dataset_name,     split=[\"train\", \"test\"],     with_info=True,     as_supervised=True ) train_dataset = train_dataset.take(100) test_dataset = test_dataset.take(100) config.classes = [name.split(\"-\")[-1] for name in info.features[\"label\"].names] In\u00a0[\u00a0]: Copied! <pre>train_dataset = train_dataset.map(\n    lambda image, label: (\n        tf_image.resize(image, (config.image_size, config.image_size)), label\n    )\n)\ntest_dataset = test_dataset.map(\n    lambda image, label: (\n        tf_image.resize(image, (config.image_size, config.image_size)), label\n    )\n)\n</pre> train_dataset = train_dataset.map(     lambda image, label: (         tf_image.resize(image, (config.image_size, config.image_size)), label     ) ) test_dataset = test_dataset.map(     lambda image, label: (         tf_image.resize(image, (config.image_size, config.image_size)), label     ) ) In\u00a0[\u00a0]: Copied! <pre>def input_preprocess(image, label):\n    label = tf.one_hot(label, len(config.classes))\n    return image, label\n\n\ntrain_dataset = train_dataset.map(\n    input_preprocess, num_parallel_calls=tf_data.AUTOTUNE\n)\ntrain_dataset = train_dataset.batch(\n    batch_size=config.batch_size, drop_remainder=True\n)\ntrain_dataset = train_dataset.prefetch(tf_data.AUTOTUNE)\n\ntest_dataset = test_dataset.map(\n    input_preprocess, num_parallel_calls=tf_data.AUTOTUNE\n)\ntest_dataset = test_dataset.batch(\n    batch_size=config.batch_size, drop_remainder=True\n)\ntest_dataset = test_dataset.prefetch(tf_data.AUTOTUNE)\n</pre> def input_preprocess(image, label):     label = tf.one_hot(label, len(config.classes))     return image, label   train_dataset = train_dataset.map(     input_preprocess, num_parallel_calls=tf_data.AUTOTUNE ) train_dataset = train_dataset.batch(     batch_size=config.batch_size, drop_remainder=True ) train_dataset = train_dataset.prefetch(tf_data.AUTOTUNE)  test_dataset = test_dataset.map(     input_preprocess, num_parallel_calls=tf_data.AUTOTUNE ) test_dataset = test_dataset.batch(     batch_size=config.batch_size, drop_remainder=True ) test_dataset = test_dataset.prefetch(tf_data.AUTOTUNE) In\u00a0[\u00a0]: Copied! <pre>inputs = keras.Input(shape=(config.image_size, config.image_size, 3))\nx = inputs\nmodel = applications.EfficientNetB0(\n    include_top=False, input_tensor=x, weights=\"imagenet\"\n)\n\n# Freeze the pretrained weights\nmodel.trainable = False\n\n# Rebuild top\nx = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\nx = layers.BatchNormalization()(x)\n\nx = layers.Dropout(config.top_dropout_rate, name=\"top_dropout\")(x)\noutputs = layers.Dense(len(config.classes), activation=\"softmax\", name=\"pred\")(x)\n\n# Compile\nmodel = keras.Model(inputs, outputs, name=\"EfficientNet\")\noptimizer = keras.optimizers.Adam(learning_rate=1e-2)\nmodel.compile(\n    optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n)\n</pre> inputs = keras.Input(shape=(config.image_size, config.image_size, 3)) x = inputs model = applications.EfficientNetB0(     include_top=False, input_tensor=x, weights=\"imagenet\" )  # Freeze the pretrained weights model.trainable = False  # Rebuild top x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output) x = layers.BatchNormalization()(x)  x = layers.Dropout(config.top_dropout_rate, name=\"top_dropout\")(x) outputs = layers.Dense(len(config.classes), activation=\"softmax\", name=\"pred\")(x)  # Compile model = keras.Model(inputs, outputs, name=\"EfficientNet\") optimizer = keras.optimizers.Adam(learning_rate=1e-2) model.compile(     optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"] ) In\u00a0[\u00a0]: Copied! <pre>callbacks = [\n    # Log your metrics as you train\n    WandbMetricsLogger(log_freq=\"batch\"),\n    # Save and version your model checkpoints\n    WandbModelCheckpoint(\"model.keras\"),\n    # Visualize your model's performance\n    WandBImageClassificationCallback(\n        dataset=test_dataset,\n        class_labels=config.classes,\n        max_items_for_visualization=50,\n    )\n]\n\n# Start training...\nmodel.fit(\n    train_dataset,\n    validation_data=test_dataset,\n    epochs=config.epochs,\n    callbacks=callbacks\n)\n\n# Finish the experiment\nwandb.finish()\n</pre> callbacks = [     # Log your metrics as you train     WandbMetricsLogger(log_freq=\"batch\"),     # Save and version your model checkpoints     WandbModelCheckpoint(\"model.keras\"),     # Visualize your model's performance     WandBImageClassificationCallback(         dataset=test_dataset,         class_labels=config.classes,         max_items_for_visualization=50,     ) ]  # Start training... model.fit(     train_dataset,     validation_data=test_dataset,     epochs=config.epochs,     callbacks=callbacks )  # Finish the experiment wandb.finish()"},{"location":"keras/examples/image_classification/#image-classification-using-keras","title":"Image Classification using Keras\u00b6","text":"<p>This notebook demonstrates how to perform transfer-learning using a pre-trained model from Keras Applications and perform experiment tracking using Weights &amp; Biases.</p>"},{"location":"keras/examples/image_classification/#install-dependencies","title":"Install Dependencies\u00b6","text":""},{"location":"keras/examples/image_classification/#import-the-libraries","title":"Import the Libraries\u00b6","text":""},{"location":"keras/examples/image_classification/#initialize-a-weights-biases-run-and-set-up-the-configs","title":"Initialize a Weights &amp; Biases run and Set up the Configs\u00b6","text":""},{"location":"keras/examples/image_classification/#setting-up-the-dataset-pipeline","title":"Setting up the Dataset Pipeline\u00b6","text":""},{"location":"keras/examples/image_classification/#load-the-data","title":"Load the data\u00b6","text":""},{"location":"keras/examples/image_classification/#resize-the-data","title":"Resize the Data\u00b6","text":""},{"location":"keras/examples/image_classification/#preprocess-the-labels","title":"Preprocess the Labels\u00b6","text":""},{"location":"keras/examples/image_classification/#defining-the-model","title":"Defining the Model\u00b6","text":""},{"location":"keras/examples/image_classification/#start-training","title":"Start Training\u00b6","text":""},{"location":"keras/examples/object_detection_inference/","title":"Object-Detection using KerasCV and Weights &amp; Biases","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install --upgrade -q git+https://github.com/keras-team/keras-cv\n!pip install --upgrade -q git+https://github.com/soumik12345/wandb-addons\n!pip install -q namex\n!apt install python3.10-venv\n!git clone https://github.com/keras-team/keras-core.git &amp;&amp; cd keras-core\n!python pip_build.py --install\n</pre> !pip install --upgrade -q git+https://github.com/keras-team/keras-cv !pip install --upgrade -q git+https://github.com/soumik12345/wandb-addons !pip install -q namex !apt install python3.10-venv !git clone https://github.com/keras-team/keras-core.git &amp;&amp; cd keras-core !python pip_build.py --install In\u00a0[\u00a0]: Copied! <pre>import keras_cv\nimport keras as keras\nfrom keras import ops\n\nimport numpy as np\n\nimport wandb\nfrom wandb_addons.keras.detection import log_predictions_to_wandb\n</pre> import keras_cv import keras as keras from keras import ops  import numpy as np  import wandb from wandb_addons.keras.detection import log_predictions_to_wandb In\u00a0[\u00a0]: Copied! <pre>wandb.init(project=\"keras-community-days\", job_type=\"detection/inference\")\n\nconfig = wandb.config\nconfig.model_name = \"retinanet_resnet50_pascalvoc\"\nconfig.image_size = 640\n\nclass_ids = [\n    \"Aeroplane\",\n    \"Bicycle\",\n    \"Bird\",\n    \"Boat\",\n    \"Bottle\",\n    \"Bus\",\n    \"Car\",\n    \"Cat\",\n    \"Chair\",\n    \"Cow\",\n    \"Dining Table\",\n    \"Dog\",\n    \"Horse\",\n    \"Motorbike\",\n    \"Person\",\n    \"Potted Plant\",\n    \"Sheep\",\n    \"Sofa\",\n    \"Train\",\n    \"Tvmonitor\",\n    \"Total\",\n]\nconfig.class_mapping = dict(zip(range(len(class_ids)), class_ids))\n</pre> wandb.init(project=\"keras-community-days\", job_type=\"detection/inference\")  config = wandb.config config.model_name = \"retinanet_resnet50_pascalvoc\" config.image_size = 640  class_ids = [     \"Aeroplane\",     \"Bicycle\",     \"Bird\",     \"Boat\",     \"Bottle\",     \"Bus\",     \"Car\",     \"Cat\",     \"Chair\",     \"Cow\",     \"Dining Table\",     \"Dog\",     \"Horse\",     \"Motorbike\",     \"Person\",     \"Potted Plant\",     \"Sheep\",     \"Sofa\",     \"Train\",     \"Tvmonitor\",     \"Total\", ] config.class_mapping = dict(zip(range(len(class_ids)), class_ids)) In\u00a0[\u00a0]: Copied! <pre>filepath_1 = keras.utils.get_file(\n    origin=\"https://i.imgur.com/gCNcJJI.jpg\"\n)\nfilepath_2 = keras.utils.get_file(\n    origin=\"https://i.imgur.com/M8LR4fz.png\"\n)\nfilepath_3 = keras.utils.get_file(\n    origin=\"https://i.imgur.com/H2d3VJC.jpeg\"\n)\nfilepath_4 = keras.utils.get_file(\n    origin=\"https://i.imgur.com/eTxzk46.jpeg\"\n)\n\nimage_1 = keras.utils.load_img(filepath_1)\nimage_1 = np.array(image_1)\nimage_2 = keras.utils.load_img(filepath_2)\nimage_2 = np.array(image_2)\nimage_3 = keras.utils.load_img(filepath_3)\nimage_3 = np.array(image_3)\nimage_4 = keras.utils.load_img(filepath_4)\nimage_4 = np.array(image_4)\n</pre> filepath_1 = keras.utils.get_file(     origin=\"https://i.imgur.com/gCNcJJI.jpg\" ) filepath_2 = keras.utils.get_file(     origin=\"https://i.imgur.com/M8LR4fz.png\" ) filepath_3 = keras.utils.get_file(     origin=\"https://i.imgur.com/H2d3VJC.jpeg\" ) filepath_4 = keras.utils.get_file(     origin=\"https://i.imgur.com/eTxzk46.jpeg\" )  image_1 = keras.utils.load_img(filepath_1) image_1 = np.array(image_1) image_2 = keras.utils.load_img(filepath_2) image_2 = np.array(image_2) image_3 = keras.utils.load_img(filepath_3) image_3 = np.array(image_3) image_4 = keras.utils.load_img(filepath_4) image_4 = np.array(image_4) In\u00a0[\u00a0]: Copied! <pre>inference_resizing = keras_cv.layers.Resizing(\n    config.image_size,\n    config.image_size,\n    pad_to_aspect_ratio=True,\n    bounding_box_format=\"xywh\"\n)\nimage_1_resized = inference_resizing([image_1])\nimage_2_resized = inference_resizing([image_2])\nimage_3_resized = inference_resizing([image_3])\nimage_4_resized = inference_resizing([image_4])\nimage_batch = ops.concatenate(\n    [\n        image_1_resized,\n        image_2_resized,\n        image_3_resized,\n        image_4_resized\n    ],\n    axis=0\n)\nimage_batch.shape\n</pre> inference_resizing = keras_cv.layers.Resizing(     config.image_size,     config.image_size,     pad_to_aspect_ratio=True,     bounding_box_format=\"xywh\" ) image_1_resized = inference_resizing([image_1]) image_2_resized = inference_resizing([image_2]) image_3_resized = inference_resizing([image_3]) image_4_resized = inference_resizing([image_4]) image_batch = ops.concatenate(     [         image_1_resized,         image_2_resized,         image_3_resized,         image_4_resized     ],     axis=0 ) image_batch.shape In\u00a0[\u00a0]: Copied! <pre>pretrained_model = keras_cv.models.RetinaNet.from_preset(\n    config.model_name, bounding_box_format=\"xywh\"\n)\n</pre> pretrained_model = keras_cv.models.RetinaNet.from_preset(     config.model_name, bounding_box_format=\"xywh\" ) In\u00a0[\u00a0]: Copied! <pre>y_pred = pretrained_model.predict(image_batch)\nlog_predictions_to_wandb(image_batch, y_pred, config.class_mapping)\n</pre> y_pred = pretrained_model.predict(image_batch) log_predictions_to_wandb(image_batch, y_pred, config.class_mapping) <p>Need more control over the visualization? Reduce the threshold for Confidence by uncommenting the following code!</p> In\u00a0[\u00a0]: Copied! <pre># prediction_decoder = keras_cv.layers.MultiClassNonMaxSuppression(\n#     bounding_box_format=\"xywh\",\n#     from_logits=True,\n#     iou_threshold=0.01,\n#     confidence_threshold=0.01,\n# )\n# pretrained_model.prediction_decoder = prediction_decoder\n# y_pred = pretrained_model.predict(image_batch)\n# log_predictions_to_wandb(image_batch, y_pred, config.class_mapping)\n</pre> # prediction_decoder = keras_cv.layers.MultiClassNonMaxSuppression( #     bounding_box_format=\"xywh\", #     from_logits=True, #     iou_threshold=0.01, #     confidence_threshold=0.01, # ) # pretrained_model.prediction_decoder = prediction_decoder # y_pred = pretrained_model.predict(image_batch) # log_predictions_to_wandb(image_batch, y_pred, config.class_mapping) <p>Finish the run!</p> In\u00a0[\u00a0]: Copied! <pre>wandb.finish()\n</pre> wandb.finish()"},{"location":"keras/examples/object_detection_inference/#object-detection-using-kerascv-and-weights-biases","title":"Object-Detection using KerasCV and Weights &amp; Biases\u00b6","text":"<p>This notebook demonstrates how to use a pre-trained object-detection model from KerasCV and visualize the results using Weights &amp; Biases.</p>"},{"location":"keras/examples/object_detection_inference/#import-the-libraries","title":"Import the Libraries\u00b6","text":""},{"location":"keras/examples/object_detection_inference/#initialize-a-weights-biases-run-and-set-up-the-configs","title":"Initialize a Weights &amp; Biases run and Set up the Configs\u00b6","text":""},{"location":"keras/examples/object_detection_inference/#fetch-the-images","title":"Fetch the Images\u00b6","text":""},{"location":"keras/examples/object_detection_inference/#preprocess-the-images","title":"Preprocess the Images\u00b6","text":""},{"location":"keras/examples/object_detection_inference/#define-a-pre-trained-model-from-kerascv","title":"Define a pre-trained model from KerasCV\u00b6","text":""},{"location":"keras/examples/object_detection_inference/#perform-inference","title":"Perform Inference\u00b6","text":""},{"location":"keras/examples/train_retinanet/","title":"Train a RetinaNet using KerasCV and Weights &amp; Biases","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install --upgrade -q git+https://github.com/keras-team/keras-cv\n!pip install --upgrade -q git+https://github.com/soumik12345/wandb-addons\n</pre> !pip install --upgrade -q git+https://github.com/keras-team/keras-cv !pip install --upgrade -q git+https://github.com/soumik12345/wandb-addons In\u00a0[\u00a0]: Copied! <pre>import tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow_datasets as tfds\n\nimport keras_cv\nfrom tqdm.auto import tqdm\n\nimport wandb\nfrom wandb.keras import WandbMetricsLogger\nfrom wandb_addons.keras.detection import WandBDetectionVisualizationCallback\n</pre> import tensorflow as tf from tensorflow import keras import tensorflow_datasets as tfds  import keras_cv from tqdm.auto import tqdm  import wandb from wandb.keras import WandbMetricsLogger from wandb_addons.keras.detection import WandBDetectionVisualizationCallback In\u00a0[\u00a0]: Copied! <pre>wandb.init(\n    project=\"keras-cv-callbacks\", entity=\"geekyrakshit\", job_type=\"detection\"\n)\n\nconfig = wandb.config\nconfig.batch_size = 4\nconfig.base_lr = 0.005\nconfig.image_size = 640\nconfig.model_name = \"retinanet_resnet50_pascalvoc\"\nconfig.momentum = 0.9\nconfig.global_clipnorm = 10.0\nconfig.epochs = 3\n\n\nclass_ids = [\n    \"Aeroplane\",\n    \"Bicycle\",\n    \"Bird\",\n    \"Boat\",\n    \"Bottle\",\n    \"Bus\",\n    \"Car\",\n    \"Cat\",\n    \"Chair\",\n    \"Cow\",\n    \"Dining Table\",\n    \"Dog\",\n    \"Horse\",\n    \"Motorbike\",\n    \"Person\",\n    \"Potted Plant\",\n    \"Sheep\",\n    \"Sofa\",\n    \"Train\",\n    \"Tvmonitor\",\n    \"Total\",\n]\nconfig.class_mapping = dict(zip(range(len(class_ids)), class_ids))\n</pre> wandb.init(     project=\"keras-cv-callbacks\", entity=\"geekyrakshit\", job_type=\"detection\" )  config = wandb.config config.batch_size = 4 config.base_lr = 0.005 config.image_size = 640 config.model_name = \"retinanet_resnet50_pascalvoc\" config.momentum = 0.9 config.global_clipnorm = 10.0 config.epochs = 3   class_ids = [     \"Aeroplane\",     \"Bicycle\",     \"Bird\",     \"Boat\",     \"Bottle\",     \"Bus\",     \"Car\",     \"Cat\",     \"Chair\",     \"Cow\",     \"Dining Table\",     \"Dog\",     \"Horse\",     \"Motorbike\",     \"Person\",     \"Potted Plant\",     \"Sheep\",     \"Sofa\",     \"Train\",     \"Tvmonitor\",     \"Total\", ] config.class_mapping = dict(zip(range(len(class_ids)), class_ids)) In\u00a0[\u00a0]: Copied! <pre>train_ds = tfds.load(\n    \"voc/2007\",\n    split=\"train+validation\",\n    with_info=False,\n    shuffle_files=True,\n)\ntrain_ds = train_ds.concatenate(\n    tfds.load(\n        \"voc/2012\",\n        split=\"train+validation\",\n        with_info=False,\n        shuffle_files=True,\n    )\n)\neval_ds = tfds.load(\"voc/2007\", split=\"test\", with_info=False)\n</pre> train_ds = tfds.load(     \"voc/2007\",     split=\"train+validation\",     with_info=False,     shuffle_files=True, ) train_ds = train_ds.concatenate(     tfds.load(         \"voc/2012\",         split=\"train+validation\",         with_info=False,         shuffle_files=True,     ) ) eval_ds = tfds.load(\"voc/2007\", split=\"test\", with_info=False) In\u00a0[\u00a0]: Copied! <pre>def unpackage_tfds_inputs(inputs, bounding_box_format):\n    image = inputs[\"image\"]\n    boxes = keras_cv.bounding_box.convert_format(\n        inputs[\"objects\"][\"bbox\"],\n        images=image,\n        source=\"rel_yxyx\",\n        target=bounding_box_format,\n    )\n    bounding_boxes = {\n        \"classes\": tf.cast(inputs[\"objects\"][\"label\"], dtype=tf.float32),\n        \"boxes\": tf.cast(boxes, dtype=tf.float32),\n    }\n    return {\n        \"images\": tf.cast(image, tf.float32),\n        \"bounding_boxes\": bounding_boxes,\n    }\n\n\ntrain_ds = train_ds.map(\n    lambda inputs: unpackage_tfds_inputs(inputs, bounding_box_format=\"xywh\"),\n    num_parallel_calls=tf.data.AUTOTUNE,\n)\neval_ds = eval_ds.map(\n    lambda inputs: unpackage_tfds_inputs(inputs, bounding_box_format=\"xywh\"),\n    num_parallel_calls=tf.data.AUTOTUNE,\n)\n</pre> def unpackage_tfds_inputs(inputs, bounding_box_format):     image = inputs[\"image\"]     boxes = keras_cv.bounding_box.convert_format(         inputs[\"objects\"][\"bbox\"],         images=image,         source=\"rel_yxyx\",         target=bounding_box_format,     )     bounding_boxes = {         \"classes\": tf.cast(inputs[\"objects\"][\"label\"], dtype=tf.float32),         \"boxes\": tf.cast(boxes, dtype=tf.float32),     }     return {         \"images\": tf.cast(image, tf.float32),         \"bounding_boxes\": bounding_boxes,     }   train_ds = train_ds.map(     lambda inputs: unpackage_tfds_inputs(inputs, bounding_box_format=\"xywh\"),     num_parallel_calls=tf.data.AUTOTUNE, ) eval_ds = eval_ds.map(     lambda inputs: unpackage_tfds_inputs(inputs, bounding_box_format=\"xywh\"),     num_parallel_calls=tf.data.AUTOTUNE, ) In\u00a0[\u00a0]: Copied! <pre>train_ds = train_ds.ragged_batch(config.batch_size, drop_remainder=True)\neval_ds = eval_ds.ragged_batch(config.batch_size, drop_remainder=True)\n</pre> train_ds = train_ds.ragged_batch(config.batch_size, drop_remainder=True) eval_ds = eval_ds.ragged_batch(config.batch_size, drop_remainder=True) In\u00a0[\u00a0]: Copied! <pre>augmenter = keras.Sequential(\n    layers=[\n        keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=\"xywh\"),\n        keras_cv.layers.JitteredResize(\n            target_size=(config.image_size, config.image_size),\n            scale_factor=(0.75, 1.3),\n            bounding_box_format=\"xywh\"\n        ),\n    ]\n)\n\ntrain_ds = train_ds.map(augmenter, num_parallel_calls=tf.data.AUTOTUNE)\n</pre> augmenter = keras.Sequential(     layers=[         keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=\"xywh\"),         keras_cv.layers.JitteredResize(             target_size=(config.image_size, config.image_size),             scale_factor=(0.75, 1.3),             bounding_box_format=\"xywh\"         ),     ] )  train_ds = train_ds.map(augmenter, num_parallel_calls=tf.data.AUTOTUNE) In\u00a0[\u00a0]: Copied! <pre>inference_resizing = keras_cv.layers.Resizing(\n    config.image_size,\n    config.image_size,\n    bounding_box_format=\"xywh\",\n    pad_to_aspect_ratio=True\n)\neval_ds = eval_ds.map(inference_resizing, num_parallel_calls=tf.data.AUTOTUNE)\n</pre> inference_resizing = keras_cv.layers.Resizing(     config.image_size,     config.image_size,     bounding_box_format=\"xywh\",     pad_to_aspect_ratio=True ) eval_ds = eval_ds.map(inference_resizing, num_parallel_calls=tf.data.AUTOTUNE) In\u00a0[\u00a0]: Copied! <pre>def dict_to_tuple(inputs):\n    return inputs[\"images\"], keras_cv.bounding_box.to_dense(\n        inputs[\"bounding_boxes\"], max_boxes=32\n    )\n\n\ntrain_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\neval_ds = eval_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n\ntrain_ds = train_ds.prefetch(tf.data.AUTOTUNE)\neval_ds = eval_ds.prefetch(tf.data.AUTOTUNE)\n</pre> def dict_to_tuple(inputs):     return inputs[\"images\"], keras_cv.bounding_box.to_dense(         inputs[\"bounding_boxes\"], max_boxes=32     )   train_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE) eval_ds = eval_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)  train_ds = train_ds.prefetch(tf.data.AUTOTUNE) eval_ds = eval_ds.prefetch(tf.data.AUTOTUNE) In\u00a0[\u00a0]: Copied! <pre>model = keras_cv.models.RetinaNet.from_preset(\n    \"resnet50_imagenet\",\n    num_classes=len(config.class_mapping),\n    bounding_box_format=\"xywh\",\n)\nprint(model.prediction_decoder)\n\n\noptimizer = keras.optimizers.SGD(\n    learning_rate=config.base_lr,\n    momentum=config.momentum,\n    global_clipnorm=config.global_clipnorm,\n)\nmodel.compile(\n    classification_loss=\"focal\",\n    box_loss=\"smoothl1\",\n    optimizer=optimizer,\n    metrics=None,\n)\n</pre> model = keras_cv.models.RetinaNet.from_preset(     \"resnet50_imagenet\",     num_classes=len(config.class_mapping),     bounding_box_format=\"xywh\", ) print(model.prediction_decoder)   optimizer = keras.optimizers.SGD(     learning_rate=config.base_lr,     momentum=config.momentum,     global_clipnorm=config.global_clipnorm, ) model.compile(     classification_loss=\"focal\",     box_loss=\"smoothl1\",     optimizer=optimizer,     metrics=None, ) In\u00a0[\u00a0]: Copied! <pre>sampled_ds = train_ds.take(20)\n\nmodel.fit(\n    sampled_ds,\n    validation_data=sampled_ds,\n    epochs=config.epochs,\n    callbacks=[\n        WandbMetricsLogger(log_freq=\"batch\"),\n        WandBDetectionVisualizationCallback(\n            dataset=sampled_ds,\n            class_mapping=config.class_mapping,\n            max_batches_to_visualize=2,\n        ),\n    ],\n)\n\n# Finish the experiment\nwandb.finish()\n</pre> sampled_ds = train_ds.take(20)  model.fit(     sampled_ds,     validation_data=sampled_ds,     epochs=config.epochs,     callbacks=[         WandbMetricsLogger(log_freq=\"batch\"),         WandBDetectionVisualizationCallback(             dataset=sampled_ds,             class_mapping=config.class_mapping,             max_batches_to_visualize=2,         ),     ], )  # Finish the experiment wandb.finish()"},{"location":"keras/examples/train_retinanet/#train-a-retinanet-using-kerascv-and-weights-biases","title":"Train a RetinaNet using KerasCV and Weights &amp; Biases\u00b6","text":"<p>This notebook demonstrates how you can put together a simple pipeline for training an object detection model using KerasCV and tracking and managing the experiment using Weights &amp; Biases.</p> <p>Original Source: https://keras.io/guides/keras_cv/object_detection_keras_cv</p>"},{"location":"keras/examples/train_retinanet/#install-the-dependencies","title":"Install the Dependencies\u00b6","text":""},{"location":"keras/examples/train_retinanet/#initialize-a-weights-biases-run-and-set-up-the-configs","title":"Initialize a Weights &amp; Biases run and Set up the Configs\u00b6","text":""},{"location":"keras/examples/train_retinanet/#setup-the-dataset-pipeline","title":"Setup the Dataset Pipeline\u00b6","text":""},{"location":"keras/examples/train_retinanet/#define-and-compile-the-model","title":"Define and Compile the Model\u00b6","text":""},{"location":"keras/examples/train_retinanet/#start-training","title":"Start Training\u00b6","text":""},{"location":"keras/examples/visualize_dataset/","title":"Visualize Object-Detection Datasets using Weights &amp; Biases","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install --upgrade -q git+https://github.com/keras-team/keras-cv\n!pip install --upgrade -q git+https://github.com/soumik12345/wandb-addons\n</pre> !pip install --upgrade -q git+https://github.com/keras-team/keras-cv !pip install --upgrade -q git+https://github.com/soumik12345/wandb-addons In\u00a0[\u00a0]: Copied! <pre>import tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow_datasets as tfds\n\nimport keras_cv\n\nimport wandb\nfrom wandb_addons.keras.detection import visualize_dataset\n</pre> import tensorflow as tf from tensorflow import keras import tensorflow_datasets as tfds  import keras_cv  import wandb from wandb_addons.keras.detection import visualize_dataset In\u00a0[\u00a0]: Copied! <pre>wandb.init(project=\"keras-community-days\", job_type=\"visualization\")\n\nconfig = wandb.config\nconfig.batch_size = 4\nconfig.base_lr = 0.005\nconfig.model_name = \"retinanet_resnet50_pascalvoc\"\nconfig.momentum = 0.9\nconfig.global_clipnorm = 10.0\n\n\nclass_ids = [\n    \"Aeroplane\",\n    \"Bicycle\",\n    \"Bird\",\n    \"Boat\",\n    \"Bottle\",\n    \"Bus\",\n    \"Car\",\n    \"Cat\",\n    \"Chair\",\n    \"Cow\",\n    \"Dining Table\",\n    \"Dog\",\n    \"Horse\",\n    \"Motorbike\",\n    \"Person\",\n    \"Potted Plant\",\n    \"Sheep\",\n    \"Sofa\",\n    \"Train\",\n    \"Tvmonitor\",\n    \"Total\",\n]\nconfig.class_mapping = dict(zip(range(len(class_ids)), class_ids))\n</pre> wandb.init(project=\"keras-community-days\", job_type=\"visualization\")  config = wandb.config config.batch_size = 4 config.base_lr = 0.005 config.model_name = \"retinanet_resnet50_pascalvoc\" config.momentum = 0.9 config.global_clipnorm = 10.0   class_ids = [     \"Aeroplane\",     \"Bicycle\",     \"Bird\",     \"Boat\",     \"Bottle\",     \"Bus\",     \"Car\",     \"Cat\",     \"Chair\",     \"Cow\",     \"Dining Table\",     \"Dog\",     \"Horse\",     \"Motorbike\",     \"Person\",     \"Potted Plant\",     \"Sheep\",     \"Sofa\",     \"Train\",     \"Tvmonitor\",     \"Total\", ] config.class_mapping = dict(zip(range(len(class_ids)), class_ids)) In\u00a0[\u00a0]: Copied! <pre>def unpackage_raw_tfds_inputs(inputs, bounding_box_format):\n    image = inputs[\"image\"]\n    boxes = keras_cv.bounding_box.convert_format(\n        inputs[\"objects\"][\"bbox\"],\n        images=image,\n        source=\"rel_yxyx\",\n        target=bounding_box_format,\n    )\n    bounding_boxes = {\n        \"classes\": tf.cast(inputs[\"objects\"][\"label\"], dtype=tf.float32),\n        \"boxes\": tf.cast(boxes, dtype=tf.float32),\n    }\n    return {\n        \"images\": tf.cast(image, tf.float32),\n        \"bounding_boxes\": bounding_boxes\n    }\n\n\ndef load_pascal_voc(split, dataset, bounding_box_format):\n    ds = tfds.load(dataset, split=split, with_info=False, shuffle_files=True)\n    ds = ds.map(\n        lambda x: unpackage_raw_tfds_inputs(\n            x, bounding_box_format=bounding_box_format\n        ),\n        num_parallel_calls=tf.data.AUTOTUNE,\n    )\n    return ds\n\n\ntrain_ds = load_pascal_voc(\n    split=\"train\", dataset=\"voc/2007\", bounding_box_format=\"xywh\"\n)\neval_ds = load_pascal_voc(split=\"test\", dataset=\"voc/2007\", bounding_box_format=\"xywh\")\n\ntrain_ds = train_ds.shuffle(config.batch_size * 4)\n</pre> def unpackage_raw_tfds_inputs(inputs, bounding_box_format):     image = inputs[\"image\"]     boxes = keras_cv.bounding_box.convert_format(         inputs[\"objects\"][\"bbox\"],         images=image,         source=\"rel_yxyx\",         target=bounding_box_format,     )     bounding_boxes = {         \"classes\": tf.cast(inputs[\"objects\"][\"label\"], dtype=tf.float32),         \"boxes\": tf.cast(boxes, dtype=tf.float32),     }     return {         \"images\": tf.cast(image, tf.float32),         \"bounding_boxes\": bounding_boxes     }   def load_pascal_voc(split, dataset, bounding_box_format):     ds = tfds.load(dataset, split=split, with_info=False, shuffle_files=True)     ds = ds.map(         lambda x: unpackage_raw_tfds_inputs(             x, bounding_box_format=bounding_box_format         ),         num_parallel_calls=tf.data.AUTOTUNE,     )     return ds   train_ds = load_pascal_voc(     split=\"train\", dataset=\"voc/2007\", bounding_box_format=\"xywh\" ) eval_ds = load_pascal_voc(split=\"test\", dataset=\"voc/2007\", bounding_box_format=\"xywh\")  train_ds = train_ds.shuffle(config.batch_size * 4) In\u00a0[\u00a0]: Copied! <pre>train_ds = train_ds.ragged_batch(config.batch_size, drop_remainder=True)\neval_ds = eval_ds.ragged_batch(config.batch_size, drop_remainder=True)\n\n\nvisualize_dataset(\n    dataset=train_ds,\n    class_mapping=config.class_mapping,\n    title=\"Train-Dataset\",\n    max_batches_to_visualize=10,\n)\nvisualize_dataset(\n    dataset=eval_ds,\n    class_mapping=config.class_mapping,\n    title=\"Eval-Dataset\",\n    max_batches_to_visualize=10,\n)\n</pre> train_ds = train_ds.ragged_batch(config.batch_size, drop_remainder=True) eval_ds = eval_ds.ragged_batch(config.batch_size, drop_remainder=True)   visualize_dataset(     dataset=train_ds,     class_mapping=config.class_mapping,     title=\"Train-Dataset\",     max_batches_to_visualize=10, ) visualize_dataset(     dataset=eval_ds,     class_mapping=config.class_mapping,     title=\"Eval-Dataset\",     max_batches_to_visualize=10, ) In\u00a0[\u00a0]: Copied! <pre>augmenter = keras.Sequential(\n    layers=[\n        keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=\"xywh\"),\n        keras_cv.layers.JitteredResize(\n            target_size=(640, 640), scale_factor=(0.75, 1.3), bounding_box_format=\"xywh\"\n        ),\n    ]\n)\n\ntrain_ds = train_ds.map(augmenter, num_parallel_calls=tf.data.AUTOTUNE)\nvisualize_dataset(\n    dataset=train_ds,\n    class_mapping=config.class_mapping,\n    title=\"Augmented-Train-Dataset\",\n    max_batches_to_visualize=10,\n)\n</pre> augmenter = keras.Sequential(     layers=[         keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=\"xywh\"),         keras_cv.layers.JitteredResize(             target_size=(640, 640), scale_factor=(0.75, 1.3), bounding_box_format=\"xywh\"         ),     ] )  train_ds = train_ds.map(augmenter, num_parallel_calls=tf.data.AUTOTUNE) visualize_dataset(     dataset=train_ds,     class_mapping=config.class_mapping,     title=\"Augmented-Train-Dataset\",     max_batches_to_visualize=10, ) In\u00a0[\u00a0]: Copied! <pre>inference_resizing = keras_cv.layers.Resizing(\n    640, 640, bounding_box_format=\"xywh\", pad_to_aspect_ratio=True\n)\neval_ds = eval_ds.map(inference_resizing, num_parallel_calls=tf.data.AUTOTUNE)\nvisualize_dataset(\n    dataset=eval_ds,\n    class_mapping=config.class_mapping,\n    title=\"Resized-Train-Dataset\",\n    max_batches_to_visualize=10,\n)\n</pre> inference_resizing = keras_cv.layers.Resizing(     640, 640, bounding_box_format=\"xywh\", pad_to_aspect_ratio=True ) eval_ds = eval_ds.map(inference_resizing, num_parallel_calls=tf.data.AUTOTUNE) visualize_dataset(     dataset=eval_ds,     class_mapping=config.class_mapping,     title=\"Resized-Train-Dataset\",     max_batches_to_visualize=10, ) In\u00a0[\u00a0]: Copied! <pre>def dict_to_tuple(inputs):\n    return inputs[\"images\"], keras_cv.bounding_box.to_dense(\n        inputs[\"bounding_boxes\"], max_boxes=32\n    )\n\n\ntrain_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\neval_ds = eval_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n\ntrain_ds = train_ds.prefetch(tf.data.AUTOTUNE)\neval_ds = eval_ds.prefetch(tf.data.AUTOTUNE)\n</pre> def dict_to_tuple(inputs):     return inputs[\"images\"], keras_cv.bounding_box.to_dense(         inputs[\"bounding_boxes\"], max_boxes=32     )   train_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE) eval_ds = eval_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)  train_ds = train_ds.prefetch(tf.data.AUTOTUNE) eval_ds = eval_ds.prefetch(tf.data.AUTOTUNE) In\u00a0[\u00a0]: Copied! <pre>pretrained_model = keras_cv.models.RetinaNet.from_preset(\n    config.model_name, bounding_box_format=\"xywh\"\n)\n\noptimizer = keras.optimizers.SGD(\n    learning_rate=config.base_lr,\n    momentum=config.momentum,\n    global_clipnorm=config.global_clipnorm\n)\n\ncoco_metrics = keras_cv.metrics.BoxCOCOMetrics(\n    bounding_box_format=\"xywh\", evaluate_freq=20\n)\n\npretrained_model.compile(\n    classification_loss=\"focal\",\n    box_loss=\"smoothl1\",\n    optimizer=optimizer,\n    metrics=[coco_metrics],\n)\n</pre> pretrained_model = keras_cv.models.RetinaNet.from_preset(     config.model_name, bounding_box_format=\"xywh\" )  optimizer = keras.optimizers.SGD(     learning_rate=config.base_lr,     momentum=config.momentum,     global_clipnorm=config.global_clipnorm )  coco_metrics = keras_cv.metrics.BoxCOCOMetrics(     bounding_box_format=\"xywh\", evaluate_freq=20 )  pretrained_model.compile(     classification_loss=\"focal\",     box_loss=\"smoothl1\",     optimizer=optimizer,     metrics=[coco_metrics], ) In\u00a0[\u00a0]: Copied! <pre>coco_metrics.reset_state()\nresult = pretrained_model.evaluate(eval_ds.take(40))\nresult = coco_metrics.result(force=True)\nwandb.log({f\"Evaluation/{k}\": v.numpy() for k, v in result.items()})\n</pre> coco_metrics.reset_state() result = pretrained_model.evaluate(eval_ds.take(40)) result = coco_metrics.result(force=True) wandb.log({f\"Evaluation/{k}\": v.numpy() for k, v in result.items()}) In\u00a0[\u00a0]: Copied! <pre># Finish the experiment\nwandb.finish()\n</pre> # Finish the experiment wandb.finish()"},{"location":"keras/examples/visualize_dataset/#visualize-object-detection-datasets-using-weights-biases","title":"Visualize Object-Detection Datasets using Weights &amp; Biases\u00b6","text":"<p>This notebook demostrates how you can visualize and debug your data input pipeline for object detetction using Weights &amp; Biases.</p> <p>Original Notebook: https://keras.io/guides/keras_cv/object_detection_keras_cv/</p>"},{"location":"keras/examples/visualize_dataset/#install-dependencies","title":"Install Dependencies\u00b6","text":""},{"location":"keras/examples/visualize_dataset/#initialize-a-weights-biases-run-and-set-up-the-configs","title":"Initialize a Weights &amp; Biases run and Set up the Configs\u00b6","text":""},{"location":"keras/examples/visualize_dataset/#load-pascal-voc-dataset","title":"Load Pascal VOC Dataset\u00b6","text":""},{"location":"keras/examples/visualize_dataset/#visualize-the-raw-dataset-with-bounding-box-annotations","title":"Visualize the Raw Dataset with Bounding Box Annotations\u00b6","text":""},{"location":"keras/examples/visualize_dataset/#visualize-the-training-dataset-with-augmentations","title":"Visualize the Training Dataset with Augmentations\u00b6","text":""},{"location":"keras/examples/visualize_dataset/#visualize-the-resized-validation-dataset","title":"Visualize the Resized Validation Dataset\u00b6","text":""},{"location":"keras/examples/visualize_dataset/#prepare-dataset-for-evaluation","title":"Prepare Dataset for Evaluation\u00b6","text":""},{"location":"keras/examples/visualize_dataset/#define-the-pre-trained-model","title":"Define the Pre-trained Model\u00b6","text":""},{"location":"keras/examples/visualize_dataset/#evaluate-the-pre-trained-model-on-the-evaluation-dataset","title":"Evaluate the Pre-trained Model on the Evaluation Dataset\u00b6","text":""},{"location":"monai/monai/","title":"MonAI Handlers","text":"<p>Handlers for experiment tracking on Weights &amp; Biases with MonAI Engines.</p>"},{"location":"monai/monai/#wandb_addons.monai.stats_handler.WandbStatsHandler","title":"<code>WandbStatsHandler</code>","text":"<p><code>WandbStatsHandler</code> defines a set of Ignite Event-handlers for all the Weights &amp; Biases logging logic. It can be used for any Ignite Engine(trainer, validator and evaluator) and support both epoch level and iteration level. The expected data source is Ignite <code>engine.state.output</code> and <code>engine.state.metrics</code>.</p> Default behaviors <ul> <li>When EPOCH_COMPLETED, write each dictionary item in <code>engine.state.metrics</code> to     Weights &amp; Biases.</li> <li>When ITERATION_COMPLETED, write each dictionary item in     <code>self.output_transform(engine.state.output)</code> to Weights &amp; Biases.</li> </ul> <p>Usage:</p> <pre><code># WandbStatsHandler for logging training metrics and losses at\n# every iteration to Weights &amp; Biases\ntrain_wandb_stats_handler = WandbStatsHandler(output_transform=lambda x: x)\ntrain_wandb_stats_handler.attach(trainer)\n\n# WandbStatsHandler for logging validation metrics and losses at\n# every iteration to Weights &amp; Biases\nval_wandb_stats_handler = WandbStatsHandler(\n    output_transform=lambda x: None,\n    global_epoch_transform=lambda x: trainer.state.epoch,\n)\nval_wandb_stats_handler.attach(evaluator)\n</code></pre> Example notebooks: <ul> <li>3D classification using MonAI.</li> <li>3D segmentation using MonAI.</li> </ul> Pull Request to add <code>WandbStatsHandler</code> to MonAI repository <p>There is an open pull request to add <code>WandbStatsHandler</code> to MonAI.</p> <p>Parameters:</p> Name Type Description Default <code>iteration_log</code> <code>bool</code> <p>Whether to write data to Weights &amp; Biases when iteration completed, default to <code>True</code>.</p> <code>True</code> <code>epoch_log</code> <code>bool</code> <p>Whether to write data to Weights &amp; Biases when epoch completed, default to <code>True</code>.</p> <code>True</code> <code>epoch_event_writer</code> <code>Optional[Callable[[Engine, Any], Any]]</code> <p>Customized callable Weights &amp; Biases writer for epoch level. Must accept parameter \"engine\" and \"summary_writer\", use default event writer if None.</p> <code>None</code> <code>epoch_interval</code> <code>int</code> <p>The epoch interval at which the epoch_event_writer is called. Defaults to 1.</p> <code>1</code> <code>iteration_event_writer</code> <code>Optional[Callable[[Engine, Any], Any]]</code> <p>Customized callable Weights &amp; Biases writer for iteration level. Must accept parameter \"engine\" and \"summary_writer\", use default event writer if None.</p> <code>None</code> <code>iteration_interval</code> <code>int</code> <p>The iteration interval at which the iteration_event_writer is called. Defaults to 1.</p> <code>1</code> <code>output_transform</code> <code>Callable</code> <p>A callable that is used to transform the <code>ignite.engine.state.output</code> into a scalar to plot, or a dictionary of <code>{key: scalar}</code>. In the latter case, the output string will be formatted as key: value. By default this value plotting happens when every iteration completed. The default behavior is to print loss from output[0] as output is a decollated list and we replicated loss value for every item of the decollated list. <code>engine.state</code> and <code>output_transform</code> inherit from the ignite concept: https://pytorch.org/ignite/concepts.html#state, explanation and usage example are in the tutorial: https://github.com/Project-MONAI/tutorials/blob/master/modules/batch_output_transform.ipynb.</p> <code>lambda : x[0]</code> <code>global_epoch_transform</code> <code>Callable</code> <p>A callable that is used to customize global epoch number. For example, in evaluation, the evaluator engine might want to use trainer engines epoch number when plotting epoch vs metric curves.</p> <code>lambda : x</code> <code>state_attributes</code> <code>Optional[Sequence[str]]</code> <p>Expected attributes from <code>engine.state</code>, if provided, will extract them when epoch completed.</p> <code>None</code> <code>tag_name</code> <code>str</code> <p>When iteration output is a scalar, tag_name is used to plot, defaults to <code>'Loss'</code>.</p> <code>DEFAULT_TAG</code> Source code in <code>wandb_addons/monai/stats_handler.py</code> <pre><code>class WandbStatsHandler:\n    \"\"\"\n    `WandbStatsHandler` defines a set of Ignite Event-handlers for all the Weights &amp; Biases logging\n    logic. It can be used for any Ignite Engine(trainer, validator and evaluator) and support both\n    epoch level and iteration level. The expected data source is Ignite `engine.state.output` and\n    `engine.state.metrics`.\n\n    Default behaviors:\n        - When EPOCH_COMPLETED, write each dictionary item in `engine.state.metrics` to\n            Weights &amp; Biases.\n        - When ITERATION_COMPLETED, write each dictionary item in\n            `self.output_transform(engine.state.output)` to Weights &amp; Biases.\n\n    **Usage:**\n\n    ```python\n    # WandbStatsHandler for logging training metrics and losses at\n    # every iteration to Weights &amp; Biases\n    train_wandb_stats_handler = WandbStatsHandler(output_transform=lambda x: x)\n    train_wandb_stats_handler.attach(trainer)\n\n    # WandbStatsHandler for logging validation metrics and losses at\n    # every iteration to Weights &amp; Biases\n    val_wandb_stats_handler = WandbStatsHandler(\n        output_transform=lambda x: None,\n        global_epoch_transform=lambda x: trainer.state.epoch,\n    )\n    val_wandb_stats_handler.attach(evaluator)\n    ```\n\n    ??? example \"Example notebooks:\"\n        - [3D classification using MonAI](../examples/densenet_training_dict).\n        - [3D segmentation using MonAI](../examples/unet_3d_segmentation).\n\n    ??? note \"Pull Request to add `WandbStatsHandler` to MonAI repository\"\n\n        There is an [open pull request](https://github.com/Project-MONAI/MONAI/pull/6305)\n        to add `WandbStatsHandler` to [MonAI](https://github.com/Project-MONAI/MONAI).\n\n\n    Args:\n        iteration_log (bool): Whether to write data to Weights &amp; Biases when iteration completed,\n            default to `True`.\n        epoch_log (bool): Whether to write data to Weights &amp; Biases when epoch completed, default to\n            `True`.\n        epoch_event_writer (Optional[Callable[[Engine, Any], Any]]): Customized callable\n            Weights &amp; Biases writer for epoch level. Must accept parameter \"engine\" and\n            \"summary_writer\", use default event writer if None.\n        epoch_interval (int): The epoch interval at which the epoch_event_writer is called. Defaults\n            to 1.\n        iteration_event_writer (Optional[Callable[[Engine, Any], Any]]): Customized callable\n            Weights &amp; Biases writer for iteration level. Must accept parameter \"engine\" and\n            \"summary_writer\", use default event writer if None.\n        iteration_interval (int): The iteration interval at which the iteration_event_writer is\n            called. Defaults to 1.\n        output_transform (Callable): A callable that is used to transform the\n            `ignite.engine.state.output` into a scalar to plot, or a dictionary of `{key: scalar}`. In\n            the latter case, the output string will be formatted as key: value. By default this value\n            plotting happens when every iteration completed. The default behavior is to print loss\n            from output[0] as output is a decollated list and we replicated loss value for every item\n            of the decollated list. `engine.state` and `output_transform` inherit from the ignite\n            concept: https://pytorch.org/ignite/concepts.html#state, explanation and usage example are\n            in the tutorial:\n            https://github.com/Project-MONAI/tutorials/blob/master/modules/batch_output_transform.ipynb.\n        global_epoch_transform (Callable): A callable that is used to customize global epoch number. For\n            example, in evaluation, the evaluator engine might want to use trainer engines epoch number\n            when plotting epoch vs metric curves.\n        state_attributes (Optional[Sequence[str]]): Expected attributes from `engine.state`, if provided,\n            will extract them when epoch completed.\n        tag_name (str): When iteration output is a scalar, tag_name is used to plot, defaults to `'Loss'`.\n    \"\"\"\n\n    def __init__(\n        self,\n        iteration_log: bool = True,\n        epoch_log: bool = True,\n        epoch_event_writer: Optional[Callable[[Engine, Any], Any]] = None,\n        epoch_interval: int = 1,\n        iteration_event_writer: Optional[Callable[[Engine, Any], Any]] = None,\n        iteration_interval: int = 1,\n        output_transform: Callable = lambda x: x[0],\n        global_epoch_transform: Callable = lambda x: x,\n        state_attributes: Optional[Sequence[str]] = None,\n        tag_name: str = DEFAULT_TAG,\n    ):\n        if wandb.run is None:\n            raise wandb.Error(\"You must call `wandb.init()` before WandbStatsHandler()\")\n\n        self.iteration_log = iteration_log\n        self.epoch_log = epoch_log\n        self.epoch_event_writer = epoch_event_writer\n        self.epoch_interval = epoch_interval\n        self.iteration_event_writer = iteration_event_writer\n        self.iteration_interval = iteration_interval\n        self.output_transform = output_transform\n        self.global_epoch_transform = global_epoch_transform\n        self.state_attributes = state_attributes\n        self.tag_name = tag_name\n\n    def attach(self, engine: Engine) -&gt; None:\n        \"\"\"\n        Register a set of Ignite Event-Handlers to a specified Ignite engine.\n\n        Args:\n            engine (ignite.engine.engine.Engine): Ignite Engine, it can be a trainer, validator\n                or evaluator.\n        \"\"\"\n        if self.iteration_log and not engine.has_event_handler(\n            self.iteration_completed, Events.ITERATION_COMPLETED\n        ):\n            engine.add_event_handler(\n                Events.ITERATION_COMPLETED(every=self.iteration_interval),\n                self.iteration_completed,\n            )\n        if self.epoch_log and not engine.has_event_handler(\n            self.epoch_completed, Events.EPOCH_COMPLETED\n        ):\n            engine.add_event_handler(\n                Events.EPOCH_COMPLETED(every=self.epoch_interval), self.epoch_completed\n            )\n\n    def epoch_completed(self, engine: Engine) -&gt; None:\n        \"\"\"\n        Handler for train or validation/evaluation epoch completed Event. Write epoch level events\n        to Weights &amp; Biases, default values are from Ignite `engine.state.metrics` dict.\n\n        Args:\n            engine (ignite.engine.engine.Engine): Ignite Engine, it can be a trainer, validator\n                or evaluator.\n        \"\"\"\n        if self.epoch_event_writer is not None:\n            self.epoch_event_writer(engine)\n        else:\n            self._default_epoch_writer(engine)\n\n    def iteration_completed(self, engine: Engine) -&gt; None:\n        \"\"\"\n        Handler for train or validation/evaluation iteration completed Event. Write iteration level\n        events to Weighs &amp; Biases, default values are from Ignite `engine.state.output`.\n\n        Args:\n            engine (ignite.engine.engine.Engine): Ignite Engine, it can be a trainer, validator\n                or evaluator.\n        \"\"\"\n        if self.iteration_event_writer is not None:\n            self.iteration_event_writer(engine)\n        else:\n            self._default_iteration_writer(engine)\n\n    def _default_epoch_writer(self, engine: Engine) -&gt; None:\n        \"\"\"\n        Execute epoch level event write operation. Default to write the values from Ignite\n        `engine.state.metrics` dict and write the values of specified attributes of `engine.state`\n        to [Weights &amp; Biases](https://wandb.ai/site).\n\n        Args:\n            engine (ignite.engine.engine.Engine): Ignite Engine, it can be a trainer, validator\n                or evaluator.\n        \"\"\"\n        summary_dict = engine.state.metrics\n\n        for key, value in summary_dict.items():\n            if is_scalar(value):\n                value = value.item() if isinstance(value, torch.Tensor) else value\n                wandb.log({key: value})\n\n        if self.state_attributes is not None:\n            for attr in self.state_attributes:\n                value = getattr(engine.state, attr, None)\n                value = value.item() if isinstance(value, torch.Tensor) else value\n                wandb.log({attr: value})\n\n    def _default_iteration_writer(self, engine: Engine) -&gt; None:\n        \"\"\"\n        Execute iteration level event write operation based on Ignite `engine.state.output` data.\n        Extract the values from `self.output_transform(engine.state.output)`. Since\n        `engine.state.output` is a decollated list and we replicated the loss value for every item\n        of the decollated list, the default behavior is to track the loss from `output[0]`.\n\n        Args:\n            engine (ignite.engine.engine.Engine): Ignite Engine, it can be a trainer, validator\n                or evaluator.\n        \"\"\"\n        loss = self.output_transform(engine.state.output)\n        if loss is None:\n            return  # do nothing if output is empty\n        log_dict = dict()\n        if isinstance(loss, dict):\n            for key, value in loss.items():\n                if not is_scalar(value):\n                    warnings.warn(\n                        \"ignoring non-scalar output in WandbStatsHandler,\"\n                        \" make sure `output_transform(engine.state.output)` returns\"\n                        \" a scalar or dictionary of key and scalar pairs to avoid this warning.\"\n                        \" {}:{}\".format(key, type(value))\n                    )\n                    continue  # not plot multi dimensional output\n                log_dict[key] = (\n                    value.item() if isinstance(value, torch.Tensor) else value\n                )\n        elif is_scalar(loss):  # not printing multi dimensional output\n            log_dict[self.tag_name] = (\n                loss.item() if isinstance(loss, torch.Tensor) else loss\n            )\n        else:\n            warnings.warn(\n                \"ignoring non-scalar output in WandbStatsHandler,\"\n                \" make sure `output_transform(engine.state.output)` returns\"\n                \" a scalar or a dictionary of key and scalar pairs to avoid this warning.\"\n                \" {}\".format(type(loss))\n            )\n\n        wandb.log(log_dict)\n\n    def close(self):\n        \"\"\"Close `WandbStatsHandler`\"\"\"\n        wandb.finish()\n</code></pre>"},{"location":"monai/monai/#wandb_addons.monai.stats_handler.WandbStatsHandler.attach","title":"<code>attach(engine)</code>","text":"<p>Register a set of Ignite Event-Handlers to a specified Ignite engine.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>Ignite Engine, it can be a trainer, validator or evaluator.</p> required Source code in <code>wandb_addons/monai/stats_handler.py</code> <pre><code>def attach(self, engine: Engine) -&gt; None:\n    \"\"\"\n    Register a set of Ignite Event-Handlers to a specified Ignite engine.\n\n    Args:\n        engine (ignite.engine.engine.Engine): Ignite Engine, it can be a trainer, validator\n            or evaluator.\n    \"\"\"\n    if self.iteration_log and not engine.has_event_handler(\n        self.iteration_completed, Events.ITERATION_COMPLETED\n    ):\n        engine.add_event_handler(\n            Events.ITERATION_COMPLETED(every=self.iteration_interval),\n            self.iteration_completed,\n        )\n    if self.epoch_log and not engine.has_event_handler(\n        self.epoch_completed, Events.EPOCH_COMPLETED\n    ):\n        engine.add_event_handler(\n            Events.EPOCH_COMPLETED(every=self.epoch_interval), self.epoch_completed\n        )\n</code></pre>"},{"location":"monai/monai/#wandb_addons.monai.stats_handler.WandbStatsHandler.close","title":"<code>close()</code>","text":"<p>Close <code>WandbStatsHandler</code></p> Source code in <code>wandb_addons/monai/stats_handler.py</code> <pre><code>def close(self):\n    \"\"\"Close `WandbStatsHandler`\"\"\"\n    wandb.finish()\n</code></pre>"},{"location":"monai/monai/#wandb_addons.monai.stats_handler.WandbStatsHandler.epoch_completed","title":"<code>epoch_completed(engine)</code>","text":"<p>Handler for train or validation/evaluation epoch completed Event. Write epoch level events to Weights &amp; Biases, default values are from Ignite <code>engine.state.metrics</code> dict.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>Ignite Engine, it can be a trainer, validator or evaluator.</p> required Source code in <code>wandb_addons/monai/stats_handler.py</code> <pre><code>def epoch_completed(self, engine: Engine) -&gt; None:\n    \"\"\"\n    Handler for train or validation/evaluation epoch completed Event. Write epoch level events\n    to Weights &amp; Biases, default values are from Ignite `engine.state.metrics` dict.\n\n    Args:\n        engine (ignite.engine.engine.Engine): Ignite Engine, it can be a trainer, validator\n            or evaluator.\n    \"\"\"\n    if self.epoch_event_writer is not None:\n        self.epoch_event_writer(engine)\n    else:\n        self._default_epoch_writer(engine)\n</code></pre>"},{"location":"monai/monai/#wandb_addons.monai.stats_handler.WandbStatsHandler.iteration_completed","title":"<code>iteration_completed(engine)</code>","text":"<p>Handler for train or validation/evaluation iteration completed Event. Write iteration level events to Weighs &amp; Biases, default values are from Ignite <code>engine.state.output</code>.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>Ignite Engine, it can be a trainer, validator or evaluator.</p> required Source code in <code>wandb_addons/monai/stats_handler.py</code> <pre><code>def iteration_completed(self, engine: Engine) -&gt; None:\n    \"\"\"\n    Handler for train or validation/evaluation iteration completed Event. Write iteration level\n    events to Weighs &amp; Biases, default values are from Ignite `engine.state.output`.\n\n    Args:\n        engine (ignite.engine.engine.Engine): Ignite Engine, it can be a trainer, validator\n            or evaluator.\n    \"\"\"\n    if self.iteration_event_writer is not None:\n        self.iteration_event_writer(engine)\n    else:\n        self._default_iteration_writer(engine)\n</code></pre>"},{"location":"monai/monai/#wandb_addons.monai.checkpoint_handler.WandbModelCheckpointSaver","title":"<code>WandbModelCheckpointSaver</code>","text":"<p>             Bases: <code>BaseSaveHandler</code></p> <p><code>WandbModelCheckpointSaver</code> is a save handler for PyTorch Ignite that saves model checkpoints as Weights &amp; Biases Artifacts.</p> <p>Usage:</p> <pre><code>from wandb_addons.monai import WandbModelCheckpointSaver\n\ncheckpoint_handler = Checkpoint(\n    {\"model\": model, \"optimizer\": optimizer},\n    WandbModelCheckpointSaver(),\n    n_saved=1,\n    filename_prefix=\"best_checkpoint\",\n    score_name=metric_name,\n    global_step_transform=global_step_from_engine(trainer)\n)\nevaluator.add_event_handler(Events.COMPLETED, checkpoint_handler)\n</code></pre> Source code in <code>wandb_addons/monai/checkpoint_handler.py</code> <pre><code>class WandbModelCheckpointSaver(BaseSaveHandler):\n    \"\"\"`WandbModelCheckpointSaver` is a save handler for PyTorch Ignite that saves model checkpoints as\n    [Weights &amp; Biases Artifacts](https://docs.wandb.ai/guides/artifacts).\n\n    Usage:\n\n    ```python\n    from wandb_addons.monai import WandbModelCheckpointSaver\n\n    checkpoint_handler = Checkpoint(\n        {\"model\": model, \"optimizer\": optimizer},\n        WandbModelCheckpointSaver(),\n        n_saved=1,\n        filename_prefix=\"best_checkpoint\",\n        score_name=metric_name,\n        global_step_transform=global_step_from_engine(trainer)\n    )\n    evaluator.add_event_handler(Events.COMPLETED, checkpoint_handler)\n    ```\n    \"\"\"\n\n    @one_rank_only()\n    def __init__(self):\n        if wandb.run is None:\n            raise wandb.Error(\n                \"You must call `wandb.init()` before `WandbModelCheckpointSaver()`\"\n            )\n\n        self.checkpoint_dir = tempfile.mkdtemp()\n\n    @one_rank_only()\n    def __call__(self, checkpoint: Mapping, filename: Union[str, Path]):\n        checkpoint_path = os.path.join(self.checkpoint_dir, filename)\n        torch.save(checkpoint, checkpoint_path)\n\n        artifact = wandb.Artifact(f\"{wandb.run.id}-checkpoint\", type=\"model\")\n\n        if os.path.isfile(checkpoint_path):\n            artifact.add_file(checkpoint_path)\n        elif os.path.isdir(checkpoint_path):\n            artifact.add_dir(checkpoint_path)\n        else:\n            raise wandb.Error(\n                f\"Unable to local checkpoint path {checkpoint_path} to artifact\"\n            )\n\n        wandb.log_artifact(artifact)\n\n    @one_rank_only()\n    def remove(self, filename):\n        if os.path.exists(filename):\n            shutil.rmtree(filename)\n</code></pre>"},{"location":"monai/examples/densenet_training_dict/","title":"Densenet training dict","text":"<p>Original Source: https://github.com/Project-MONAI/tutorials/blob/main/3d_classification/ignite/densenet_training_dict.py</p> In\u00a0[\u00a0]: Copied! <pre>!mkdir dataset\n%cd dataset\n!wget http://biomedic.doc.ic.ac.uk/brain-development/downloads/IXI/IXI-T1.tar\n!wget http://biomedic.doc.ic.ac.uk/brain-development/downloads/IXI/IXI-T2.tar\n!tar -xf IXI-T1.tar &amp;&amp; tar -xf IXI-T2.tar &amp;&amp; rm -rf IXI-T1.tar &amp;&amp; rm -rf IXI-T2.tar\n%cd ..\n!git clone https://github.com/soumik12345/wandb-addons\n!pip install -q --upgrade pip setuptools\n!pip install -q -e wandb-addons[monai]\n</pre> !mkdir dataset %cd dataset !wget http://biomedic.doc.ic.ac.uk/brain-development/downloads/IXI/IXI-T1.tar !wget http://biomedic.doc.ic.ac.uk/brain-development/downloads/IXI/IXI-T2.tar !tar -xf IXI-T1.tar &amp;&amp; tar -xf IXI-T2.tar &amp;&amp; rm -rf IXI-T1.tar &amp;&amp; rm -rf IXI-T2.tar %cd .. !git clone https://github.com/soumik12345/wandb-addons !pip install -q --upgrade pip setuptools !pip install -q -e wandb-addons[monai] In\u00a0[\u00a0]: Copied! <pre>import os\nimport sys\nfrom glob import glob\n\nimport numpy as np\nimport wandb\nimport torch\nfrom ignite.engine import Events, _prepare_batch, create_supervised_evaluator, create_supervised_trainer\nfrom ignite.handlers import EarlyStopping, ModelCheckpoint\n\nimport monai\nfrom monai.data import decollate_batch, DataLoader\nfrom monai.handlers import ROCAUC, StatsHandler, TensorBoardStatsHandler, stopping_fn_from_metric\nfrom monai.transforms import Activations, AsDiscrete, Compose, LoadImaged, RandRotate90d, Resized, ScaleIntensityd\n\nfrom wandb_addons.monai import WandbStatsHandler, WandbModelCheckpointHandler\n\nmonai.config.print_config()\n</pre> import os import sys from glob import glob  import numpy as np import wandb import torch from ignite.engine import Events, _prepare_batch, create_supervised_evaluator, create_supervised_trainer from ignite.handlers import EarlyStopping, ModelCheckpoint  import monai from monai.data import decollate_batch, DataLoader from monai.handlers import ROCAUC, StatsHandler, TensorBoardStatsHandler, stopping_fn_from_metric from monai.transforms import Activations, AsDiscrete, Compose, LoadImaged, RandRotate90d, Resized, ScaleIntensityd  from wandb_addons.monai import WandbStatsHandler, WandbModelCheckpointHandler  monai.config.print_config() In\u00a0[\u00a0]: Copied! <pre>wandb.tensorboard.patch(root_logdir=\"./runs\")\nwandb.init(project=\"monai-integration\", sync_tensorboard=True, save_code=True)\n</pre> wandb.tensorboard.patch(root_logdir=\"./runs\") wandb.init(project=\"monai-integration\", sync_tensorboard=True, save_code=True) In\u00a0[\u00a0]: Copied! <pre>images = glob(\"./dataset/*\")[:20]\nlabels = np.array([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0], dtype=np.int64)\ntrain_files = [{\"img\": img, \"label\": label} for img, label in zip(images[:10], labels[:10])]\nval_files = [{\"img\": img, \"label\": label} for img, label in zip(images[-10:], labels[-10:])]\n</pre> images = glob(\"./dataset/*\")[:20] labels = np.array([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0], dtype=np.int64) train_files = [{\"img\": img, \"label\": label} for img, label in zip(images[:10], labels[:10])] val_files = [{\"img\": img, \"label\": label} for img, label in zip(images[-10:], labels[-10:])] In\u00a0[\u00a0]: Copied! <pre>train_transforms = Compose(\n    [\n        LoadImaged(keys=[\"img\"], ensure_channel_first=True),\n        ScaleIntensityd(keys=[\"img\"]),\n        Resized(keys=[\"img\"], spatial_size=(96, 96, 96)),\n        RandRotate90d(keys=[\"img\"], prob=0.8, spatial_axes=[0, 2]),\n    ]\n)\nval_transforms = Compose(\n    [\n        LoadImaged(keys=[\"img\"], ensure_channel_first=True),\n        ScaleIntensityd(keys=[\"img\"]),\n        Resized(keys=[\"img\"], spatial_size=(96, 96, 96)),\n    ]\n)\n</pre> train_transforms = Compose(     [         LoadImaged(keys=[\"img\"], ensure_channel_first=True),         ScaleIntensityd(keys=[\"img\"]),         Resized(keys=[\"img\"], spatial_size=(96, 96, 96)),         RandRotate90d(keys=[\"img\"], prob=0.8, spatial_axes=[0, 2]),     ] ) val_transforms = Compose(     [         LoadImaged(keys=[\"img\"], ensure_channel_first=True),         ScaleIntensityd(keys=[\"img\"]),         Resized(keys=[\"img\"], spatial_size=(96, 96, 96)),     ] ) In\u00a0[\u00a0]: Copied! <pre>check_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\ncheck_loader = DataLoader(check_ds, batch_size=2, num_workers=4, pin_memory=torch.cuda.is_available())\ncheck_data = monai.utils.misc.first(check_loader)\nprint(check_data[\"img\"].shape, check_data[\"label\"])\n</pre> check_ds = monai.data.Dataset(data=train_files, transform=train_transforms) check_loader = DataLoader(check_ds, batch_size=2, num_workers=4, pin_memory=torch.cuda.is_available()) check_data = monai.utils.misc.first(check_loader) print(check_data[\"img\"].shape, check_data[\"label\"]) In\u00a0[\u00a0]: Copied! <pre>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnet = monai.networks.nets.DenseNet121(spatial_dims=3, in_channels=1, out_channels=2).to(device)\nloss = torch.nn.CrossEntropyLoss()\nlr = 1e-5\nopt = torch.optim.Adam(net.parameters(), lr)\n</pre> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") net = monai.networks.nets.DenseNet121(spatial_dims=3, in_channels=1, out_channels=2).to(device) loss = torch.nn.CrossEntropyLoss() lr = 1e-5 opt = torch.optim.Adam(net.parameters(), lr) In\u00a0[\u00a0]: Copied! <pre>def prepare_batch(batch, device=None, non_blocking=False):\n    return _prepare_batch((batch[\"img\"], batch[\"label\"]), device, non_blocking)\n\ntrainer = create_supervised_trainer(net, opt, loss, device, False, prepare_batch=prepare_batch)\n</pre> def prepare_batch(batch, device=None, non_blocking=False):     return _prepare_batch((batch[\"img\"], batch[\"label\"]), device, non_blocking)  trainer = create_supervised_trainer(net, opt, loss, device, False, prepare_batch=prepare_batch) In\u00a0[\u00a0]: Copied! <pre>checkpoint_handler = WandbModelCheckpointHandler(\"./runs_dict/\", \"net\", n_saved=10, require_empty=False)\ntrainer.add_event_handler(\n    event_name=Events.EPOCH_COMPLETED, handler=checkpoint_handler, to_save={\"net\": net, \"opt\": opt}\n)\n\ntrain_stats_handler = StatsHandler(name=\"trainer\", output_transform=lambda x: x)\ntrain_stats_handler.attach(trainer)\n\ntrain_tensorboard_stats_handler = TensorBoardStatsHandler(output_transform=lambda x: x)\ntrain_tensorboard_stats_handler.attach(trainer)\n\n# WandbStatsHandler logs loss at every iteration\ntrain_wandb_stats_handler = WandbStatsHandler(output_transform=lambda x: x)\ntrain_wandb_stats_handler.attach(trainer)\n</pre> checkpoint_handler = WandbModelCheckpointHandler(\"./runs_dict/\", \"net\", n_saved=10, require_empty=False) trainer.add_event_handler(     event_name=Events.EPOCH_COMPLETED, handler=checkpoint_handler, to_save={\"net\": net, \"opt\": opt} )  train_stats_handler = StatsHandler(name=\"trainer\", output_transform=lambda x: x) train_stats_handler.attach(trainer)  train_tensorboard_stats_handler = TensorBoardStatsHandler(output_transform=lambda x: x) train_tensorboard_stats_handler.attach(trainer)  # WandbStatsHandler logs loss at every iteration train_wandb_stats_handler = WandbStatsHandler(output_transform=lambda x: x) train_wandb_stats_handler.attach(trainer) In\u00a0[\u00a0]: Copied! <pre># set parameters for validation\nvalidation_every_n_epochs = 1\n\nmetric_name = \"AUC\"\n# add evaluation metric to the evaluator engine\nval_metrics = {metric_name: ROCAUC()}\n\npost_label = Compose([AsDiscrete(to_onehot=2)])\npost_pred = Compose([Activations(softmax=True)])\n# Ignite evaluator expects batch=(img, label) and returns output=(y_pred, y) at every iteration,\n# user can add output_transform to return other values\nevaluator = create_supervised_evaluator(\n    net,\n    val_metrics,\n    device,\n    True,\n    prepare_batch=prepare_batch,\n    output_transform=lambda x, y, y_pred: (\n        [post_pred(i) for i in decollate_batch(y_pred)],\n        [post_label(i) for i in decollate_batch(y, detach=False)],\n    ),\n)\n</pre>  # set parameters for validation validation_every_n_epochs = 1  metric_name = \"AUC\" # add evaluation metric to the evaluator engine val_metrics = {metric_name: ROCAUC()}  post_label = Compose([AsDiscrete(to_onehot=2)]) post_pred = Compose([Activations(softmax=True)]) # Ignite evaluator expects batch=(img, label) and returns output=(y_pred, y) at every iteration, # user can add output_transform to return other values evaluator = create_supervised_evaluator(     net,     val_metrics,     device,     True,     prepare_batch=prepare_batch,     output_transform=lambda x, y, y_pred: (         [post_pred(i) for i in decollate_batch(y_pred)],         [post_label(i) for i in decollate_batch(y, detach=False)],     ), ) In\u00a0[\u00a0]: Copied! <pre># add stats event handler to print validation stats via evaluator\nval_stats_handler = StatsHandler(\n    name=\"evaluator\",\n    output_transform=lambda x: None,  # no need to print loss value, so disable per iteration output\n    global_epoch_transform=lambda x: trainer.state.epoch,\n)  # fetch global epoch number from trainer\nval_stats_handler.attach(evaluator)\n\n# add handler to record metrics to TensorBoard at every epoch\nval_tensorboard_stats_handler = TensorBoardStatsHandler(\n    output_transform=lambda x: None,  # no need to plot loss value, so disable per iteration output\n    global_epoch_transform=lambda x: trainer.state.epoch,\n)  # fetch global epoch number from trainer\nval_tensorboard_stats_handler.attach(evaluator)\n\n# add handler to record metrics to Weights &amp; Biases at every epoch\nval_wandb_stats_handler = WandbStatsHandler(\n    output_transform=lambda x: None,\n    global_epoch_transform=lambda x: trainer.state.epoch,\n)\nval_wandb_stats_handler.attach(trainer)\n</pre> # add stats event handler to print validation stats via evaluator val_stats_handler = StatsHandler(     name=\"evaluator\",     output_transform=lambda x: None,  # no need to print loss value, so disable per iteration output     global_epoch_transform=lambda x: trainer.state.epoch, )  # fetch global epoch number from trainer val_stats_handler.attach(evaluator)  # add handler to record metrics to TensorBoard at every epoch val_tensorboard_stats_handler = TensorBoardStatsHandler(     output_transform=lambda x: None,  # no need to plot loss value, so disable per iteration output     global_epoch_transform=lambda x: trainer.state.epoch, )  # fetch global epoch number from trainer val_tensorboard_stats_handler.attach(evaluator)  # add handler to record metrics to Weights &amp; Biases at every epoch val_wandb_stats_handler = WandbStatsHandler(     output_transform=lambda x: None,     global_epoch_transform=lambda x: trainer.state.epoch, ) val_wandb_stats_handler.attach(trainer) In\u00a0[\u00a0]: Copied! <pre># add early stopping handler to evaluator\nearly_stopper = EarlyStopping(patience=4, score_function=stopping_fn_from_metric(metric_name), trainer=trainer)\nevaluator.add_event_handler(event_name=Events.EPOCH_COMPLETED, handler=early_stopper)\n</pre> # add early stopping handler to evaluator early_stopper = EarlyStopping(patience=4, score_function=stopping_fn_from_metric(metric_name), trainer=trainer) evaluator.add_event_handler(event_name=Events.EPOCH_COMPLETED, handler=early_stopper) In\u00a0[\u00a0]: Copied! <pre># create a validation data loader\nval_ds = monai.data.Dataset(data=val_files, transform=val_transforms)\nval_loader = DataLoader(val_ds, batch_size=2, num_workers=4, pin_memory=torch.cuda.is_available())\n</pre> # create a validation data loader val_ds = monai.data.Dataset(data=val_files, transform=val_transforms) val_loader = DataLoader(val_ds, batch_size=2, num_workers=4, pin_memory=torch.cuda.is_available()) In\u00a0[\u00a0]: Copied! <pre>@trainer.on(Events.EPOCH_COMPLETED(every=validation_every_n_epochs))\ndef run_validation(engine):\n    evaluator.run(val_loader)\n</pre> @trainer.on(Events.EPOCH_COMPLETED(every=validation_every_n_epochs)) def run_validation(engine):     evaluator.run(val_loader) In\u00a0[\u00a0]: Copied! <pre># create a training data loader\ntrain_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\ntrain_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4, pin_memory=torch.cuda.is_available())\n</pre> # create a training data loader train_ds = monai.data.Dataset(data=train_files, transform=train_transforms) train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4, pin_memory=torch.cuda.is_available()) In\u00a0[\u00a0]: Copied! <pre>train_epochs = 30\nstate = trainer.run(train_loader, train_epochs)\nprint(state)\nwandb.finish()\n</pre> train_epochs = 30 state = trainer.run(train_loader, train_epochs) print(state) wandb.finish() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"monai/examples/unet_3d_segmentation/","title":"Unet 3d segmentation","text":"<p>Original Source: https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/unet_segmentation_3d_ignite.ipynb</p> In\u00a0[\u00a0]: Copied! <pre>!python -c \"import monai\" || pip install -q \"monai-weekly[gdown, nibabel, tqdm, ignite]\"\n!python -c \"import wandb\" || pip install -q wandb\n!pip install -q --upgrade git+https://github.com/soumik12345/wandb-addons@\n</pre> !python -c \"import monai\" || pip install -q \"monai-weekly[gdown, nibabel, tqdm, ignite]\" !python -c \"import wandb\" || pip install -q wandb !pip install -q --upgrade git+https://github.com/soumik12345/wandb-addons@ In\u00a0[\u00a0]: Copied! <pre>import glob\nimport logging\nimport os\nfrom pathlib import Path\nimport shutil\nimport sys\nimport tempfile\n\nimport nibabel as nib\nimport numpy as np\nfrom monai.config import print_config\nfrom monai.data import (\n    ArrayDataset,\n    create_test_image_3d,\n    decollate_batch,\n    DataLoader\n)\nfrom monai.handlers import (\n    MeanDice,\n    StatsHandler,\n    TensorBoardImageHandler,\n    TensorBoardStatsHandler,\n)\nfrom monai.losses import DiceLoss\nfrom monai.networks.nets import UNet\nfrom monai.transforms import (\n    Activations,\n    EnsureChannelFirst,\n    AsDiscrete,\n    Compose,\n    LoadImage,\n    RandSpatialCrop,\n    Resize,\n    ScaleIntensity,\n)\nfrom monai.utils import first\n\nimport wandb\nfrom wandb_addons.monai import WandbStatsHandler, WandbModelCheckpointHandler\n\nimport ignite\nimport torch\nfrom tqdm.auto import tqdm\n\nprint_config()\n</pre> import glob import logging import os from pathlib import Path import shutil import sys import tempfile  import nibabel as nib import numpy as np from monai.config import print_config from monai.data import (     ArrayDataset,     create_test_image_3d,     decollate_batch,     DataLoader ) from monai.handlers import (     MeanDice,     StatsHandler,     TensorBoardImageHandler,     TensorBoardStatsHandler, ) from monai.losses import DiceLoss from monai.networks.nets import UNet from monai.transforms import (     Activations,     EnsureChannelFirst,     AsDiscrete,     Compose,     LoadImage,     RandSpatialCrop,     Resize,     ScaleIntensity, ) from monai.utils import first  import wandb from wandb_addons.monai import WandbStatsHandler, WandbModelCheckpointHandler  import ignite import torch from tqdm.auto import tqdm  print_config() In\u00a0[\u00a0]: Copied! <pre>directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\nroot_dir = tempfile.mkdtemp() if directory is None else directory\nprint(root_dir)\n</pre> directory = os.environ.get(\"MONAI_DATA_DIRECTORY\") root_dir = tempfile.mkdtemp() if directory is None else directory print(root_dir) In\u00a0[\u00a0]: Copied! <pre>log_dir = os.path.join(root_dir, \"logs\")\nwandb.tensorboard.patch(log_dir)\nwandb.init(project=\"monai-integration\", save_code=True, sync_tensorboard=True)\n</pre> log_dir = os.path.join(root_dir, \"logs\") wandb.tensorboard.patch(log_dir) wandb.init(project=\"monai-integration\", save_code=True, sync_tensorboard=True) In\u00a0[\u00a0]: Copied! <pre>logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n</pre> logging.basicConfig(stream=sys.stdout, level=logging.INFO) In\u00a0[\u00a0]: Copied! <pre>for i in tqdm(range(40)):\n    im, seg = create_test_image_3d(128, 128, 128, num_seg_classes=1)\n\n    n = nib.Nifti1Image(im, np.eye(4))\n    nib.save(n, os.path.join(root_dir, f\"im{i}.nii.gz\"))\n\n    n = nib.Nifti1Image(seg, np.eye(4))\n    nib.save(n, os.path.join(root_dir, f\"seg{i}.nii.gz\"))\n\nimages = sorted(glob.glob(os.path.join(root_dir, \"im*.nii.gz\")))\nsegs = sorted(glob.glob(os.path.join(root_dir, \"seg*.nii.gz\")))\n</pre> for i in tqdm(range(40)):     im, seg = create_test_image_3d(128, 128, 128, num_seg_classes=1)      n = nib.Nifti1Image(im, np.eye(4))     nib.save(n, os.path.join(root_dir, f\"im{i}.nii.gz\"))      n = nib.Nifti1Image(seg, np.eye(4))     nib.save(n, os.path.join(root_dir, f\"seg{i}.nii.gz\"))  images = sorted(glob.glob(os.path.join(root_dir, \"im*.nii.gz\"))) segs = sorted(glob.glob(os.path.join(root_dir, \"seg*.nii.gz\"))) In\u00a0[\u00a0]: Copied! <pre># Define transforms for image and segmentation\nimtrans = Compose(\n    [\n        LoadImage(image_only=True),\n        ScaleIntensity(),\n        EnsureChannelFirst(),\n        RandSpatialCrop((96, 96, 96), random_size=False),\n    ]\n)\nsegtrans = Compose(\n    [\n        LoadImage(image_only=True),\n        EnsureChannelFirst(),\n        RandSpatialCrop((96, 96, 96), random_size=False),\n    ]\n)\n\n# Define nifti dataset, dataloader\nds = ArrayDataset(images, imtrans, segs, segtrans)\nloader = DataLoader(ds, batch_size=10, num_workers=2, pin_memory=torch.cuda.is_available())\nim, seg = first(loader)\nprint(im.shape, seg.shape)\n</pre> # Define transforms for image and segmentation imtrans = Compose(     [         LoadImage(image_only=True),         ScaleIntensity(),         EnsureChannelFirst(),         RandSpatialCrop((96, 96, 96), random_size=False),     ] ) segtrans = Compose(     [         LoadImage(image_only=True),         EnsureChannelFirst(),         RandSpatialCrop((96, 96, 96), random_size=False),     ] )  # Define nifti dataset, dataloader ds = ArrayDataset(images, imtrans, segs, segtrans) loader = DataLoader(ds, batch_size=10, num_workers=2, pin_memory=torch.cuda.is_available()) im, seg = first(loader) print(im.shape, seg.shape) In\u00a0[\u00a0]: Copied! <pre># Create UNet, DiceLoss and Adam optimizer\ndevice = torch.device(\"cuda:0\")\nnet = UNet(\n    spatial_dims=3,\n    in_channels=1,\n    out_channels=1,\n    channels=(16, 32, 64, 128, 256),\n    strides=(2, 2, 2, 2),\n    num_res_units=2,\n).to(device)\n\nloss = DiceLoss(sigmoid=True)\nlr = 1e-3\nopt = torch.optim.Adam(net.parameters(), lr)\n</pre> # Create UNet, DiceLoss and Adam optimizer device = torch.device(\"cuda:0\") net = UNet(     spatial_dims=3,     in_channels=1,     out_channels=1,     channels=(16, 32, 64, 128, 256),     strides=(2, 2, 2, 2),     num_res_units=2, ).to(device)  loss = DiceLoss(sigmoid=True) lr = 1e-3 opt = torch.optim.Adam(net.parameters(), lr) In\u00a0[\u00a0]: Copied! <pre># Create trainer\ntrainer = ignite.engine.create_supervised_trainer(net, opt, loss, device, False)\n</pre> # Create trainer trainer = ignite.engine.create_supervised_trainer(net, opt, loss, device, False) In\u00a0[\u00a0]: Copied! <pre># optional section for checkpoint and tensorboard logging\n# adding checkpoint handler to save models (network\n# params and optimizer stats) during training\nlog_dir = os.path.join(root_dir, \"logs\")\ncheckpoint_handler = ignite.handlers.ModelCheckpoint(log_dir, \"net\", n_saved=10, require_empty=False)\ntrainer.add_event_handler(\n    event_name=ignite.engine.Events.EPOCH_COMPLETED,\n    handler=checkpoint_handler,\n    to_save={\"net\": net, \"opt\": opt},\n)\n\n# StatsHandler prints loss at every iteration\n# user can also customize print functions and can use output_transform to convert\n# engine.state.output if it's not a loss value\ntrain_stats_handler = StatsHandler(name=\"trainer\", output_transform=lambda x: x)\ntrain_stats_handler.attach(trainer)\n\n# TensorBoardStatsHandler plots loss at every iteration\ntrain_tensorboard_stats_handler = TensorBoardStatsHandler(log_dir=log_dir, output_transform=lambda x: x)\ntrain_tensorboard_stats_handler.attach(trainer)\n\n# WandbStatsHandler logs loss at every iteration to Weights &amp; Biases\ntrain_wandb_stats_handler = WandbStatsHandler(output_transform=lambda x: x)\ntrain_wandb_stats_handler.attach(trainer)\n\n# CheckpointHandler with `WandbModelCheckpointSaver` logs model\n# checkpoints at every iteration\ncheckpoint_handler = Checkpoint(\n    {\"model\": net, \"optimizer\": opt},\n    WandbModelCheckpointSaver(),\n    n_saved=1,\n    filename_prefix=\"best_checkpoint\",\n    score_name=metric_name,\n    global_step_transform=global_step_from_engine(trainer)\n)\nevaluator.add_event_handler(Events.COMPLETED, checkpoint_handler)\n</pre> # optional section for checkpoint and tensorboard logging # adding checkpoint handler to save models (network # params and optimizer stats) during training log_dir = os.path.join(root_dir, \"logs\") checkpoint_handler = ignite.handlers.ModelCheckpoint(log_dir, \"net\", n_saved=10, require_empty=False) trainer.add_event_handler(     event_name=ignite.engine.Events.EPOCH_COMPLETED,     handler=checkpoint_handler,     to_save={\"net\": net, \"opt\": opt}, )  # StatsHandler prints loss at every iteration # user can also customize print functions and can use output_transform to convert # engine.state.output if it's not a loss value train_stats_handler = StatsHandler(name=\"trainer\", output_transform=lambda x: x) train_stats_handler.attach(trainer)  # TensorBoardStatsHandler plots loss at every iteration train_tensorboard_stats_handler = TensorBoardStatsHandler(log_dir=log_dir, output_transform=lambda x: x) train_tensorboard_stats_handler.attach(trainer)  # WandbStatsHandler logs loss at every iteration to Weights &amp; Biases train_wandb_stats_handler = WandbStatsHandler(output_transform=lambda x: x) train_wandb_stats_handler.attach(trainer)  # CheckpointHandler with `WandbModelCheckpointSaver` logs model # checkpoints at every iteration checkpoint_handler = Checkpoint(     {\"model\": net, \"optimizer\": opt},     WandbModelCheckpointSaver(),     n_saved=1,     filename_prefix=\"best_checkpoint\",     score_name=metric_name,     global_step_transform=global_step_from_engine(trainer) ) evaluator.add_event_handler(Events.COMPLETED, checkpoint_handler) In\u00a0[\u00a0]: Copied! <pre># optional section for model validation during training\nvalidation_every_n_epochs = 1\n# Set parameters for validation\nmetric_name = \"Mean_Dice\"\n# add evaluation metric to the evaluator engine\nval_metrics = {metric_name: MeanDice()}\npost_pred = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\npost_label = Compose([AsDiscrete(threshold=0.5)])\n# Ignite evaluator expects batch=(img, seg) and\n# returns output=(y_pred, y) at every iteration,\n# user can add output_transform to return other values\nevaluator = ignite.engine.create_supervised_evaluator(\n    net,\n    val_metrics,\n    device,\n    True,\n    output_transform=lambda x, y, y_pred: (\n        [post_pred(i) for i in decollate_batch(y_pred)],\n        [post_label(i) for i in decollate_batch(y)],\n    ),\n)\n\n# create a validation data loader\nval_imtrans = Compose(\n    [\n        LoadImage(image_only=True),\n        ScaleIntensity(),\n        EnsureChannelFirst(),\n        Resize((96, 96, 96)),\n    ]\n)\nval_segtrans = Compose(\n    [\n        LoadImage(image_only=True),\n        EnsureChannelFirst(),\n        Resize((96, 96, 96)),\n    ]\n)\nval_ds = ArrayDataset(images[21:], val_imtrans, segs[21:], val_segtrans)\nval_loader = DataLoader(val_ds, batch_size=5, num_workers=8, pin_memory=torch.cuda.is_available())\n\n\n@trainer.on(ignite.engine.Events.EPOCH_COMPLETED(every=validation_every_n_epochs))\ndef run_validation(engine):\n    evaluator.run(val_loader)\n\n\n# Add stats event handler to print validation stats via evaluator\nval_stats_handler = StatsHandler(\n    name=\"evaluator\",\n    # no need to print loss value, so disable per iteration output\n    output_transform=lambda x: None,\n    # fetch global epoch number from trainer\n    global_epoch_transform=lambda x: trainer.state.epoch,\n)\nval_stats_handler.attach(evaluator)\n\n# add handler to record metrics to TensorBoard at every validation epoch\nval_tensorboard_stats_handler = TensorBoardStatsHandler(\n    log_dir=log_dir,\n    # no need to plot loss value, so disable per iteration output\n    output_transform=lambda x: None,\n    # fetch global epoch number from trainer\n    global_epoch_transform=lambda x: trainer.state.epoch,\n)\nval_tensorboard_stats_handler.attach(evaluator)\n\nval_wandb_stats_handler = WandbStatsHandler(\n    output_transform=lambda x: None,\n    global_epoch_transform=lambda x: trainer.state.epoch,\n)\nval_wandb_stats_handler.attach(evaluator)\n\n# add handler to draw the first image and the corresponding\n# label and model output in the last batch\n# here we draw the 3D output as GIF format along Depth\n# axis, at every validation epoch\nval_tensorboard_image_handler = TensorBoardImageHandler(\n    log_dir=log_dir,\n    batch_transform=lambda batch: (batch[0], batch[1]),\n    output_transform=lambda output: output[0],\n    global_iter_transform=lambda x: trainer.state.epoch,\n)\nevaluator.add_event_handler(\n    event_name=ignite.engine.Events.EPOCH_COMPLETED,\n    handler=val_tensorboard_image_handler,\n)\n\n# The `Checkpoint` handler for PyTorch Ignite along with `WandbModelCheckpointSaver()`\n# logs model checkpoints as WandB Artifacts.\ncheckpoint_handler = Checkpoint(\n    {\"model\": model, \"optimizer\": optimizer},\n    WandbModelCheckpointSaver(),\n    n_saved=1,\n    filename_prefix=\"best_checkpoint\",\n    score_name=metric_name,\n    global_step_transform=global_step_from_engine(trainer)\n)\nevaluator.add_event_handler(Events.COMPLETED, checkpoint_handler)\n</pre> # optional section for model validation during training validation_every_n_epochs = 1 # Set parameters for validation metric_name = \"Mean_Dice\" # add evaluation metric to the evaluator engine val_metrics = {metric_name: MeanDice()} post_pred = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)]) post_label = Compose([AsDiscrete(threshold=0.5)]) # Ignite evaluator expects batch=(img, seg) and # returns output=(y_pred, y) at every iteration, # user can add output_transform to return other values evaluator = ignite.engine.create_supervised_evaluator(     net,     val_metrics,     device,     True,     output_transform=lambda x, y, y_pred: (         [post_pred(i) for i in decollate_batch(y_pred)],         [post_label(i) for i in decollate_batch(y)],     ), )  # create a validation data loader val_imtrans = Compose(     [         LoadImage(image_only=True),         ScaleIntensity(),         EnsureChannelFirst(),         Resize((96, 96, 96)),     ] ) val_segtrans = Compose(     [         LoadImage(image_only=True),         EnsureChannelFirst(),         Resize((96, 96, 96)),     ] ) val_ds = ArrayDataset(images[21:], val_imtrans, segs[21:], val_segtrans) val_loader = DataLoader(val_ds, batch_size=5, num_workers=8, pin_memory=torch.cuda.is_available())   @trainer.on(ignite.engine.Events.EPOCH_COMPLETED(every=validation_every_n_epochs)) def run_validation(engine):     evaluator.run(val_loader)   # Add stats event handler to print validation stats via evaluator val_stats_handler = StatsHandler(     name=\"evaluator\",     # no need to print loss value, so disable per iteration output     output_transform=lambda x: None,     # fetch global epoch number from trainer     global_epoch_transform=lambda x: trainer.state.epoch, ) val_stats_handler.attach(evaluator)  # add handler to record metrics to TensorBoard at every validation epoch val_tensorboard_stats_handler = TensorBoardStatsHandler(     log_dir=log_dir,     # no need to plot loss value, so disable per iteration output     output_transform=lambda x: None,     # fetch global epoch number from trainer     global_epoch_transform=lambda x: trainer.state.epoch, ) val_tensorboard_stats_handler.attach(evaluator)  val_wandb_stats_handler = WandbStatsHandler(     output_transform=lambda x: None,     global_epoch_transform=lambda x: trainer.state.epoch, ) val_wandb_stats_handler.attach(evaluator)  # add handler to draw the first image and the corresponding # label and model output in the last batch # here we draw the 3D output as GIF format along Depth # axis, at every validation epoch val_tensorboard_image_handler = TensorBoardImageHandler(     log_dir=log_dir,     batch_transform=lambda batch: (batch[0], batch[1]),     output_transform=lambda output: output[0],     global_iter_transform=lambda x: trainer.state.epoch, ) evaluator.add_event_handler(     event_name=ignite.engine.Events.EPOCH_COMPLETED,     handler=val_tensorboard_image_handler, )  # The `Checkpoint` handler for PyTorch Ignite along with `WandbModelCheckpointSaver()` # logs model checkpoints as WandB Artifacts. checkpoint_handler = Checkpoint(     {\"model\": model, \"optimizer\": optimizer},     WandbModelCheckpointSaver(),     n_saved=1,     filename_prefix=\"best_checkpoint\",     score_name=metric_name,     global_step_transform=global_step_from_engine(trainer) ) evaluator.add_event_handler(Events.COMPLETED, checkpoint_handler) In\u00a0[\u00a0]: Copied! <pre># create a training data loader\ntrain_ds = ArrayDataset(images[:20], imtrans, segs[:20], segtrans)\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=5,\n    shuffle=True,\n    num_workers=8,\n    pin_memory=torch.cuda.is_available(),\n)\n\nmax_epochs = 10\nstate = trainer.run(train_loader, max_epochs)\n</pre> # create a training data loader train_ds = ArrayDataset(images[:20], imtrans, segs[:20], segtrans) train_loader = DataLoader(     train_ds,     batch_size=5,     shuffle=True,     num_workers=8,     pin_memory=torch.cuda.is_available(), )  max_epochs = 10 state = trainer.run(train_loader, max_epochs) In\u00a0[\u00a0]: Copied! <pre>wandb.finish()\n\nif directory is None:\n    shutil.rmtree(root_dir)\n</pre> wandb.finish()  if directory is None:     shutil.rmtree(root_dir)"},{"location":"monai/examples/unet_3d_segmentation/#setup-environment","title":"Setup environment\u00b6","text":""},{"location":"monai/examples/unet_3d_segmentation/#setup-weights-biases-run","title":"Setup Weights &amp; Biases run\u00b6","text":""},{"location":"monai/examples/unet_3d_segmentation/#setup-logging","title":"Setup logging\u00b6","text":""},{"location":"monai/examples/unet_3d_segmentation/#setup-demo-data","title":"Setup demo data\u00b6","text":""},{"location":"monai/examples/unet_3d_segmentation/#setup-transforms-dataset","title":"Setup transforms, dataset\u00b6","text":""},{"location":"monai/examples/unet_3d_segmentation/#create-model-loss-optimizer","title":"Create Model, Loss, Optimizer\u00b6","text":""},{"location":"monai/examples/unet_3d_segmentation/#create-supervised_trainer-using-ignite","title":"Create supervised_trainer using ignite\u00b6","text":""},{"location":"monai/examples/unet_3d_segmentation/#setup-event-handlers-for-checkpointing-and-logging","title":"Setup event handlers for checkpointing and logging\u00b6","text":""},{"location":"monai/examples/unet_3d_segmentation/#add-validation-every-n-epochs","title":"Add Validation every N epochs\u00b6","text":""},{"location":"monai/examples/unet_3d_segmentation/#run-training-loop","title":"Run training loop\u00b6","text":""},{"location":"prompts/tracer/","title":"Trace","text":"<p>A high level implementation of <code>Trace</code> for Weight &amp; Biases Prompts.</p>"},{"location":"prompts/tracer/#wandb_addons.prompts.tracer.Trace","title":"<code>Trace</code>","text":"<p>Manage and log a trace - a collection of spans their metadata and hierarchy.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>(str) The name of the root span.</p> required <code>kind</code> <code>str</code> <p>(str, optional) The kind of the root span.</p> <code>None</code> <code>status_code</code> <code>str</code> <p>(str, optional) The status of the root span, either \"error\" or \"success\".</p> <code>None</code> <code>status_message</code> <code>str</code> <p>(str, optional) Any status message associated with the root span.</p> <code>None</code> <code>metadata</code> <code>dict</code> <p>(dict, optional) Any additional metadata for the root span.</p> <code>None</code> <code>start_time_ms</code> <code>int</code> <p>(int, optional) The start time of the root span in milliseconds.</p> <code>None</code> <code>end_time_ms</code> <code>int</code> <p>(int, optional) The end time of the root span in milliseconds.</p> <code>None</code> <code>inputs</code> <code>dict</code> <p>(dict, optional) The named inputs of the root span.</p> <code>None</code> <code>outputs</code> <code>dict</code> <p>(dict, optional) The named outputs of the root span.</p> <code>None</code> <code>model_dict</code> <code>dict</code> <p>(dict, optional) A json serializable dictionary containing the model architecture details.</p> <code>None</code> Source code in <code>wandb_addons/prompts/tracer.py</code> <pre><code>class Trace:\n    \"\"\"Manage and log a trace - a collection of spans their metadata and hierarchy.\n\n    Arguments:\n        name: (str) The name of the root span.\n        kind: (str, optional) The kind of the root span.\n        status_code: (str, optional) The status of the root span, either \"error\" or \"success\".\n        status_message: (str, optional) Any status message associated with the root span.\n        metadata: (dict, optional) Any additional metadata for the root span.\n        start_time_ms: (int, optional) The start time of the root span in milliseconds.\n        end_time_ms: (int, optional) The end time of the root span in milliseconds.\n        inputs: (dict, optional) The named inputs of the root span.\n        outputs: (dict, optional) The named outputs of the root span.\n        model_dict: (dict, optional) A json serializable dictionary containing the model architecture details.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        kind: str = None,\n        status_code: str = None,\n        status_message: str = None,\n        metadata: dict = None,\n        start_time_ms: int = None,\n        end_time_ms: int = None,\n        inputs: dict = None,\n        outputs: dict = None,\n        model_dict: dict = None,\n    ):\n        self._span = self._assert_and_create_span(\n            name=name,\n            kind=kind,\n            status_code=status_code,\n            status_message=status_message,\n            metadata=metadata,\n            start_time_ms=start_time_ms,\n            end_time_ms=end_time_ms,\n            inputs=inputs,\n            outputs=outputs,\n        )\n        if model_dict is not None:\n            assert isinstance(model_dict, dict), \"Model dict must be a dictionary\"\n        self._model_dict = model_dict\n\n    def _assert_and_create_span(\n        self,\n        name: str,\n        kind: Optional[str] = None,\n        status_code: Optional[str] = None,\n        status_message: Optional[str] = None,\n        metadata: Optional[dict] = None,\n        start_time_ms: Optional[int] = None,\n        end_time_ms: Optional[int] = None,\n        inputs: Optional[dict] = None,\n        outputs: Optional[dict] = None,\n    ):\n        if kind is not None:\n            assert (\n                kind.upper() in SpanKind.__members__\n            ), \"Invalid span kind, can be one of 'LLM', 'AGENT', 'CHAIN', 'TOOL'\"\n            kind = SpanKind(kind.upper())\n        if status_code is not None:\n            assert (\n                status_code.upper() in StatusCode.__members__\n            ), \"Invalid status code, can be one of 'SUCCESS' or 'ERROR'\"\n            status_code = StatusCode(status_code.upper())\n        if inputs is not None and outputs is not None:\n            assert isinstance(inputs, dict), \"Inputs must be a dictionary\"\n            assert isinstance(outputs, dict), \"Outputs must be a dictionary\"\n            result = Result(inputs=inputs, outputs=outputs)\n        else:\n            result = None\n\n        return Span(\n            name=name,\n            span_kind=kind,\n            status_code=status_code,\n            status_message=status_message,\n            attributes=metadata,\n            start_time_ms=start_time_ms,\n            end_time_ms=end_time_ms,\n            results=[result],\n        )\n\n    def add_child(\n        self,\n        child: \"Trace\",\n    ) -&gt; \"Trace\":\n        \"\"\"Add a child span to the current span of the trace.\"\"\"\n        self._span.add_child_span(child._span)\n        if self._model_dict is not None and child._model_dict is not None:\n            self._model_dict.update({child._span.name: child._model_dict})\n        return self\n\n    def add_metadata(self, metadata: dict) -&gt; \"Trace\":\n        \"\"\"Add metadata to the span of the current trace.\"\"\"\n        if self._span.attributes is None:\n            self._span.attributes = metadata\n        else:\n            self._span.attributes.update(metadata)\n        return self\n\n    def add_inputs_and_outputs(self, inputs: dict, outputs: dict) -&gt; \"Trace\":\n        \"\"\"Add a result to the span of the current trace.\"\"\"\n        if self._span.results == [None]:\n            result = Result(inputs=inputs, outputs=outputs)\n            self._span.results = [result]\n        else:\n            result = Result(inputs=inputs, outputs=outputs)\n            self._span.results.append(result)\n        return self\n\n    def log(self, name: str) -&gt; None:\n        \"\"\"Log the trace to a wandb run\"\"\"\n        trace_tree = WBTraceTree(self._span, self._model_dict)\n        assert (\n            wandb.run is not None\n        ), \"You must call wandb.init() before logging a trace\"\n        wandb.run.log({name: trace_tree})\n</code></pre>"},{"location":"prompts/tracer/#wandb_addons.prompts.tracer.Trace.add_child","title":"<code>add_child(child)</code>","text":"<p>Add a child span to the current span of the trace.</p> Source code in <code>wandb_addons/prompts/tracer.py</code> <pre><code>def add_child(\n    self,\n    child: \"Trace\",\n) -&gt; \"Trace\":\n    \"\"\"Add a child span to the current span of the trace.\"\"\"\n    self._span.add_child_span(child._span)\n    if self._model_dict is not None and child._model_dict is not None:\n        self._model_dict.update({child._span.name: child._model_dict})\n    return self\n</code></pre>"},{"location":"prompts/tracer/#wandb_addons.prompts.tracer.Trace.add_inputs_and_outputs","title":"<code>add_inputs_and_outputs(inputs, outputs)</code>","text":"<p>Add a result to the span of the current trace.</p> Source code in <code>wandb_addons/prompts/tracer.py</code> <pre><code>def add_inputs_and_outputs(self, inputs: dict, outputs: dict) -&gt; \"Trace\":\n    \"\"\"Add a result to the span of the current trace.\"\"\"\n    if self._span.results == [None]:\n        result = Result(inputs=inputs, outputs=outputs)\n        self._span.results = [result]\n    else:\n        result = Result(inputs=inputs, outputs=outputs)\n        self._span.results.append(result)\n    return self\n</code></pre>"},{"location":"prompts/tracer/#wandb_addons.prompts.tracer.Trace.add_metadata","title":"<code>add_metadata(metadata)</code>","text":"<p>Add metadata to the span of the current trace.</p> Source code in <code>wandb_addons/prompts/tracer.py</code> <pre><code>def add_metadata(self, metadata: dict) -&gt; \"Trace\":\n    \"\"\"Add metadata to the span of the current trace.\"\"\"\n    if self._span.attributes is None:\n        self._span.attributes = metadata\n    else:\n        self._span.attributes.update(metadata)\n    return self\n</code></pre>"},{"location":"prompts/tracer/#wandb_addons.prompts.tracer.Trace.log","title":"<code>log(name)</code>","text":"<p>Log the trace to a wandb run</p> Source code in <code>wandb_addons/prompts/tracer.py</code> <pre><code>def log(self, name: str) -&gt; None:\n    \"\"\"Log the trace to a wandb run\"\"\"\n    trace_tree = WBTraceTree(self._span, self._model_dict)\n    assert (\n        wandb.run is not None\n    ), \"You must call wandb.init() before logging a trace\"\n    wandb.run.log({name: trace_tree})\n</code></pre>"},{"location":"prompts/examples/Trace_QuickStart/","title":"Trace QuickStart","text":"<p>A quick start example that demonstrates how to use the <code>Trace</code> class, a high-level API to log LLM calls with the wandb prompts feature.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install -qqq -U openai langchain wandb\n</pre> !pip install -qqq -U openai langchain wandb In\u00a0[\u00a0]: Copied! <pre>import datetime\n\nimport wandb\nfrom langchain.chains import LLMChain\nfrom langchain.llms.fake import FakeListLLM\nfrom langchain.prompts import PromptTemplate\n\nfrom wandb_addons.prompts import Trace\n</pre> import datetime  import wandb from langchain.chains import LLMChain from langchain.llms.fake import FakeListLLM from langchain.prompts import PromptTemplate  from wandb_addons.prompts import Trace In\u00a0[\u00a0]: Copied! <pre>PROJECT=\"high_level_trace\"\n</pre> PROJECT=\"high_level_trace\" In\u00a0[\u00a0]: Copied! <pre>#trace langchain chains\nrun = wandb.init(project=PROJECT)\nllm = FakeListLLM(responses=[f\"Fake response: {i}\" for i in range(100)])\nprompt_template = \"What is a good name for a company that makes {product}?\"\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=prompt_template,\n)\n\nchain = LLMChain(llm=llm, prompt=prompt)\n\nfor i in range(2):\n    product = f\"q: {i} - {datetime.datetime.now().timestamp()}\"\n    start_time_ms = datetime.datetime.now().timestamp() * 1000\n    response = chain(product)\n    end_time_ms = datetime.datetime.now().timestamp() * 1000\n    trace = Trace(name=f\"fake_chain_{i}\",\n          kind=\"chain\",\n          status_code=\"success\",\n          metadata=None,\n          start_time_ms=start_time_ms,\n          end_time_ms=end_time_ms,\n          inputs={\"prompt\":prompt_template.format(product=product)},\n          outputs={\"response\": response[\"text\"]},\n          )\n    trace.log(name=f\"trace_{i}\")\nrun.finish()\n</pre> #trace langchain chains run = wandb.init(project=PROJECT) llm = FakeListLLM(responses=[f\"Fake response: {i}\" for i in range(100)]) prompt_template = \"What is a good name for a company that makes {product}?\" prompt = PromptTemplate(     input_variables=[\"product\"],     template=prompt_template, )  chain = LLMChain(llm=llm, prompt=prompt)  for i in range(2):     product = f\"q: {i} - {datetime.datetime.now().timestamp()}\"     start_time_ms = datetime.datetime.now().timestamp() * 1000     response = chain(product)     end_time_ms = datetime.datetime.now().timestamp() * 1000     trace = Trace(name=f\"fake_chain_{i}\",           kind=\"chain\",           status_code=\"success\",           metadata=None,           start_time_ms=start_time_ms,           end_time_ms=end_time_ms,           inputs={\"prompt\":prompt_template.format(product=product)},           outputs={\"response\": response[\"text\"]},           )     trace.log(name=f\"trace_{i}\") run.finish() In\u00a0[\u00a0]: Copied! <pre># trace openai api calls\nfrom getpass import getpass\nimport openai\n\nopenai.api_key = getpass(\"Please enter your openai api key\")\n</pre> # trace openai api calls from getpass import getpass import openai  openai.api_key = getpass(\"Please enter your openai api key\") In\u00a0[\u00a0]: Copied! <pre>run = wandb.init(project=PROJECT)\nrequest_kwargs = dict(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n        {\n            \"role\": \"assistant\",\n            \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\",\n        },\n        {\"role\": \"user\", \"content\": \"Where was it played?\"},\n    ],\n)\n\nstart_time_ms = datetime.datetime.now().timestamp() * 1000\nresponse = openai.ChatCompletion.create(**request_kwargs)\nend_time_ms = datetime.datetime.now().timestamp() * 1000\n\ntrace = Trace(\n    name=f\"openai_chat_completion\",\n    kind=\"llm\",\n    status_code=\"success\",\n    metadata={\"model\": \"gpt-3.5-turbo\"},\n    start_time_ms=start_time_ms,\n    end_time_ms=end_time_ms,\n    inputs={\"messages\":request_kwargs[\"messages\"]},\n    outputs={\"response\": response.choices[0][\"message\"][\"content\"]},\n)\n\ntrace.log(name=f\"openai_trace\")\nrun.finish()\ndisplay(run)\n</pre> run = wandb.init(project=PROJECT) request_kwargs = dict(     model=\"gpt-3.5-turbo\",     messages=[         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},         {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},         {             \"role\": \"assistant\",             \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\",         },         {\"role\": \"user\", \"content\": \"Where was it played?\"},     ], )  start_time_ms = datetime.datetime.now().timestamp() * 1000 response = openai.ChatCompletion.create(**request_kwargs) end_time_ms = datetime.datetime.now().timestamp() * 1000  trace = Trace(     name=f\"openai_chat_completion\",     kind=\"llm\",     status_code=\"success\",     metadata={\"model\": \"gpt-3.5-turbo\"},     start_time_ms=start_time_ms,     end_time_ms=end_time_ms,     inputs={\"messages\":request_kwargs[\"messages\"]},     outputs={\"response\": response.choices[0][\"message\"][\"content\"]}, )  trace.log(name=f\"openai_trace\") run.finish() display(run) In\u00a0[\u00a0]: Copied! <pre># use with promprtify\n!pip install -qqq -U promptify\n</pre> # use with promprtify !pip install -qqq -U promptify In\u00a0[\u00a0]: Copied! <pre>from promptify import OpenAI\nfrom promptify import Prompter\n\nrun = wandb.init(project=PROJECT)\n\n# NER example\nsentence = \"The patient is a 93-year-old female with a medical history of chronic right hip pain, osteoporosis, hypertension, depression, and chronic atrial fibrillation admitted for evaluation and management of severe nausea and vomiting and urinary tract infection\"\n\nmodel = OpenAI(openai.api_key) # or `HubModel()` for Huggingface-based inference\nnlp_prompter = Prompter(model)\n\nstart_time_ms = datetime.datetime.now().timestamp() * 1000\nresult = nlp_prompter.fit(\n    'ner.jinja',\n    domain='medical',\n    text_input=sentence,\n    labels=None)\nend_time_ms = datetime.datetime.now().timestamp() * 1000\n\n\ntrace = Trace(\n    name=f\"openai_chat_completion\",\n    kind=\"llm\",\n    status_code=\"success\",\n    metadata={k:v for k,v in result.items() if k != \"text\"},\n    start_time_ms=start_time_ms,\n    end_time_ms=end_time_ms,\n    inputs={\"sentence\":sentence},\n    outputs={\"entities\": result[\"text\"]},\n)\ntrace.log(name=\"promptify_ner\")\nrun.finish()\ndisplay(run)\n</pre> from promptify import OpenAI from promptify import Prompter  run = wandb.init(project=PROJECT)  # NER example sentence = \"The patient is a 93-year-old female with a medical history of chronic right hip pain, osteoporosis, hypertension, depression, and chronic atrial fibrillation admitted for evaluation and management of severe nausea and vomiting and urinary tract infection\"  model = OpenAI(openai.api_key) # or `HubModel()` for Huggingface-based inference nlp_prompter = Prompter(model)  start_time_ms = datetime.datetime.now().timestamp() * 1000 result = nlp_prompter.fit(     'ner.jinja',     domain='medical',     text_input=sentence,     labels=None) end_time_ms = datetime.datetime.now().timestamp() * 1000   trace = Trace(     name=f\"openai_chat_completion\",     kind=\"llm\",     status_code=\"success\",     metadata={k:v for k,v in result.items() if k != \"text\"},     start_time_ms=start_time_ms,     end_time_ms=end_time_ms,     inputs={\"sentence\":sentence},     outputs={\"entities\": result[\"text\"]}, ) trace.log(name=\"promptify_ner\") run.finish() display(run) In\u00a0[\u00a0]: Copied! <pre>!pip install -qqq -U guidance\n</pre> !pip install -qqq -U guidance In\u00a0[\u00a0]: Copied! <pre>import guidance\n\nrun = wandb.init(project=PROJECT)\n# define the model we will use\nguidance.llm = guidance.llms.OpenAI(\"text-davinci-003\", api_key=openai.api_key)\n\n# define the few shot examples\nexamples = [\n    {'input': 'I wrote about shakespeare',\n    'entities': [{'entity': 'I', 'time': 'present'}, {'entity': 'Shakespeare', 'time': '16th century'}],\n    'reasoning': 'I can write about Shakespeare because he lived in the past with respect to me.',\n    'answer': 'No'},\n    {'input': 'Shakespeare wrote about me',\n    'entities': [{'entity': 'Shakespeare', 'time': '16th century'}, {'entity': 'I', 'time': 'present'}],\n    'reasoning': 'Shakespeare cannot have written about me, because he died before I was born',\n    'answer': 'Yes'}\n]\n\n# define the guidance program\nstructure_prompt = guidance(\n'''Given a sentence tell me whether it contains an anachronism (i.e. whether it could have happened or not based on the time periods associated with the entities).\n----\n\n{{~! display the few-shot examples ~}}\n{{~#each examples}}\nSentence: {{this.input}}\nEntities and dates:{{#each this.entities}}\n{{this.entity}}: {{this.time}}{{/each}}\nReasoning: {{this.reasoning}}\nAnachronism: {{this.answer}}\n---\n{{~/each}}\n\n{{~! place the real question at the end }}\nSentence: {{input}}\nEntities and dates:\n{{gen \"entities\"}}\nReasoning:{{gen \"Reasoning\"}}\nAnachronism:{{#select \"answer\"}} Yes{{or}} No{{/select}}''')\n\nstart_time_ms = datetime.datetime.now().timestamp() * 1000\n# execute the program\nresult = structure_prompt(\n    examples=examples,\n    input='The T-rex bit my dog'\n)\nend_time_ms = datetime.datetime.now().timestamp() * 1000\n# trace guidance\n\ntrace = Trace(\n    name=f\"guidance_anachronism\",\n    kind=\"llm\",\n    status_code=\"success\",\n    metadata=None,\n    start_time_ms=start_time_ms,\n    end_time_ms=end_time_ms,\n    inputs={\"sentence\":result.variables()[\"input\"]},\n    outputs={\"entities\": result.variables()[\"entities\"], \"answer\": result.variables()[\"answer\"]},\n)\ntrace.log(name=\"guidance_anachronism\")\nrun.finish()\ndisplay(run)\n</pre> import guidance  run = wandb.init(project=PROJECT) # define the model we will use guidance.llm = guidance.llms.OpenAI(\"text-davinci-003\", api_key=openai.api_key)  # define the few shot examples examples = [     {'input': 'I wrote about shakespeare',     'entities': [{'entity': 'I', 'time': 'present'}, {'entity': 'Shakespeare', 'time': '16th century'}],     'reasoning': 'I can write about Shakespeare because he lived in the past with respect to me.',     'answer': 'No'},     {'input': 'Shakespeare wrote about me',     'entities': [{'entity': 'Shakespeare', 'time': '16th century'}, {'entity': 'I', 'time': 'present'}],     'reasoning': 'Shakespeare cannot have written about me, because he died before I was born',     'answer': 'Yes'} ]  # define the guidance program structure_prompt = guidance( '''Given a sentence tell me whether it contains an anachronism (i.e. whether it could have happened or not based on the time periods associated with the entities). ----  {{~! display the few-shot examples ~}} {{~#each examples}} Sentence: {{this.input}} Entities and dates:{{#each this.entities}} {{this.entity}}: {{this.time}}{{/each}} Reasoning: {{this.reasoning}} Anachronism: {{this.answer}} --- {{~/each}}  {{~! place the real question at the end }} Sentence: {{input}} Entities and dates: {{gen \"entities\"}} Reasoning:{{gen \"Reasoning\"}} Anachronism:{{#select \"answer\"}} Yes{{or}} No{{/select}}''')  start_time_ms = datetime.datetime.now().timestamp() * 1000 # execute the program result = structure_prompt(     examples=examples,     input='The T-rex bit my dog' ) end_time_ms = datetime.datetime.now().timestamp() * 1000 # trace guidance  trace = Trace(     name=f\"guidance_anachronism\",     kind=\"llm\",     status_code=\"success\",     metadata=None,     start_time_ms=start_time_ms,     end_time_ms=end_time_ms,     inputs={\"sentence\":result.variables()[\"input\"]},     outputs={\"entities\": result.variables()[\"entities\"], \"answer\": result.variables()[\"answer\"]}, ) trace.log(name=\"guidance_anachronism\") run.finish() display(run) In\u00a0[\u00a0]: Copied! <pre># example hierarchies usage in the Trace class\nimport time\n\nroot_trace = Trace(\n    name=\"Parent Model\",\n    kind=\"LLM\",\n    status_code= \"SUCCESS\",\n    metadata={\"attr_1\": 1, \"attr_2\": 2,},\n    start_time_ms=int(round(time.time() * 1000)),\n    end_time_ms=int(round(time.time() * 1000))+1000,\n    inputs={\"user\": \"How old is google?\"},\n    outputs={\"assistant\": \"25 years old\"},\n    model_dict={\"_kind\": \"openai\", \"api_type\": \"azure\"}\n      )\n\nfirst_child = Trace(\n    name=\"Child 1 Model\",\n    kind=\"LLM\",\n    status_code= \"ERROR\",\n    metadata={\"child1_attr_1\": 1, \"child1_attr_2\": 2,},\n    start_time_ms=int(round(time.time() * 1000))+2000,\n    end_time_ms=int(round(time.time() * 1000))+3000,\n    inputs={\"user\": \"How old is google?\"},\n    outputs={\"assistant\": \"25 years old\"},\n    model_dict={\"_kind\": \"openai\", \"api_type\": \"child1_azure\"}\n      )\n\nsecond_child = Trace(\n    name=\"Child 2 Model\",\n    kind=\"LLM\",\n    status_code=\"SUCCESS\",\n    metadata={\"child2_attr_1\": 1, \"child2_attr_2\": 2,},\n    start_time_ms=int(round(time.time() * 1000))+4000,\n    end_time_ms=int(round(time.time() * 1000))+5000,\n    inputs={\"user\": \"How old is google?\"},\n    outputs={\"assistant\": \"25 years old\"},\n    model_dict={\"_kind\": \"openai\", \"api_type\": \"child2_azure\"}\n      )\n</pre> # example hierarchies usage in the Trace class import time  root_trace = Trace(     name=\"Parent Model\",     kind=\"LLM\",     status_code= \"SUCCESS\",     metadata={\"attr_1\": 1, \"attr_2\": 2,},     start_time_ms=int(round(time.time() * 1000)),     end_time_ms=int(round(time.time() * 1000))+1000,     inputs={\"user\": \"How old is google?\"},     outputs={\"assistant\": \"25 years old\"},     model_dict={\"_kind\": \"openai\", \"api_type\": \"azure\"}       )  first_child = Trace(     name=\"Child 1 Model\",     kind=\"LLM\",     status_code= \"ERROR\",     metadata={\"child1_attr_1\": 1, \"child1_attr_2\": 2,},     start_time_ms=int(round(time.time() * 1000))+2000,     end_time_ms=int(round(time.time() * 1000))+3000,     inputs={\"user\": \"How old is google?\"},     outputs={\"assistant\": \"25 years old\"},     model_dict={\"_kind\": \"openai\", \"api_type\": \"child1_azure\"}       )  second_child = Trace(     name=\"Child 2 Model\",     kind=\"LLM\",     status_code=\"SUCCESS\",     metadata={\"child2_attr_1\": 1, \"child2_attr_2\": 2,},     start_time_ms=int(round(time.time() * 1000))+4000,     end_time_ms=int(round(time.time() * 1000))+5000,     inputs={\"user\": \"How old is google?\"},     outputs={\"assistant\": \"25 years old\"},     model_dict={\"_kind\": \"openai\", \"api_type\": \"child2_azure\"}       ) In\u00a0[\u00a0]: Copied! <pre># simple heirarchy\nrun = wandb.init(project=PROJECT, job_type=\"simple_heirarchy\")\n\nroot_trace.add_child(first_child)\nfirst_child.add_child(second_child)\n\nroot_trace.log(\"root_trace\")\n\nwandb.finish()\ndisplay(run)\n</pre> # simple heirarchy run = wandb.init(project=PROJECT, job_type=\"simple_heirarchy\")  root_trace.add_child(first_child) first_child.add_child(second_child)  root_trace.log(\"root_trace\")  wandb.finish() display(run) In\u00a0[\u00a0]: Copied! <pre># nested heirarchy\nrun = wandb.init(project=PROJECT, job_type=\"nested_heirarchy\")\n\nroot_trace.add_child(first_child)\nfirst_child.add_child(second_child)\nroot_trace.add_child(second_child)\n\nroot_trace.log(\"root_trace\")\n\nwandb.finish()\ndisplay(run)\n</pre> # nested heirarchy run = wandb.init(project=PROJECT, job_type=\"nested_heirarchy\")  root_trace.add_child(first_child) first_child.add_child(second_child) root_trace.add_child(second_child)  root_trace.log(\"root_trace\")  wandb.finish() display(run) In\u00a0[\u00a0]: Copied! <pre># all traces\nrun = wandb.init(project=PROJECT, job_type=\"all_traces\")\n\nroot_trace.add_child(first_child)\nfirst_child.add_child(second_child)\n\nsecond_child.log(\"second_child\")\nfirst_child.log(\"first_child\")\nroot_trace.log(\"root_trace\")\n\nwandb.finish()\ndisplay(run)\n</pre> # all traces run = wandb.init(project=PROJECT, job_type=\"all_traces\")  root_trace.add_child(first_child) first_child.add_child(second_child)  second_child.log(\"second_child\") first_child.log(\"first_child\") root_trace.log(\"root_trace\")  wandb.finish() display(run)"},{"location":"ultralytics/yolo/","title":"Ultralytics Integration","text":"<p>Weights &amp; Biases integration with Ultralytics.</p>"},{"location":"ultralytics/yolo/#wandb_addons.ultralytics.callback.WandBUltralyticsCallback","title":"<code>WandBUltralyticsCallback</code>","text":"<p>Stateful callback for logging model checkpoints, predictions, and ground-truth annotations with interactive overlays for bounding boxes to Weights &amp; Biases Tables during training, validation and prediction for a <code>ultratytics</code> workflow.</p> <p>Warning</p> <p>This callback has been deprecated in favor of the feature-complete integration with Ultralytics which was shipped with Weights &amp; Biases Release v0.15.10. Instead of using <code>from wandb_addons.ultralytics import add_wandb_callback</code> please use <code>from wandb.integration.ultralytics import add_wandb_callback</code>.</p> <p>Example</p> <ul> <li>Ultralytics Integration Demo.</li> </ul> <p>Usage:</p> <pre><code>from ultralytics.yolo.engine.model import YOLO\n\nimport wandb\nfrom wandb_addons.ultralytics import add_wandb_callback\n\n# initialize wandb run\nwandb.init(project=\"YOLOv8\")\n\n# initialize YOLO model\nmodel = YOLO(\"yolov8n.pt\")\n\n# add wandb callback\nadd_wandb_callback(model, max_validation_batches=2, enable_model_checkpointing=True)\n\n# train\nmodel.train(data=\"coco128.yaml\", epochs=5, imgsz=640)\n\n# validate\nmodel.val()\n\n# perform inference\nmodel(['img1.jpeg', 'img2.jpeg'])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>YOLO</code> <p>YOLO Model.</p> required <code>max_validation_batches</code> <code>int</code> <p>maximum number of validation batches to log to a table per epoch.</p> <code>1</code> <code>enable_model_checkpointing</code> <code>bool</code> <p>enable logging model checkpoints as artifacts at the end of eveny epoch if set to <code>True</code>.</p> <code>False</code> Source code in <code>wandb_addons/ultralytics/callback.py</code> <pre><code>class WandBUltralyticsCallback:\n    \"\"\"Stateful callback for logging model checkpoints, predictions, and\n    ground-truth annotations with interactive overlays for bounding boxes\n    to Weights &amp; Biases Tables during training, validation and prediction\n    for a `ultratytics` workflow.\n\n    !!! warning\n        This callback has been deprecated in favor of the feature-complete\n        [integration with Ultralytics](https://docs.wandb.ai/guides/integrations/ultralytics)\n        which was shipped with [Weights &amp; Biases Release v0.15.10](https://github.com/wandb/wandb/releases/tag/v0.15.10).\n        Instead of using `from wandb_addons.ultralytics import add_wandb_callback`\n        please use `from wandb.integration.ultralytics import add_wandb_callback`.\n\n    !!! example \"Example\"\n        - [Ultralytics Integration Demo](https://wandb.ai/geekyrakshit/YOLOv8/reports/Ultralytics-Integration-Demo--Vmlldzo0Nzk5OTEz).\n\n    **Usage:**\n\n    ```python\n    from ultralytics.yolo.engine.model import YOLO\n\n    import wandb\n    from wandb_addons.ultralytics import add_wandb_callback\n\n    # initialize wandb run\n    wandb.init(project=\"YOLOv8\")\n\n    # initialize YOLO model\n    model = YOLO(\"yolov8n.pt\")\n\n    # add wandb callback\n    add_wandb_callback(model, max_validation_batches=2, enable_model_checkpointing=True)\n\n    # train\n    model.train(data=\"coco128.yaml\", epochs=5, imgsz=640)\n\n    # validate\n    model.val()\n\n    # perform inference\n    model(['img1.jpeg', 'img2.jpeg'])\n    ```\n\n    Args:\n        model (ultralytics.yolo.engine.model.YOLO): YOLO Model.\n        max_validation_batches (int): maximum number of validation batches to log to\n            a table per epoch.\n        enable_model_checkpointing (bool): enable logging model checkpoints as artifacts\n            at the end of eveny epoch if set to `True`.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: YOLO,\n        max_validation_batches: int = 1,\n        enable_model_checkpointing: bool = False,\n    ) -&gt; None:\n        self.max_validation_batches = max_validation_batches\n        self.enable_model_checkpointing = enable_model_checkpointing\n        self.train_validation_table = wandb.Table(\n            columns=[\"Epoch\", \"Data-Index\", \"Batch-Index\", \"Image\", \"Mean-Confidence\"]\n        )\n        self.validation_table = wandb.Table(\n            columns=[\"Data-Index\", \"Batch-Index\", \"Image\", \"Mean-Confidence\"]\n        )\n        self.prediction_table = wandb.Table(\n            columns=[\"Image\", \"Num-Objects\", \"Mean-Confidence\"]\n        )\n        self._make_predictor(model)\n\n    def _make_predictor(self, model: YOLO):\n        overrides = model.overrides.copy()\n        overrides[\"conf\"] = 0.1\n        self.predictor = TASK_MAP[model.task][3](overrides=overrides, _callbacks=None)\n\n    def _save_model(self, trainer: DetectionTrainer):\n        model_checkpoint_artifact = wandb.Artifact(f\"run_{wandb.run.id}_model\", \"model\")\n        checkpoint_dict = {\n            \"epoch\": trainer.epoch,\n            \"best_fitness\": trainer.best_fitness,\n            \"model\": copy.deepcopy(de_parallel(self.model)).half(),\n            \"ema\": copy.deepcopy(trainer.ema.ema).half(),\n            \"updates\": trainer.ema.updates,\n            \"optimizer\": trainer.optimizer.state_dict(),\n            \"train_args\": vars(trainer.args),\n            \"date\": datetime.now().isoformat(),\n            \"version\": __version__,\n        }\n        checkpoint_path = trainer.wdir / f\"epoch{trainer.epoch}.pt\"\n        torch.save(checkpoint_dict, checkpoint_path, pickle_module=pickle)\n        model_checkpoint_artifact.add_file(checkpoint_path)\n        wandb.log_artifact(\n            model_checkpoint_artifact, aliases=[f\"epoch_{trainer.epoch}\"]\n        )\n\n    def on_fit_epoch_end(self, trainer: DetectionTrainer):\n        validator = trainer.validator\n        dataloader = validator.dataloader\n        class_label_map = validator.names\n        with torch.no_grad():\n            self.device = next(trainer.model.parameters()).device\n            trainer.model.to(\"cpu\")\n            self.model = copy.deepcopy(trainer.model).eval().to(self.device)\n            self.predictor.setup_model(model=self.model, verbose=False)\n            self.train_validation_table = plot_validation_results(\n                dataloader=dataloader,\n                class_label_map=class_label_map,\n                predictor=self.predictor,\n                table=self.train_validation_table,\n                max_validation_batches=self.max_validation_batches,\n                epoch=trainer.epoch,\n            )\n        if self.enable_model_checkpointing:\n            self._save_model(trainer)\n        trainer.model.to(self.device)\n\n    def on_train_end(self, trainer: DetectionTrainer):\n        wandb.log({\"Train-Validation-Table\": self.train_validation_table})\n\n    def on_val_end(self, trainer: DetectionValidator):\n        validator = trainer\n        dataloader = validator.dataloader\n        class_label_map = validator.names\n        with torch.no_grad():\n            self.predictor.setup_model(model=self.model, verbose=False)\n            self.validation_table = plot_validation_results(\n                dataloader=dataloader,\n                class_label_map=class_label_map,\n                predictor=self.predictor,\n                table=self.validation_table,\n                max_validation_batches=self.max_validation_batches,\n            )\n        wandb.log({\"Validation-Table\": self.validation_table})\n\n    def on_predict_end(self, predictor: DetectionPredictor):\n        for result in tqdm(predictor.results):\n            self.prediction_table = plot_predictions(result, self.prediction_table)\n        wandb.log({\"Prediction-Table\": self.prediction_table})\n\n    @property\n    def callbacks(self) -&gt; Dict[str, Callable]:\n        \"\"\"Property contains all the relevant callbacks to add to the YOLO model for\n        the Weights &amp; Biases logging.\"\"\"\n        return {\n            \"on_fit_epoch_end\": self.on_fit_epoch_end,\n            \"on_train_end\": self.on_train_end,\n            \"on_val_end\": self.on_val_end,\n            \"on_predict_end\": self.on_predict_end,\n        }\n</code></pre>"},{"location":"ultralytics/yolo/#wandb_addons.ultralytics.callback.WandBUltralyticsCallback.callbacks","title":"<code>callbacks: Dict[str, Callable]</code>  <code>property</code>","text":"<p>Property contains all the relevant callbacks to add to the YOLO model for the Weights &amp; Biases logging.</p>"},{"location":"ultralytics/yolo/#wandb_addons.ultralytics.callback.add_wandb_callback","title":"<code>add_wandb_callback(model, enable_model_checkpointing=False, enable_train_validation_logging=True, enable_validation_logging=True, enable_prediction_logging=True, max_validation_batches=1)</code>","text":"<p>Function to add the <code>WandBUltralyticsCallback</code> callback to the <code>YOLO</code> model.</p> <p>Warning</p> <p>This callback has been deprecated in favor of the feature-complete integration with Ultralytics which was shipped with Weights &amp; Biases Release v0.15.10. Instead of using <code>from wandb_addons.ultralytics import add_wandb_callback</code> please use <code>from wandb.integration.ultralytics import add_wandb_callback</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>YOLO</code> <p>YOLO Model.</p> required <code>enable_model_checkpointing</code> <code>bool</code> <p>enable logging model checkpoints as artifacts at the end of eveny epoch if set to <code>True</code>.</p> <code>False</code> <code>enable_train_validation_logging</code> <code>bool</code> <p>enable logging the predictions and ground-truths as interactive image overlays on the images from the validation dataloader to a <code>wandb.Table</code> along with mean-confidence of the predictions per-class at the end of each training epoch.</p> <code>True</code> <code>enable_validation_logging</code> <code>bool</code> <p>enable logging the predictions and ground-truths as interactive image overlays on the images from the validation dataloader to a <code>wandb.Table</code> along with mean-confidence of the predictions per-class at the end of validation.</p> <code>True</code> <code>enable_prediction_logging</code> <code>bool</code> <p>enable logging the predictions and ground-truths as interactive image overlays on the images from the validation dataloader to a <code>wandb.Table</code> along with mean-confidence of the predictions per-class at the end of each prediction/inference.</p> <code>True</code> <code>max_validation_batches</code> <code>int</code> <p>maximum number of validation batches to log to a table per epoch.</p> <code>1</code> Source code in <code>wandb_addons/ultralytics/callback.py</code> <pre><code>def add_wandb_callback(\n    model: YOLO,\n    enable_model_checkpointing: bool = False,\n    enable_train_validation_logging: bool = True,\n    enable_validation_logging: bool = True,\n    enable_prediction_logging: bool = True,\n    max_validation_batches: Optional[int] = 1,\n):\n    \"\"\"Function to add the `WandBUltralyticsCallback` callback to the `YOLO` model.\n\n    !!! warning\n        This callback has been deprecated in favor of the feature-complete\n        [integration with Ultralytics](https://docs.wandb.ai/guides/integrations/ultralytics)\n        which was shipped with [Weights &amp; Biases Release v0.15.10](https://github.com/wandb/wandb/releases/tag/v0.15.10).\n        Instead of using `from wandb_addons.ultralytics import add_wandb_callback`\n        please use `from wandb.integration.ultralytics import add_wandb_callback`.\n\n    Args:\n        model (ultralytics.yolo.engine.model.YOLO): YOLO Model.\n        enable_model_checkpointing (bool): enable logging model checkpoints as artifacts\n            at the end of eveny epoch if set to `True`.\n        enable_train_validation_logging (bool): enable logging the predictions and\n            ground-truths as interactive image overlays on the images from the validation\n            dataloader to a `wandb.Table` along with mean-confidence of the predictions\n            per-class at the end of each training epoch.\n        enable_validation_logging (bool): enable logging the predictions and\n            ground-truths as interactive image overlays on the images from the validation\n            dataloader to a `wandb.Table` along with mean-confidence of the predictions\n            per-class at the end of validation.\n        enable_prediction_logging (bool): enable logging the predictions and\n            ground-truths as interactive image overlays on the images from the validation\n            dataloader to a `wandb.Table` along with mean-confidence of the predictions\n            per-class at the end of each prediction/inference.\n        max_validation_batches (int): maximum number of validation batches to log to\n            a table per epoch.\n    \"\"\"\n    if RANK in [-1, 0]:\n        wandb_callback = WandBUltralyticsCallback(\n            copy.deepcopy(model), max_validation_batches, enable_model_checkpointing\n        )\n        if not enable_train_validation_logging:\n            _ = wandb_callback.callbacks.pop(\"on_fit_epoch_end\")\n            _ = wandb_callback.callbacks.pop(\"on_train_end\")\n        if not enable_validation_logging:\n            _ = wandb_callback.callbacks.pop(\"on_val_end\")\n        if not enable_prediction_logging:\n            _ = wandb_callback.callbacks.pop(\"on_predict_end\")\n        for event, callback_fn in wandb_callback.callbacks.items():\n            model.add_callback(event, callback_fn)\n    else:\n        wandb.termerror(\n            \"The RANK of the process to add the callbacks was neither 0 or -1. \"\n            \"No Weights &amp; Biases callbacks were added to this instance of the \"\n            \"YOLO model.\"\n        )\n    return model\n</code></pre>"}]}